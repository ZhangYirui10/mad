nohup: ignoring input
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.98s/it]
Some parameters are on the meta device because they were offloaded to the cpu.
Starting fresh data collection
Processing examples:   0%|          | 0/2000 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 1/2000 [00:10<5:53:56, 10.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 2/2000 [00:18<4:51:59,  8.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 3/2000 [00:26<4:51:52,  8.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 4/2000 [00:33<4:28:04,  8.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 5/2000 [00:41<4:20:44,  7.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 6/2000 [00:48<4:15:29,  7.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 7/2000 [00:56<4:18:44,  7.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 8/2000 [01:04<4:20:49,  7.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 9/2000 [01:10<3:56:56,  7.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   0%|          | 10/2000 [01:15<3:34:31,  6.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "Gas prices were WAY higher in 2008, under REPUBLICAN president George W. Bush."
Error processing example 12994: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 885.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "So now if you don’t comply" and get a COVID-19 vaccination, and you’re on "Social Security or disability, you won’t be able to receive your money."
Error processing example 12995: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Desmond Tutu died on Oct. 20, 2021.
Error processing example 12996: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 885.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says N.C. Attorney General Josh Stein has gone after Jan. 6 protesters but he has "said nothing" about violent summer protests in North Carolina and was "derelict in his duty."
Error processing example 12997: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cargo ships "can’t get into port" because of Donald Trump’s executive order 13959.
Error processing example 12998: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 885.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The newly drawn congressional district map in Arkansas "keeps communities together."
Error processing example 12999: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] We are seeing "the Justice Department treat American parents as domestic terrorists" under the Patriot Act.
Error processing example 13000: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 883.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats are "asking for a blank check book to spend as much as they want without worrying about the repercussions."
Error processing example 13001: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "NIH COVID Treatment Guidelines Approve Ivermectin"
Error processing example 13002: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We’ve created more jobs in the first eight months of my administration than any president in American history — total number of jobs created."
Error processing example 13003: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 883.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Spanish High Court has announced (COVID-19) is a bio weapon with a patent, not a virus."
Processing examples:   1%|          | 11/2000 [01:20<3:21:51,  6.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 12/2000 [01:27<3:33:37,  6.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 13/2000 [01:33<3:26:52,  6.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 14/2000 [01:38<3:17:13,  5.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 15/2000 [01:45<3:22:55,  6.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 16/2000 [01:51<3:27:56,  6.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 17/2000 [01:57<3:24:19,  6.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 18/2000 [02:03<3:23:01,  6.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 19/2000 [02:10<3:29:28,  6.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 20/2000 [02:16<3:25:58,  6.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 21/2000 [02:22<3:21:44,  6.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13004: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "National Guard to fill in for cops who refuse to get vaccinated."
Error processing example 13005: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The nation reported adding 194,000 jobs in September, and Florida accounted for 84,500."
Error processing example 13006: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Australia to Implement Quarantine Camps for the Unvaxed"
Error processing example 13007: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden will "control health care costs by slapping a 95% tax on prescription drugs."
Error processing example 13008: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden Department of Justice "would use the FBI to target parents who speak out against critical race theory."
Error processing example 13009: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Barack Obama signals That Democrats Intend to Cheat in the Virginia Governor’s Election."
Error processing example 13010: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The migrant caravan from Tapachula, Mexico "is the population of Minneapolis."
Error processing example 13011: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 371.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Kansas City tourism sign had a terrible typo.
Error processing example 13012: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "COP26 luxury EVs to be recharged using diesel generators"
Error processing example 13013: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Product shortages are manufactured; goods are stuck in warehouses.
Error processing example 13014: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 885.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] With COVID-19, vaccinated people "are the real threat to" unvaccinated people.
Processing examples:   1%|          | 22/2000 [02:27<3:12:48,  5.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 23/2000 [02:33<3:07:45,  5.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|          | 24/2000 [02:39<3:16:52,  5.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|▏         | 25/2000 [02:45<3:10:40,  5.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|▏         | 26/2000 [02:51<3:16:00,  5.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|▏         | 27/2000 [02:57<3:13:29,  5.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|▏         | 28/2000 [03:01<3:01:18,  5.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   1%|▏         | 29/2000 [03:07<3:02:10,  5.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 30/2000 [03:12<2:54:29,  5.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 31/2000 [03:17<2:50:10,  5.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 32/2000 [03:22<2:54:42,  5.33s/it]Error processing example 13015: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A photo taken on Jan. 6, 2021, at the United States Capitol riot was "doctored to look as if the Capitol was on fire."
Error processing example 13016: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 369.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Children who receive COVID-19 vaccinations "will be walking factories of spike proteins" that will "undoubtedly cause illnesses to spread like wildfire in schools."
Error processing example 13017: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 369.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Scam artists are using Amazon to send people face masks laced with "something toxic."
Error processing example 13018: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 883.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "You've had over 2 million women drop out of the workforce during this pandemic."
Error processing example 13019: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Gas in North Carolina increased 14 cents per gallon this past week, the highest in the nation."
Error processing example 13020: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden threatened to "swoop down with Special Forces folks and gather up every gun in America."
Error processing example 13021: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Dr. Anthony Fauci planned the AIDS epidemic.
Error processing example 13022: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Unmask your kids! Any carbon dioxide concentrations above" 2,000 parts per million are "dangerous!"
Error processing example 13023: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "States run elections, not the federal government."
Error processing example 13024: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 885.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Greenland's ice sheet isn't shrinking any more rapidly today than it was 80 years ago."
Error processing example 13025: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 33/2000 [03:28<2:58:07,  5.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 34/2000 [03:33<2:53:28,  5.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 35/2000 [03:38<2:47:25,  5.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 36/2000 [03:43<2:46:00,  5.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 37/2000 [03:48<2:50:59,  5.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 38/2000 [03:53<2:47:43,  5.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 39/2000 [03:59<2:53:27,  5.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 40/2000 [04:05<3:00:21,  5.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 41/2000 [04:10<2:53:41,  5.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 42/2000 [04:15<2:49:19,  5.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "There is no such thing as a booster. There is a failed product that is sometimes used again in hopes of a different outcome."
Error processing example 13026: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 371.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden withdrew troops from Afghanistan "simply because of Trump Derangement Syndrome and they wanted to reverse any policy that Trump had implemented."
Error processing example 13027: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 371.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "FUN FACT: West Virginia is home to zero billionaires."
Error processing example 13028: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pfizer Confirms COVID-Vaccinated People Can ‘Shed’ Spike Proteins And Harm The Unvaccinated"
Error processing example 13029: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A video shows "your packages were rerouted and have arrived in the port of Miami" as a result of Gov. Ron DeSantis.
Error processing example 13030: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 371.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Virginia voters are being turned away from polling locations because they were told they already requested absentee ballots when they haven’t.
Error processing example 13031: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Keystone Pipeline was blocked by Obama on Warren Buffett’s behalf.
Error processing example 13032: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Japan drops vax rollout"
Error processing example 13033: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 375.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Moderna has been pulled in Iceland and pulled for everyone under age 30 in Finland, Sweden, Norway, Denmark due to heart issues."
Error processing example 13034: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says President Joe Biden "is in talks to pay illegal immigrants $450,000."
Error processing example 13035: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A chart shows China and India are leading in per capita carbon dioxide emissions.
Processing examples:   2%|▏         | 43/2000 [04:20<2:49:18,  5.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 44/2000 [04:25<2:51:10,  5.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 45/2000 [04:31<2:55:42,  5.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 46/2000 [04:36<2:54:55,  5.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 47/2000 [04:41<2:52:02,  5.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 48/2000 [04:46<2:49:50,  5.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▏         | 49/2000 [04:52<2:54:07,  5.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   2%|▎         | 50/2000 [04:57<2:50:24,  5.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 51/2000 [05:02<2:47:06,  5.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 52/2000 [05:07<2:47:10,  5.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13036: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "U.S. households are on track to spend $19 billion more on energy by 2030."
Error processing example 13037: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Scientists say this map represents the US in 30 years if we don’t reverse climate change."
Error processing example 13038: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Virginia Democrat operatives" and Terry McAuliffe campaign staffers posed as white supremacists and stood by Republican Glenn Youngkin’s bus.
Error processing example 13039: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Stalin once ripped all the feathers off a live chicken as a lesson to his followers."
Error processing example 13040: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you’re a low-wage worker and you’re single and don’t have children, we’re literally taxing you into poverty."
Error processing example 13041: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 883.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Fairfax County is not requiring the last four digits of Social Security Numbers on absentee ballots in this election."
Error processing example 13042: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The premier of Australia’s Victoria state "is freezing people’s bank accts" if they are not vaccinated. "They can’t buy or sell until forced into submission."
Error processing example 13043: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There are over 1 million phantom voters on the Florida voter rolls."
Error processing example 13044: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 373.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you were in the military before 2002 you get an additional $1,200 a year, ($100 per month) in Social Security. But you have to ask for it and almost no one knows about it."
Error processing example 13045: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 369.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Federal agents directly incited people on Jan. 6, 2021, and "intentionally entrapped" American citizens.
Processing examples:   3%|▎         | 53/2000 [05:13<2:50:02,  5.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 54/2000 [05:18<2:53:47,  5.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 55/2000 [05:24<2:59:24,  5.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 56/2000 [05:30<3:01:42,  5.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 57/2000 [05:32<2:22:37,  4.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 58/2000 [05:37<2:30:05,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 59/2000 [05:41<2:26:04,  4.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 60/2000 [05:41<1:43:16,  3.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 62/2000 [05:42<1:03:04,  1.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13046: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Jan. 6 is being used as a pretext to strip millions of Americans, disfavored Americans, of their core constitutional rights."
Error processing example 13047: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 371.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] No new COVID-19 cases in Uttar Pradesh, India, due to use of ivermectin
Error processing example 13048: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 117.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Report finds increased risk of spontaneous abortion following COVID-19 vaccination during pregnancy" after correcting "major error" in CDC study.
Error processing example 13049: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Global warming is part of (a) natural cycle and there’s nothing we can actually do to stop these cycles."
Error processing example 13050: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wages have gone up higher, faster than inflation."
Error processing example 13051: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 359.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "DEA agent Mark Ibrahim was forced from his job and later indicted simply because he was near the Capitol protest on Jan. 6."
Error processing example 13052: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Arizona Department of Education gives green light on face mask and vaccine mandates, as well as implementing Critical (Race) Theory."
Error processing example 13053: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows irate airline passenger demanding a seat away from an unvaccinated passenger and being asked to leave the airplane.
Error processing example 13054: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "More fraud in New Jersey’s election uncovered – voting machines would not allow citizens to vote for Republican governor candidate."
Error processing example 13055: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Vitamin D levels of 50 ng/mL correlates to zero mortality rate from COVID."
Processing examples:   3%|▎         | 63/2000 [05:43<51:50,  1.61s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 64/2000 [05:44<52:24,  1.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 65/2000 [05:47<58:58,  1.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 66/2000 [05:50<1:09:25,  2.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 67/2000 [05:51<1:01:16,  1.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 68/2000 [05:53<58:52,  1.83s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   3%|▎         | 69/2000 [05:55<1:01:01,  1.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▎         | 70/2000 [05:58<1:14:39,  2.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▎         | 71/2000 [06:07<2:21:18,  4.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▎         | 72/2000 [06:10<2:06:07,  3.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▎         | 73/2000 [06:16<2:22:38,  4.44s/it]Error processing example 13056: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "No governor in Virginia has ever won when...he or she is the same party as the sitting president."
Error processing example 13057: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Spike protein goes to nucleus and impairs DNA repair."
Error processing example 13058: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Sending your child to school" on a day when vaccines are being administered "is implied consent" for getting the COVID-19 vaccine.
Error processing example 13059: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The infrastructure bill itself is only 10% true infrastructure."
Error processing example 13060: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Merriam-Webster recently changed its definition of the word "vaccine" and removed the immunity portion.
Error processing example 13061: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wisconsin is almost equally divided on partisan lines" and therefore new Legislative maps could be drawn to "represent that 50-50 split."
Error processing example 13062: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Only a court can decide that somebody is incompetent to vote" -- the state Election Commission "cannot force facilities, private facilities, to allow special voting deputies" inside.
Error processing example 13063: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Children are 50 times more likely to be killed by the Covid vaccines than by the virus itself."
Error processing example 13064: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "This idea that it's the pandemic of the unvaccinated, it's just a total lie."
Error processing example 13065: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Iowa ranks 45th in the U.S. for internet connectivity."
Error processing example 13066: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 355.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.23 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▎         | 74/2000 [06:16<1:43:43,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 77/2000 [06:16<46:35,  1.45s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 79/2000 [06:19<44:09,  1.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 80/2000 [06:19<39:25,  1.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 82/2000 [06:20<25:41,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] A crime was committed when a New Jersey election worker allowed someone who said he was not a citizen to fill out a ballot.
Error processing example 13067: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Within two years, there were two 500-year (storm) events" in Wisconsin.
Error processing example 13068: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In the infrastructure bill that just passed the House, "only 10% is actual infrastructure."
Error processing example 13069: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ashley Hinson’s and Mariannette Miller-Meeks’ no votes on the infrastructure bill are "denying rural Iowa broadband internet."
Error processing example 13070: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Astroworld concert was a "test run on the vaxxed" because people who are injected with graphene oxide can be controlled through magnetic frequencies, including music.
Error processing example 13071: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration inherited gains of "50,000 jobs a month. We're now finally back to 500,000 jobs a month. We inherited a country where 4,000 people a day were dying from Covid. That's now down 75%."
Error processing example 13072: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 45.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In 2015, when Terry McAuliffe was governor, the Virginia Department of Education promoted incorporating critical race theory lens in education...They were trying to indoctrinate kids."
Error processing example 13073: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York is "leading the nation with the lowest imprisonment rate of any large state"
Error processing example 13074: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "During the AIDS crisis, can you imagine if gay men and intravenous drug users … had they been pariahs the way the non-vaccinated are? But it would've been inconceivable."
Error processing example 13075: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Live Nation scheduled the 50,000-person Astroworld music festival at a venue that has a capacity of 20,000.
Error processing example 13076: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Gavin Newsom is out of sight likely because he has Guillain-Barre syndrome from his booster shot."
Processing examples:   4%|▍         | 84/2000 [06:21<25:13,  1.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 87/2000 [06:22<17:02,  1.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 89/2000 [06:22<12:37,  2.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   4%|▍         | 90/2000 [06:22<14:27,  2.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▍         | 92/2000 [06:23<10:26,  3.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13077: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "German News Agency Tracks 75 Prominent Athletes Suddenly Dead Of Heart Attacks After COVID Vax"
Error processing example 13078: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Italy drastically reduced the country’s official COV1D-19 death count by over 97%. This means Covid killed fewer people than an average seasonal flu."
Error processing example 13079: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Actor Macaulay Culkin died from a heroin overdose.
Error processing example 13080: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kyle Rittenhouse’s mother "drove him across state lines and dropped him off in the middle of a riot armed with an assault rifle" in Kenosha.
Error processing example 13081: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A new medical diagnosis code has been designated for people who decline vaccines, and it will be used to help determine who will go into "education camps."
Error processing example 13082: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The COVID-19 vaccines "failed miserably" in animal trials and are "a type of gene therapy that several top scientists warn will kill you."
Error processing example 13083: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden’s banking nominee demands all ‘private bank accounts’ be eliminated"
Error processing example 13084: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Fauci admits Covid vax doesn't work"
Error processing example 13085: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "No economist out there is projecting that (the Build Back Better bill) will have a negative impact on inflation."
Error processing example 13086: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Only one member of Iowa’s federal delegation" voted in favor of the bipartisan infrastructure legislation.
Processing examples:   5%|▍         | 94/2000 [06:23<08:01,  3.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▍         | 95/2000 [06:24<12:32,  2.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▍         | 98/2000 [06:26<17:41,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▌         | 102/2000 [06:26<10:22,  3.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▌         | 103/2000 [06:29<19:01,  1.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13087: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A new Facebook/Meta rule allows the company to use people’s photos without their permission, and posting a notice on your page will stop it from doing so.
Error processing example 13088: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The media televised the trial of Kyle Rittenhouse "because he is a ‘white supremacist shooter,’" while limiting coverage of accused sex trafficker Ghislaine Maxwell’s trial to "some cartoon drawing….What are they hiding?"
Error processing example 13089: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Raz Simone "conquered" part of Seattle, handed out guns to form a paramilitary force that murdered three people, robbed the locals and walked away without being questioned.
Error processing example 13090: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Timothy Simpkins, a Black teen shooter, got 1 day in jail and $75,000 bail, while Kyle Rittenhouse, a white teen, got 2 months in jail and $2 million bail. "I think it’s pretty evident ... that white privilege is a myth."
Error processing example 13091: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 45.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Build Back Better bill provides "'parole' amnesty for millions of criminal illegal aliens."
Error processing example 13092: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "500 National Guardsmen being called to Kenosha ahead of the Rittenhouse verdict. Had they done this in the first place, there wouldn’t have been a trial."
Error processing example 13093: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Bill Gates is linked to the discovery of vials labeled "smallpox" at Pennsylvania lab.
Error processing example 13094: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Athletes are collapsing with heart-related issues due to the COVID-19 vaccines
Error processing example 13095: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United States had 589 coal-fired plants 10 years ago, and "we're down to 504. … We are the only nation that has reduced our reliance (on) coal energy."
Error processing example 13096: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The FBI is using its counterterrorism division to investigate and add ‘threat tags’ to parents who are protesting school boards."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▌         | 105/2000 [06:29<14:12,  2.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   5%|▌         | 109/2000 [06:30<11:32,  2.73it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 110/2000 [06:31<14:48,  2.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 113/2000 [06:32<13:30,  2.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13097: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An increase in Medicare Part B premiums means "America’s seniors are paying the price for Biden’s inflation crisis."
Error processing example 13098: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photos showed "bricks everywhere" in Kenosha during the Kyle Rittenhouse trial.
Error processing example 13099: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Wisconsin, we have 50% of the population paying 50%, 60%, 70%, sometimes 80% for their housing."
Error processing example 13100: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The FBI targeted a woman and searched her home because she "protested local school board, elections."
Error processing example 13101: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you or your family spends $28 a day — prepare to be under constant audit by the IRS."
Error processing example 13102: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Never in American history has so much been spent at one time. Never in American history will so many taxes be raised and so much borrowing be needed to pay for this reckless spending."
Error processing example 13103: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The House-passed Build Back Better bill is a "government takeover of child care."
Error processing example 13104: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Expanding Medicaid, which if we had done at the beginning of the Abbott administration, would have brought in $100 billion to this state's economy."
Error processing example 13105: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Low gas prices in Russia, Kuwait and Saudi Arabia show that Joe Biden’s decision to cancel the Keystone Pipeline project is to blame for U.S. prices.
Error processing example 13106: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "This one here has subsidies for people making as much as $800,000 to be able to buy a Tesla."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 116/2000 [06:32<09:12,  3.41it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 118/2000 [06:33<08:45,  3.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 119/2000 [06:40<40:45,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 120/2000 [06:42<44:19,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 121/2000 [06:43<43:53,  1.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▌         | 123/2000 [06:43<30:04,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13107: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kyle Rittenhouse was an "armed person" crossing state lines when he came to Kenosha protests in 2020.
Error processing example 13108: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Right now, in the average Texas fourth grade classroom, 7 out of 10 kids cannot read at grade level."
Error processing example 13109: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vaccinated people under 60 in England "are dying at twice the rate of unvaccinated people the same age. ... I don’t know how to explain this other than vaccine-caused mortality."
Error processing example 13110: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Richard von Coudenhove-Kalergi planned "the ethnocide of the peoples of Europe" through "the encouragement of mass non-white immigration."
Error processing example 13111: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Vatican owns a telescope called Lucifer.
Error processing example 13112: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 343.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The COVID-19 vaccines "suppress the immune system" and make people more susceptible to HIV, shingles and herpes.
Error processing example 13113: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Nancy Pelosi just bought a 11,000 square foot $25 million mansion in Florida."
Error processing example 13114: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Norway spends about $30,000 per child on early childhood care, Finland spends $23,000, Germany $18,000" and the U.S. $500.
Error processing example 13115: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Malcolm X’s daughter found dead in NYC days after she exposed NYPD and FBI were behind Malcolm X assassination"
Error processing example 13116: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Omicron 'hysteria' was timed so that it would coincide with and distract from the Ghislaine Maxwell trial.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▋         | 127/2000 [06:44<16:49,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   6%|▋         | 129/2000 [06:47<24:45,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 131/2000 [06:47<18:37,  1.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 133/2000 [06:49<22:46,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13117: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Electric vehicles are more likely to fail in traffic jams.
Error processing example 13118: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Don’t be fooled by the next ‘Variant’! There are actually 1000’s of them. They just will pick the next one to keep the fear going."
Error processing example 13119: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pathogens evolve to become less, not more, virulent over time" because otherwise, "they’d destroy their hosts which they depend on to live."
Error processing example 13120: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Heart complications are one symptom "listed" for the omicron variant and are being "used to cover up effects" of COVID-19 vaccines.
Error processing example 13121: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "39 out of 42 (countries) in Europe have more restrictive abortion laws" than Mississippi.
Error processing example 13122: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "BREAKING: Judge in Ghislaine Maxwell trial issues media-wide gag order: all press & spectators barred from courtroom."
Error processing example 13123: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "An independent analysis shows that payments for cancer care will be slashed by close to 45% causing cancer clinics to close and massively raising your healthcare costs."
Error processing example 13124: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Canary Islands volcano eruption could lead to a mega tsunami in the U.S., with a 160-foot wave and no warnings.
Error processing example 13125: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "I used to drive a tractor trailer … I only did it for part of a summer."
Error processing example 13126: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Joe Biden "shut down travel from 8 countries in response to the Omicron variant, but he refuses to require a #COVID19 test for illegal immigrants crossing our Southern border."
Processing examples:   7%|▋         | 134/2000 [06:49<19:47,  1.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 136/2000 [06:55<42:40,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 137/2000 [06:56<41:26,  1.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 140/2000 [07:02<47:02,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 141/2000 [07:02<43:28,  1.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 143/2000 [07:05<42:55,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13127: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] At Lions Gate Hospital in Vancouver, "13 babies were reportedly stillborn at the hospital in a period of 24 hours. All of their mothers had received a COVID-19 injection."
Error processing example 13128: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "On the first day of the Ghislaine Maxwell trial, the CEO of Twitter resigned, the CEO of Walmart resigned, the CEO of CNBC resigned"
Error processing example 13129: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The number of small businesses is up 30% compared to before the pandemic."
Error processing example 13130: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you look pre-COVID" under President Donald Trump, "we didn't import a single barrel of oil from Saudi Arabia."
Error processing example 13131: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The number of COVID-19 deaths recorded so far in 2021 has surpassed the total for 2020."
Error processing example 13132: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID-19 vaccines are gene therapy and a recent Forbes article proves that.
Error processing example 13133: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 329.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Overdoses in Virginia (are) up 35%."
Error processing example 13134: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Even after accounting for rising prices, the typical American family has more money in their pockets than they did last year."
Error processing example 13135: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] German chemist Andreas Noack was "assassinated just hours after publishing the secret of the vax by government operatives."
Error processing example 13136: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Only the fully vaccinated should fear the new ‘worst ever’ Covid-19 variant; data shows they already account for 4 in every 5 Covid deaths"
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 145/2000 [07:05<30:18,  1.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   7%|▋         | 146/2000 [07:06<30:33,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 150/2000 [07:07<17:46,  1.73it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 151/2000 [07:08<16:41,  1.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 153/2000 [07:08<12:04,  2.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13137: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Microsoft and Bill Gates created a 1999 video game called "Omikron."
Error processing example 13138: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "2,809 Dead Babies in VAERS Following COVID Shots as New Documents Prove Pfizer, the FDA, and the CDC Knew the Shots Were Not Safe for Pregnant Women"
Error processing example 13139: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "By declining the vax, I am 100% safe from adverse reactions and 99.8% safe from COVID."
Error processing example 13140: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The toxicity of COVID-Omicron is 5 times higher than that of the delta variant, and the mortality rate is also higher than that of Delta."
Error processing example 13141: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Tap water is testing positive for COVID!"
Error processing example 13142: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "SHOCKING: In the wake of Austria’s drastic lockdown of unvaccinated people, EU chief calls for throwing out Nuremberg Code."
Error processing example 13143: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Over time there have been 160-some carve-outs to the filibuster."
Error processing example 13144: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Dr. Ugur Sahin, CEO of BioNTech and inventor of the BIO N TECH Pfizer jab, refuses to take the jab for safety reasons."
Error processing example 13145: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Deaths from vaccines have totaled 1,621 from Johnson & Johnson, 4,799 from Moderna, 13,039 from Pfizer and 73 from unknown vaccines. And this is an undercount because "only 1% of deaths are reported."
Error processing example 13146: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Teens are "more likely to be hospitalized with myocarditis" from the COVID-19 vaccines than to be "hospitalized with COVID."
Processing examples:   8%|▊         | 154/2000 [07:08<12:21,  2.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 157/2000 [07:09<09:45,  3.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 158/2000 [07:10<13:40,  2.25it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 160/2000 [07:10<12:09,  2.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 161/2000 [07:11<12:18,  2.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 163/2000 [07:17<39:46,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 164/2000 [07:22<1:04:47,  2.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13147: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The COVID-19 vaccine is the "deadliest vaccine ever made."
Error processing example 13148: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "COVID-19 vaccines do not stop transmission of COVID, but instead increase it."
Error processing example 13149: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Audio clips from ad show Pat McCrory supported Black Lives Matter protesters while condemning ‘Trump supporters’
Error processing example 13150: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Actions by Joe Biden left "10,000 - 15,000 American citizens abandoned to terrorists in Afghanistan."
Error processing example 13151: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Build Back Better Act "includes $34.5 billion in cuts" for charity care funding to hospitals.
Error processing example 13152: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Canada joins the no jab, no food trend"
Error processing example 13153: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Moderna patent uncovers horror nanocensor contained in bioweapon."
Error processing example 13154: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Around the world, people who exercise their "health autonomy" and don't get vaccinated are being put "basically into internment camps."
Error processing example 13155: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Voter ID is supported by an overwhelming majority of NYers, from all across the state, walks of life, & political parties."
Error processing example 13156: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gas prices were "$1.86 when I left" the White House.
Error processing example 13157: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 329.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Chuck Grassley was "voting to slash Medicare" when voting against the debt ceiling bill.
Processing examples:   8%|▊         | 165/2000 [07:25<1:11:34,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 167/2000 [07:26<47:38,  1.56s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   8%|▊         | 170/2000 [07:26<26:50,  1.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▊         | 171/2000 [07:30<41:34,  1.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▊         | 172/2000 [07:30<33:49,  1.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▊         | 173/2000 [07:30<27:06,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▊         | 174/2000 [07:32<39:10,  1.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13158: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] NASA study says snake plants can produce enough oxygen for humans to live in a sealed room.
Error processing example 13159: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Soft-tissue cancer diagnoses have "climbed through the roof" in 2021 as a result of COVID-19 vaccines.
Error processing example 13160: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A video from British public broadcaster BBC shows the network was part of a 9/11 conspiracy.
Error processing example 13161: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The push for high-speed internet for rural areas, from the start, "was a Democratic thing."
Error processing example 13162: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "They've now killed close to twice as many kids from the vaccine as have died from COVID."
Error processing example 13163: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Polar bears have increased 400% in 45 years; whales are nearly fully recovered; extinctions are down 90% past century. Koalas are doing fine. If we could ban wind turbines we could save 85,000 birds of prey/yr in US alone."
Error processing example 13164: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 45.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The CBO says (the Build Back Better Act) is $3 trillion of deficit spending."
Error processing example 13165: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "I’ve been against that war in Afghanistan from the very beginning."
Error processing example 13166: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Kathy Castor, D-Fla., is a communist
Error processing example 13167: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The COVID-19 vaccines cause AIDS.
Error processing example 13168: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] People vaccinated for COVID-19 "do not spread the disease to anyone else."
Processing examples:   9%|▉         | 176/2000 [07:34<32:56,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▉         | 180/2000 [07:35<19:37,  1.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▉         | 183/2000 [07:36<16:15,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▉         | 185/2000 [07:39<22:03,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13169: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Electromagnetic radiation from wireless technologies, including and especially 5G, can cause the same symptoms as COVID-19!"
Error processing example 13170: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Most Americans on minimum wage earn less than a Dickensian allegory for destitution."
Error processing example 13171: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "More people, aged 18-45, died of Fentanyl overdoses in 2020 than covid, car accidents, cancer, + suicide combined."
Error processing example 13172: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Half a million sharks could be killed to make the COVID-19 vaccine."
Error processing example 13173: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The expanded child tax credit in the Build Back Better Act has "no means testing," so people making $200,000 and $400,000 would get "the same as someone making" $70,000.
Error processing example 13174: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Maybe they’re just calling the common cold … the omicron variant."
Error processing example 13175: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "New Zealand okays euthanasia for COVID patients."
Error processing example 13176: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Mask mandates on children lead to learning loss that harms early childhood development."
Error processing example 13177: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In 2021, 65 people in Milwaukee have lost their lives due to reckless driving accidents, and more than 9,000 cars have been stolen, double the amount in 2020."
Error processing example 13178: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It feels amazing" that unvaccinated people are "4.5 times LESS LIKELY to catch omicron" than people who are fully vaccinated and boosted.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▉         | 187/2000 [07:43<35:00,  1.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:   9%|▉         | 188/2000 [07:43<29:52,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 191/2000 [07:44<18:22,  1.64it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 192/2000 [07:45<20:08,  1.50it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 193/2000 [07:45<17:01,  1.77it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 194/2000 [07:45<18:04,  1.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 196/2000 [07:47<18:55,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13179: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "WHO Director-General: The vaccines are being used to kill children."
Error processing example 13180: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A tragic end today" for CNN’s Sanjay Gupta
Error processing example 13181: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Pfizer made $37 billion in profit in its last quarter.
Error processing example 13182: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Yale study: Vaccinated people more likely to be infected than those without the jab."
Error processing example 13183: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A video clip shows a failed attempt by China to launch airstrikes near Taiwan.
Error processing example 13184: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "NASA hired 24 theologians to study human reaction to aliens."
Error processing example 13185: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Jan. 6 was NOT an insurrection…but Nov. 4 at 3 a.m. was!"
Error processing example 13186: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A judge sealed "all the evidence and proof of who helped Ghislaine Maxwell sex traffic children."
Error processing example 13187: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden "never had a" COVID-19 "plan."
Error processing example 13188: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Food crops can be engineered right now based on existing technology to cause infertility in Black people alone."
Error processing example 13189: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Corporate profit margins are at their highest point in 70 years. … They’re overcharging us for gas, medicine, and groceries, and pocketing the difference."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 198/2000 [07:48<18:50,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|▉         | 199/2000 [07:54<50:07,  1.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 200/2000 [07:56<53:34,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 202/2000 [07:56<33:22,  1.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 204/2000 [07:56<22:22,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 205/2000 [07:57<21:38,  1.38it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 206/2000 [07:59<28:58,  1.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13190: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vaccines are why flights have been shut down worldwide.
Error processing example 13191: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Even the WHO has conceded that the (SARS- CoV-2) virus … is no more dangerous than the common flu, with an infection fatality rate of 0.14%."
Error processing example 13192: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 321.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 59.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Germany: 96% of Latest Omicron Patients were FULLY Vaccinated – Only 4% Unvaccinated"
Error processing example 13193: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Tylenol "ruins your ability to kill" COVID-19.
Error processing example 13194: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Chicago maintained the lowest unemployment rate through most of 2021 of any large American city."
Error processing example 13195: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Walgreens refrigerators are scanning shoppers’ hands and foreheads for "the mark of the beast," and that "if you don’t have the mark later on, you won’t be able to buy."
Error processing example 13196: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pfizer CEO: New Pill Will Have a Microchip That Transmits Info Once You Swallow It!"
Error processing example 13197: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When children are given vaccines for COVID-19, "a viral gene will be injected into your children's cells. This gene forces your child’s body to make toxic spike proteins. These proteins often cause permanent damage in children’s critical organs."
Error processing example 13198: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "When I became mayor of the City of Richmond in 2004, the crime rate was at its highest. When I left in 2008, it was at its lowest."
Error processing example 13199: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Data from around the world suggests that omicron favors the fully vaccinated."
Processing examples:  10%|█         | 207/2000 [08:01<38:48,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 208/2000 [08:03<39:58,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 209/2000 [08:04<39:38,  1.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  10%|█         | 210/2000 [08:04<30:18,  1.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 211/2000 [08:04<22:46,  1.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 212/2000 [08:05<24:49,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 213/2000 [08:05<18:49,  1.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 215/2000 [08:11<51:20,  1.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 216/2000 [08:15<1:06:22,  2.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 217/2000 [08:17<59:52,  2.02s/it]  Error processing example 13200: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Electric vehicles would not have fared well in the Virginia snowstorm traffic jam.
Error processing example 13201: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Recently released Jan. 6 footage shows "these cops using massive amounts of force against unarmed Trump supporters, including women."
Error processing example 13202: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A law backed by Virginia Democrats barred local sheriffs from having military equipment that "would have come in very helpful" with the I-95 traffic jam.
Error processing example 13203: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden holds the all-time record for the most money received from Pharma while in Congress! $8,550,422."
Error processing example 13204: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Senior citizens qualify for a Medicare "flex card" that pays for groceries, dental costs and prescriptions.
Error processing example 13205: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "No one has been hospitalized for" the omicron variant of COVID-19.
Error processing example 13206: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Hochul is an "interim Governor."
Error processing example 13207: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Omicron is "the fastest-spreading virus known to humankind."
Error processing example 13208: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Kim Reynolds, touting $210 million for Iowa broadband, "failed to mention these are actually federal funds approved by Rep. Cindy Axne and signed into law by President Joe Biden."
Error processing example 13209: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Governor Kate Brown wants to make Oregon’s mask mandate PERMANENT."
Error processing example 13210: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 219/2000 [08:17<35:43,  1.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 220/2000 [08:18<34:30,  1.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█         | 222/2000 [08:18<21:25,  1.38it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  11%|█▏        | 226/2000 [08:18<10:25,  2.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "If you stop testing (for COVID-19) it all goes away and people just have colds like before."
Error processing example 13211: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Joe Biden said "dozens of police were killed" during the Jan. 6 attack on the U.S. Capitol but "exactly zero people" died that day.
Error processing example 13212: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says he led the fight to add 200 new police officers to make the City of Milwaukee safer.
Error processing example 13213: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden announces Medicare stimulus that provides free dental work to all Seniors who need it! Crowns and implants included."
Error processing example 13214: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Dyson is canceling their contract with Walmart and selling off Dyson Supersonic" hair dryers "only for $1!"
Error processing example 13215: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Japan drops all vaxxine mandates, places myocarditis warning on label."
Error processing example 13216: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We have over 100,000 children, which we’ve never had before, in serious condition, and many on ventilators" due to the coronavirus.
Error processing example 13217: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats "are trying to ban voter ID."
Error processing example 13218: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Ron Johnson has been "rewarding companies that outsource to China."
Error processing example 13219: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Wisconsin voter roll has over 120,000 active voters who have been registered to vote for over 100 years."
Error processing example 13220: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Black Lives Matter administrator was arrested in a 1983 bombing at the U.S. Capitol.
Processing examples:  11%|█▏        | 228/2000 [08:22<23:51,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 231/2000 [08:22<15:35,  1.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 233/2000 [08:24<19:04,  1.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 234/2000 [08:29<40:10,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 236/2000 [08:30<28:13,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 238/2000 [08:30<20:00,  1.47it/s]Error processing example 13221: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Clarence Thomas’s wife was one of the organizers of Jan 6th"
Error processing example 13222: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Washington’s State Board of Health will discuss mandatory COVID-19 quarantines and vaccine requirements for schoolchildren at a Jan. 12 meeting.
Error processing example 13223: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows Alexandria Ocasio-Cortez pouring shots for a group of people "days before testing positive" for COVID-19.
Error processing example 13224: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Carlos Gimenez "wants a national database to track you and discriminate against you."
Error processing example 13225: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "You are 17 times more likely to go to the hospital if you’re not vaccinated, 20 times more likely to die."
Error processing example 13226: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In the 2020 election, President Trump voted from behind the desk in the White House in Florida."
Error processing example 13227: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] As a youth, "I got arrested" protesting for civil rights.
Error processing example 13228: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "From 1917 to 1994, half of the bills that were successfully filibustered in the Senate were civil rights legislation."
Error processing example 13229: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The CDC director just said that 75% of ‘Covid deaths’ occurred in people with at least four comorbidities."
Error processing example 13230: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We’ve brought in more than $81 billion in economic investment, more than four times any previous administration."
Error processing example 13231: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 240/2000 [08:38<49:21,  1.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 241/2000 [08:39<46:59,  1.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 243/2000 [08:40<38:32,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 244/2000 [08:41<33:15,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 246/2000 [08:41<24:05,  1.21it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  12%|█▏        | 248/2000 [08:41<16:33,  1.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Says the "NC appeals court" acknowledged the state’s election maps "are skewed," but didn’t block them because they are "partisan hacks."
Error processing example 13232: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 323.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "India has nearly 1.4 billion citizens. Nearly 800 million of them live in extreme poverty — less than $1.90 per day. Yet EVERY singly (sic) Indian citizen is required to have a valid form of ID to vote in national elections."
Error processing example 13233: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 45.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Images show how climate change destroyed the Greenland ice sheet and harmed a polar bear from 2009 to 2019.
Error processing example 13234: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Fully vaccinated Australians in hospital for COVID-19 surpass unvaccinated"
Error processing example 13235: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump "is fluent in Japanese."
Error processing example 13236: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says he has issued more pardons than "any Wisconsin governor in contemporary history."
Error processing example 13237: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Pfizer CEO Albert Bourla said: "Two doses of the vaccine offers very limited protection, if any."
Error processing example 13238: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID-19-induced pneumonia is "is actually mast cell degranulation of the lungs," a type of allergic reaction.
Error processing example 13239: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In New York state, if you’re white, you have to go to the back of the line to get medical help."
Error processing example 13240: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "People are 7% poorer now because of Biden inflation."
Error processing example 13241: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In 2022, illegal immigrants will have MORE FREEDOMS and easier access to healthcare and ballot boxes than most Americans... Just think about that."
Processing examples:  12%|█▏        | 249/2000 [08:44<26:08,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 251/2000 [08:44<17:37,  1.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 253/2000 [08:45<17:21,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 254/2000 [08:45<14:53,  1.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 256/2000 [08:46<15:29,  1.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 259/2000 [08:46<09:25,  3.08it/s]Error processing example 13242: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Novak Djokovic is the first professional athlete to be banned from a major sporting event for "not taking drugs."
Error processing example 13243: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Dems demand a photo ID to get a cheeseburger."
Error processing example 13244: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Virginia’s economic growth "has stalled at less than 1% per year for eight years."
Error processing example 13245: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Virginia’s "education standards for math and reading are now the lowest in the nation."
Error processing example 13246: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rabbi taken hostage in Texas synagogue is "anti-Zionist" who may have known gunman.
Error processing example 13247: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Sixty percent of (Virginia's) children don’t meet national proficiency standards…"
Error processing example 13248: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Loudoun County School Board spent about $300,000 (last year) … to bring CRT in some form or fashion to the school system."
Error processing example 13249: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] $2,000 stimulus checks were due on Jan. 19.
Error processing example 13250: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Every 37 seconds, someone is arrested for possession of marijuana."
Error processing example 13251: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Supreme Court Justice Sonia Sotomayor tests positive for COVID-19 despite triple vaccination, diligent masking and working from home."
Error processing example 13252: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 261/2000 [08:47<07:13,  4.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 263/2000 [08:52<26:22,  1.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 264/2000 [08:54<36:09,  1.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 266/2000 [08:55<26:53,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 267/2000 [09:00<52:04,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  13%|█▎        | 268/2000 [09:03<54:33,  1.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "There (are) over 800 prison camps in the United States, all fully operational and ready to receive prisoners. They are all staffed and even surrounded by full-time guards, but they are all empty. These camps are to be operated by FEMA."
Error processing example 13253: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID spelled backward is "divoc" which means "possession of the evil spirit" in Hebrew.
Error processing example 13254: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The CDC admits that natural immunity from prior infections is superior to vaccinated immunity alone."
Error processing example 13255: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ashli Babbitt Tried To Stop Antifa False Flag On Jan 6th"
Error processing example 13256: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden's "$4 trillion" Build Back Better bill is "the largest expansion of welfare programs in 60 years."
Error processing example 13257: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "This country’s working people actually got a raise."
Error processing example 13258: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Freedom to Vote Act is "a sprawling takeover of our whole political system."
Error processing example 13259: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If the FAA is worried about what the 5G deployment will do to the planes, what do you think it will do to our bodies?"
Error processing example 13260: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 323.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 59.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Moderna COVID vaccine is no longer recommended due to heart inflammation."
Error processing example 13261: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] List shows "1,000 peer reviewed studies questioning COVID-19 vaccine safety"
Error processing example 13262: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Kathy Hochul’s "record" includes "firing" 34,000 health care workers.
Processing examples:  14%|█▎        | 270/2000 [09:04<39:55,  1.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▎        | 271/2000 [09:05<39:14,  1.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▎        | 274/2000 [09:06<26:20,  1.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 275/2000 [09:06<22:06,  1.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 276/2000 [09:08<24:28,  1.17it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 279/2000 [09:08<15:42,  1.83it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13263: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Build Back Better "would be a huge payday for Chinese manufacturing" and "would fill our streets with cars made with Chinese parts."
Error processing example 13264: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Large, peer-reviewed research study proves ivermectin works" as a COVID-19 preventative.
Error processing example 13265: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Estimated annual COVID-19 deaths in the U.S. totals 27,530, after you subtract those who died "with, not from" COVID-19, not in nursing homes and who didn't have four or more comorbidities.
Error processing example 13266: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Manhattan DA is actually proposing to downgrade armed robbery to a misdemeanor, and to stop prosecuting resisting arrest."
Error processing example 13267: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Clips of Dr. Anthony Fauci show he "flip-flopped" on guidance for wearing masks, whether kids should be in school and whether people should change how they interact.
Error processing example 13268: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The CDC said that 75% of COVID-19 deaths have involved people with at least four comorbidities.
Error processing example 13269: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ashley Hinson voted against the bipartisan infrastructure bill that made this money (for Iowa's locks and dams) possible. Once again she’s taking credit for work she didn’t do."
Error processing example 13270: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A German doctor discovered the COVID-19 vaccines include graphene oxide or graphene hydroxide, and the full list of ingredients is secret because of the emergency use authorization.
Error processing example 13271: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden gave Congress an exemption from vaccine mandate
Error processing example 13272: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "With the ongoing pandemic, our State Park attendance is at the highest it’s ever been."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 282/2000 [09:09<11:43,  2.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 284/2000 [09:09<09:13,  3.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 286/2000 [09:15<31:31,  1.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 287/2000 [09:16<32:04,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  14%|█▍        | 288/2000 [09:17<28:17,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13273: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The New York City Police Department arrested a 9-year-old "because she didn’t have a vaccine card in the museum."
Error processing example 13274: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Total deaths from COVID MUCH LOWER than reported."
Error processing example 13275: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] American taxpayer dollars provide "free housing, free medical, free state identification" to immigrants in the country illegally.
Error processing example 13276: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Iowa, "since we have put a number of the voting laws into place over the last several years — voter ID is one of those — we've actually seen voter participation increase, even in off-election years."
Error processing example 13277: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There’s 150 I believe now – it’s over 100 professional athletes dead, professional athletes, the prime of their life, dropping dead that are vaccinated, right on the pitch, right on the field, right on the court."
Error processing example 13278: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When asked if she supports the Build Back Better bill, Abby Finkenauer said that she didn’t know what was in it.
Error processing example 13279: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 835.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.76 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 59.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vaccine makers "are shipping lots of jabs with varying ingredients, potency & EVEN placebo lots."
Error processing example 13280: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "They call" Jan. 6 "an insurrection," but "were FBI agents used as political agitators?"
Error processing example 13281: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "British data show the COVID shots are an abysmal failure, as COVID infection rates in the U.K. are higher among the ‘fully vaccinated’ in all adult cohorts."
Error processing example 13282: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says remdesivir is responsible for killing patients hospitalized with COVID-19.
Processing examples:  14%|█▍        | 290/2000 [09:18<22:55,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▍        | 291/2000 [09:18<20:33,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▍        | 293/2000 [09:19<19:20,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▍        | 294/2000 [09:20<18:45,  1.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▍        | 297/2000 [09:20<10:48,  2.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13283: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Without a shred of clinical data to support its decision, the Biden Administration has revoked the emergency use authorization for lifesaving monoclonal antibody treatments."
Error processing example 13284: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It begins. Wisconsin Assembly votes to remove electoral votes from Biden."
Error processing example 13285: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A molecule found in a Himalayan fungus "kills cancer cells with 40 times potency."
Error processing example 13286: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden won’t allow Delta-variant COVID patients to get proven and effective monoclonal treatments even though there are 50,000-100,000 Americans infected by Delta EVERY DAY."
Error processing example 13287: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "All these athletes are dropping dead on the field" after receiving the COVID-19 vaccination.
Error processing example 13288: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "More white legacy students get admitted to top universities than Black & Latinx students admitted through affirmative action."
Error processing example 13289: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden-Harris administration "wants to hand out $450,000 to illegal immigrants."
Error processing example 13290: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] ICE has been illegally releasing immigrants "in Huntersville #NC13 at big box retail locations. Feds aren’t notifying anyone, just making secret drops at 3am."
Error processing example 13291: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Thanks to the American Rescue Plan’s changes to the Affordable Care Act, "four out of five consumers (are) finding quality coverage for under $10 a month."
Error processing example 13292: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There's never been a president (before Joe Biden) that has made race and gender the defining factor" for a Supreme Court nomination.
Processing examples:  15%|█▌        | 300/2000 [09:21<09:55,  2.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▌        | 301/2000 [09:21<08:51,  3.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▌        | 304/2000 [09:22<09:18,  3.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▌        | 307/2000 [09:23<06:33,  4.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▌        | 308/2000 [09:26<20:33,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  15%|█▌        | 309/2000 [09:26<17:36,  1.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13293: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Since 2000, the Department of Defense and the Department of Energy have been spraying the entire United States sky with the toxic brew of chemicals and other biologic agents."
Error processing example 13294: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The use of ballot drop boxes and ballot harvesting is illegal."
Error processing example 13295: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Military data from 2021 show military members experienced spikes of 300% in miscarriages, almost 300% in cancer diagnoses and 1,000% in neurological issues.
Error processing example 13296: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ron DeSantis let "millions of COVID-19 tests expire."
Error processing example 13297: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Embalmers finding ‘strange clots’ in jabbed people"
Error processing example 13298: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In England, "official data shows children are up to 52 times more likely to die following COVID-19 vaccination than unvaccinated children."
Error processing example 13299: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The CDC finally admits its COVID-19 test can’t tell the difference between the virus and the flu
Error processing example 13300: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] ‘The Simpsons’ predicted Canada’s trucker convoy protest.
Error processing example 13301: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Over 65% of the country believes that (the) 2020 election was fraudulent. That number was around 35% a year ago."
Error processing example 13302: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Clarence Thomas is about to become the only member of the current Supreme Court who was nominated by a president of one party and confirmed by a Senate controlled by the other party."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 312/2000 [09:26<10:41,  2.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 314/2000 [09:27<08:02,  3.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 316/2000 [09:30<18:45,  1.50it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 317/2000 [09:31<20:49,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 319/2000 [09:31<16:09,  1.73it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13303: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden wants to eliminate a regulation that requires school districts to report allegations of teacher on student rape, alleged rape, and sexual assault."
Error processing example 13304: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Proposed changes to the Electoral Count act show Pence had the power and "could have overturned the election!"
Error processing example 13305: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pennsylvania had 2.6 million mail in votes that the court of appeals of PA held to be unconstitutional. Umm that alone means Biden lost PA."
Error processing example 13306: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Bruce Springsteen, Queen, Pearl Jam and Dave Grohl removed their music from Spotify.
Error processing example 13307: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Facebook is spamming all climate articles by misleading readers about" the accuracy of climate models.
Error processing example 13308: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We all know China created COVID."
Error processing example 13309: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Hundreds of ‘illegals’ were dropped off at Florida hotel.
Error processing example 13310: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Texas, because of the laws they passed, a 95-year-old World War II veteran is being denied the right to vote by mail because he can't produce a registration number he got back in the 1950s."
Error processing example 13311: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Holocaust isn't about race. No, it's not about race."
Error processing example 13312: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republican state Sens. Paul Newton and Warren Daniel "refused to even give testimony" in North Carolina’s latest redistricting case.
Processing examples:  16%|█▌        | 320/2000 [09:33<21:09,  1.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 322/2000 [09:33<14:24,  1.94it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▌        | 323/2000 [09:36<30:02,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▋        | 326/2000 [09:36<16:40,  1.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▋        | 328/2000 [09:42<36:19,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  16%|█▋        | 329/2000 [09:43<32:28,  1.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13313: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It’s not illegal to go inside the Capitol" as the Oath Keepers and other Jan. 6 rioters did.
Error processing example 13314: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Pfizer is "literally using the force of government and the culture of fear to jab children under 5 — with zero basis in science — to make billions of dollars."
Error processing example 13315: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Books banned in Texas include 1984, Maus, and The Handmaid's Tale, but not Mein Kampf."
Error processing example 13316: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Bob Saget predicted his death
Error processing example 13317: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Turkey, Brazil, Britain, Sweden, Spain, Czech Republic, Mexico, El Salvador, Japan and Singapore canceled all COVID-19 quarantine procedures, testing and compulsory vaccination and are considering COVID-19 "just a seasonal flu."
Error processing example 13318: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Iowa Republicans have introduced a bill that would put government-installed cameras in every single classroom to livestream school activities for parents to spy on teachers and children."
Error processing example 13319: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Nevada ranks 50th in election integrity ratings."
Error processing example 13320: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A larger percentage of Illinoisans have been vaccinated than in any other Midwestern state."
Error processing example 13321: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "48-Hour waiting periods limit crimes of passion and keep our communities safe."
Error processing example 13322: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The second booster has eight strains of HIV."
Processing examples:  16%|█▋        | 330/2000 [09:44<35:26,  1.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 333/2000 [09:49<38:31,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 336/2000 [09:49<23:32,  1.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 338/2000 [09:49<17:31,  1.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13323: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Few, If Any, Doses of Spikevax or Comirnaty Available in United States, Despite FDA Approvals"
Error processing example 13324: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Tony Evers has "only gotten one-third of the money meant for COVID relief out the door. He is sitting on $930 million in ARPA funds left unspent. In fact, he still has CARES Act money from two years ago."
Error processing example 13325: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The US has made strides in reducing carbon emissions that other parts of the world have not."
Error processing example 13326: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "As compared to last year, Texas has about 15% more power generation capacity."
Error processing example 13327: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says President Joe Biden "is trying to send taxpayer dollars to manufacturers overseas that do not abide by the same (carbon emission) standards we do at home."
Error processing example 13328: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "New Jersey and Maryland produce more solar power than Florida, the Sunshine State!"
Error processing example 13329: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Federal border officials are allowing undocumented immigrants who are "known criminals who possess an arrest warrant to fly on U.S. aircraft (which) threatens our homeland security."
Error processing example 13330: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Girl Scouts support "Planned Parenthood and pro-abortion politicians."
Error processing example 13331: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Binding masks to your face with gauze and wrapping pantyhose around your face and mask are now "suggestions to make your mask more ‘effective.’"
Error processing example 13332: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID-19 vaccines caused a 40% increase in deaths identified by a life insurance company.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 341/2000 [09:50<15:00,  1.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 342/2000 [09:50<13:09,  2.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 345/2000 [09:51<08:30,  3.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 347/2000 [09:51<06:34,  4.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  17%|█▋        | 349/2000 [09:54<18:55,  1.45it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13333: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We've had two members engage in a Democrat-led persecution of ordinary citizens who engaged in legitimate political discourse."
Error processing example 13334: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "During the first 3 months of FY2022 Customs& Border Protection recorded 518k encounters w illegal immigrants at S border That’s up 137% from same point in FY2021."
Error processing example 13335: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President John F. Kennedy "was planning to end the Federal Reserve."
Error processing example 13336: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Turning Point USA is a white nationalist organization.
Error processing example 13337: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "According to Congress, travel nurses need a pay cap."
Error processing example 13338: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Requesting a chargeback is "a better way to get back at" GoFundMe.
Error processing example 13339: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Got the old killdozer loaded up heading for Ottawa."
Error processing example 13340: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Teachers can only deduct up to $250 for school supplies on their taxes, but billionaires can write off the entire cost of a private jet."
Error processing example 13341: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A grand jury trial begins into crimes against humanity" involving a fabricated coronavirus pandemic.
Error processing example 13342: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden uses the ATF to illegally track your gun transactions."
Error processing example 13343: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 339.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden and most Democrats want to cap insulin prices at $35 per month. All 50 Republicans in the Senate are opposed to it."
Processing examples:  18%|█▊        | 351/2000 [09:58<25:42,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 352/2000 [09:58<21:53,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 353/2000 [09:58<20:07,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 354/2000 [09:59<19:11,  1.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 355/2000 [10:00<26:19,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 358/2000 [10:04<28:33,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 360/2000 [10:04<19:46,  1.38it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13344: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says the Biden administration’s Unaccompanied Alien Children process has "no vetting, no transparency."
Error processing example 13345: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Eric Schmitt "sponsored a bill to spend $480 million of your tax dollars" on cargo hub for airlines "owned by China's Communist Party" and "voted to let China buy up Missouri farmland."
Error processing example 13346: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says a Washington Post headline and graphic about COVID-19 deaths are misleading.
Error processing example 13347: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Arizona House bill to "decertify" 2020 results "could effectively recall the Biden electors."
Error processing example 13348: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Greg Casar passed paid sick leave."
Error processing example 13349: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration "is spending $30 million on crack pipes."
Error processing example 13350: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Comedian Heather McDonald collapsed on stage as a result of the COVID-19 vaccine.
Error processing example 13351: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "President Trump sanctioned Russia. President Biden gave them a pipeline."
Error processing example 13352: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Joe Biden’s America, your children are more likely to have access to a crack pipe than a mask-free education."
Error processing example 13353: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "One in 13 Black Americans are deprived of the right to vote."
Error processing example 13354: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There is absolutely no observed or clinical data that indicates any benefit whatsoever to masking K-12 students in schools."
Processing examples:  18%|█▊        | 362/2000 [10:04<13:59,  1.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 365/2000 [10:08<23:48,  1.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 367/2000 [10:13<34:23,  1.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  18%|█▊        | 369/2000 [10:13<25:13,  1.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▊        | 371/2000 [10:24<1:01:55,  2.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13355: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Millions of hardworking Americans will no longer have to worry about unexpected medical bills."
Error processing example 13356: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "According to Republican Rep. Jeff Shipley, teachers should not receive a pay raise if they teach American history that Republicans don’t agree with."
Error processing example 13357: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The CDC was "normalizing blood clots" by tweeting about the health condition before the Super Bowl.
Error processing example 13358: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Under Greg Abbott’s leadership, Texas is putting taxpayer dollars into Chinese companies."
Error processing example 13359: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Inflation has been caused by the global economies shutting down all at once, reopening all at once. And … the U.S. economy is recovering at a far faster pace than any other country in the OECD."
Error processing example 13360: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 69.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "100,000 Americans died of an overdose in a single year."
Error processing example 13361: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Trudeau declares Emergencies Act amounting to near martial law in Canada."
Error processing example 13362: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Johns Hopkins University confirms: You can be vaccinated with a PCR test, even without knowing"
Error processing example 13363: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says the FDA said that Pfizer "falsely misled" the agency about the safety of the COVID-19 vaccines and that the shots are killing more people than they save.
Error processing example 13364: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 119.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Durham’s documents show that Hillary Clinton hired people who hacked into Trump’s home and office computers" and "planted evidence, fabricated evidence connecting Trump to Russia."
Processing examples:  19%|█▊        | 372/2000 [10:30<1:16:46,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▊        | 373/2000 [10:34<1:26:46,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▊        | 374/2000 [10:35<1:10:52,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 375/2000 [10:36<1:02:53,  2.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 376/2000 [10:38<55:48,  2.06s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 377/2000 [10:39<51:57,  1.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 378/2000 [10:43<1:07:15,  2.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 379/2000 [10:46<1:11:02,  2.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 380/2000 [10:47<54:10,  2.01s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 381/2000 [10:52<1:17:48,  2.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13365: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 369.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.21 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Map shows states where "all of the children… are back to living mask-free, normal lives."
Error processing example 13366: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Grocery chain Aldi will send people who share a Facebook post a "bag full of family necessities" worth $750, and 100 random bags also have a voucher for $500.
Error processing example 13367: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The latest with the Durham report is that the Clinton campaign … actually spied on the president of the United States."
Error processing example 13368: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ivanka Trump charged with major crime."
Error processing example 13369: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The CEO of Moderna deleted his Twitter and dumped $400 million of Moderna stock. Big Pharma seems to know some bad news is coming."
Error processing example 13370: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says that President Joe Biden said Americans will start seeing "direct deposits in their bank accounts this weekend," and that Medicare recipients will get back $2,880.
Error processing example 13371: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "More than half of Virginia’s school buildings are older than 50 years."
Error processing example 13372: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Over-vaccination causes faster mutation of the (COVID-19) virus, which causes a super virus we may not have the ability to fight off."
Error processing example 13373: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Hillary Clinton spied on President Trump."
Error processing example 13374: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 361.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In 2020, more journalists were killed in Mexico than in any other country in the world."
Processing examples:  19%|█▉        | 382/2000 [10:53<1:03:36,  2.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 383/2000 [10:57<1:22:07,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 384/2000 [11:02<1:36:06,  3.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 385/2000 [11:04<1:18:23,  2.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 386/2000 [11:05<1:02:51,  2.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 387/2000 [11:07<1:05:30,  2.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 388/2000 [11:09<1:03:14,  2.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  19%|█▉        | 389/2000 [11:11<53:28,  1.99s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 390/2000 [11:12<49:37,  1.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 391/2000 [11:13<45:09,  1.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13375: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 369.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] There are now zero students, staff or anyone "related to the school system" in Union County" hospitalized with COVID-19, despite harsh criticism of school COVID policy.
Error processing example 13376: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 361.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Centers for Disease Control and Prevention has amassed the "largest collection of human data in history" through COVID-19 PCR tests.
Error processing example 13377: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Virginia is "one of only a handful of states that actually taxes our veterans’ retirement."
Error processing example 13378: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration has been "recklessly releasing potentially dangerous Afghans into our communities."
Error processing example 13379: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Here in Wisconsin, prescription drug costs grew at a pace of nearly twice as much as the average Wisconsinite's income from 2015-2019."
Error processing example 13380: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Trudeau's foundation owns 40% of Acuitas Therapeutics which makes mechanic lipids for Pfizer."
Error processing example 13381: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ukraine was the No. 1 donor to Hillary Clinton when she was running for president."
Error processing example 13382: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United States buys "206,000 barrels of oil a day" from Russia but "shut down domestic oil production a year ago."
Error processing example 13383: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Joe Biden risks war with Russia because Vladimir Putin doesn’t "believe in transgender rights."
Error processing example 13384: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photos show "Ukrainian Christians pray outdoors, in the snow, for their country in this phase of war danger."
Processing examples:  20%|█▉        | 392/2000 [11:20<1:22:29,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 393/2000 [11:24<1:28:19,  3.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 394/2000 [11:24<1:09:20,  2.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 395/2000 [11:29<1:25:38,  3.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 396/2000 [11:31<1:12:31,  2.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 397/2000 [11:35<1:28:20,  3.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 398/2000 [11:40<1:37:22,  3.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|█▉        | 399/2000 [11:41<1:18:25,  2.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 400/2000 [11:46<1:34:52,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 401/2000 [11:48<1:25:40,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 402/2000 [11:50<1:13:28,  2.76s/it]Error processing example 13385: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ottawa may euthanize truckers’ pets as punishment."
Error processing example 13386: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] January 2022 fentanyl seizures show "illicit drugs are flowing into the country at an alarming rate because of Biden’s open border."
Error processing example 13387: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In my first year in office, Aurora homicides went down by 40%."
Error processing example 13388: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The IRS is taking money away from people who received unemployment benefits.
Error processing example 13389: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Russia has never attacked anyone throughout its history."
Error processing example 13390: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 363.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Bill Gates hatches 'horribly stupid' plan to block out the sun"
Error processing example 13391: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "News report accidentally reveals doctors giving ‘banned’ COVID treatment to the Queen."
Error processing example 13392: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Republican leadership has not been clear about whether they support Rep. Ramthun’s illegal and undemocratic resolution" on the 2020 election.
Error processing example 13393: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 359.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 59.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ron Johnson pushed through a special tax loophole that benefited his own family's business … Then he cashed out of the company for $5 million … (he has) doubled his wealth since taking office."
Error processing example 13394: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida is "49th in the nation when it comes to (Medicaid) funding" for people with disabilities.
Error processing example 13395: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 403/2000 [11:51<58:02,  2.18s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 404/2000 [11:52<47:46,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 405/2000 [11:53<44:24,  1.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 406/2000 [11:56<54:33,  2.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 407/2000 [11:58<54:28,  2.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 408/2000 [11:59<47:03,  1.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 409/2000 [12:01<47:59,  1.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  20%|██        | 410/2000 [12:04<57:31,  2.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 411/2000 [12:07<58:44,  2.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 412/2000 [12:10<1:09:18,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 413/2000 [12:12<1:01:49,  2.34s/it]
[CLAIM] "NATO (under direction from the United States) is violating previous agreements and expanding eastward."
Error processing example 13396: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden "has been secretly flying illegal immigrants into communities across the country in the middle of the night."
Error processing example 13397: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In the U.S., myocarditis cases among ages 12 to 20 numbered four in 2019; four in 2020; and 2,236 in 2021.
Error processing example 13398: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We have doubled our (oil) imports from Russia in the last year."
Error processing example 13399: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Gov. Tony Evers brought Republicans and Democrats together to cut income taxes for the middle class."
Error processing example 13400: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It was revealed in court" on Feb. 23, 2022 "that Tory Lanez DNA WAS NOT found on the weapon in the Meg Thee Stallion case."
Error processing example 13401: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Russia is targeting U.S. biological weapons labs in Ukraine invasion.
Error processing example 13402: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ethnic Russians face "genocide perpetrated by the Kyiv regime."
Error processing example 13403: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ukrainian fighter-ace known as the 'Ghost of Kyiv's real name is Samuyil Hyde."
Error processing example 13404: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 2020 election "was stolen from Donald J. Trump."
Error processing example 13405: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Keystone XL pipeline "would have produced 830,000 barrels of oil per day, more than enough to offset what we import from Russia."
Error processing example 13406: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 414/2000 [12:15<1:07:25,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 415/2000 [12:20<1:26:00,  3.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 416/2000 [12:23<1:23:12,  3.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 417/2000 [12:29<1:44:10,  3.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 418/2000 [12:34<1:53:36,  4.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 419/2000 [12:38<1:50:20,  4.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 420/2000 [12:42<1:53:26,  4.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 421/2000 [12:45<1:41:44,  3.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 422/2000 [12:47<1:24:25,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██        | 423/2000 [12:50<1:26:15,  3.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] A photo showing young children saluting troops was captured in Ukraine in 2022.
Error processing example 13407: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photos show Ukrainian citizens destroying two Russian tanks with Molotov cocktails in Kyiv.
Error processing example 13408: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 363.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We are No. 1 among all large states in having fully vaccinated teenagers. We are No. 2 in all large states for having fully vaccinated five- to 11- year-olds."
Error processing example 13409: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo of injured Ukrainian woman was from 2018 Russia gas explosion.
Error processing example 13410: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 365.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The former Miss Universe-Ukraine, Anastasiia Lenna, turned in her high heels for some combat boots to fight for her country."
Error processing example 13411: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 361.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Image shows Ukrainian President Volodymyr Zelensky "has joined the front lines with his troops."
Error processing example 13412: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Actor Steven Seagal spotted among Russian special forces in Ukraine
Error processing example 13413: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 363.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "I’ve been in and out of Iraq and Afghanistan over 40 times."
Error processing example 13414: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There is no evidence Biden’s spending produced one job. Nonpartisan projections for job growth in 2021, not including that American Rescue Plan, showed higher job growth than Biden produced."
Error processing example 13415: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ukraine issued a press release about Joe Biden
Error processing example 13416: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Our economy created over 6.5 million new jobs just last year, more jobs in one year than ever before in the history of the United States of America."
Processing examples:  21%|██        | 424/2000 [12:55<1:40:43,  3.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██▏       | 425/2000 [12:57<1:24:46,  3.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██▏       | 426/2000 [13:00<1:21:46,  3.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██▏       | 427/2000 [13:05<1:37:59,  3.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██▏       | 428/2000 [13:06<1:14:16,  2.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  21%|██▏       | 429/2000 [13:07<1:01:38,  2.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 430/2000 [13:09<54:46,  2.09s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 431/2000 [13:12<1:07:26,  2.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 432/2000 [13:13<54:18,  2.08s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 433/2000 [13:14<45:56,  1.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 434/2000 [13:15<39:22,  1.51s/it]Error processing example 13417: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 361.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden's sanctions on Russia "are riddled with loopholes and don't even start for 30 days. They have carve outs for the energy and financial sectors."
Error processing example 13418: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Russian army does not occupy Ukrainian territory."
Error processing example 13419: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "NEW: Biden Admin. instructs Americans to social distance and wear masks in the event of a Nuclear Fallout."
Error processing example 13420: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 363.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The drug labels for the Pfizer COVID-19 vaccine "were blank when they should have contained all these diseases and adverse events" listed in a confidential report.
Error processing example 13421: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wearing masks is "COVID theater" and "not doing anything."
Error processing example 13422: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Donald Trump's appeasement of Putin wasn't just a personal act of treason, it's the Republican Party's official position."
Error processing example 13423: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Putin Vows to ‘Crush’ Child Traffickers in Ukraine"
Error processing example 13424: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "New York state has already lost 99% of its historic freshwater wetlands."
Error processing example 13425: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "And just like that, your $15/hr doesn’t buy you as much as $7.50/hr did a year ago."
Error processing example 13426: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] CDC Director Dr. Rochelle Walensky said she gets COVID-19 reports from CNN.
Error processing example 13427: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 435/2000 [13:18<50:32,  1.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 436/2000 [13:20<46:36,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 437/2000 [13:21<43:41,  1.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 438/2000 [13:22<40:07,  1.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 439/2000 [13:24<40:02,  1.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 440/2000 [13:25<40:44,  1.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 441/2000 [13:28<48:15,  1.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 442/2000 [13:34<1:22:17,  3.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 443/2000 [13:37<1:16:59,  2.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 444/2000 [13:42<1:32:25,  3.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "As the world rallies around Ukraine, Mike Lee was one of only two senators to oppose sanctions on Putin. Then he flew to the Kremlin and discussed dropping sanctions. Lee even opposed arming Ukrainians fighting for their lives."
Error processing example 13428: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Two years ago we were "drilling our own oil for $27 a barrel." Now, thanks to Joe Biden, we’re "paying $105 a barrel to Russia."
Error processing example 13429: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California has instituted a "bacon ban."
Error processing example 13430: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Marco Rubio is following his party bosses, like Rick Scott, with his plan to raise taxes on Florida's working families, retirees and veterans."
Error processing example 13431: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Gov. Tony Evers "tried to make us pay even more at the pump by hiking the gas tax."
Error processing example 13432: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. imports 1% of its oil from Russia while "Exxon, Chevron, BP and Shell profits are at their highest level in over 7 years."
Error processing example 13433: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden's cancellation of the Keystone pipeline "dramatically increased Americans' dependence on Russian oil."
Error processing example 13434: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Five Wisconsin cities received money for the 2020 election from Mark Zuckerberg, which amounted to "a wave of massive election bribery."
Error processing example 13435: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 363.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Gas is high because they shut down production in the U.S." under the Biden administration.
Error processing example 13436: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "While Ukrainians bled and died, Congressman Budd excused their killer."
Error processing example 13437: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 363.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 1975, then-U.S. Sen. Joe Biden opposed efforts by President Gerald Ford to aid South Vietnam and evacuate refugees.
Processing examples:  22%|██▏       | 445/2000 [13:44<1:23:09,  3.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 446/2000 [13:45<1:08:01,  2.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 447/2000 [13:46<57:13,  2.21s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 448/2000 [13:49<1:00:47,  2.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▏       | 449/2000 [13:50<46:55,  1.82s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  22%|██▎       | 450/2000 [13:50<36:59,  1.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 451/2000 [13:53<51:06,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 452/2000 [13:56<54:36,  2.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 453/2000 [14:01<1:21:22,  3.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 454/2000 [14:04<1:18:38,  3.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13438: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The oil industry has "9,000 permits to drill now. They could be drilling right now, yesterday, last week, last year."
Error processing example 13439: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Almost half of every dollar that we spend on gas, it goes to New York State."
Error processing example 13440: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] China: U.S. has biological weapons labs in Ukraine.
Error processing example 13441: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Gov. Tony Evers gave $2.4 million in COVID relief money to Planned Parenthood affiliates throughout Wisconsin "to bail them out and fund abortions."
Error processing example 13442: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Classified documents from Ukraine "confirm without a shadow of a doubt" that the Ukraine government was "covertly preparing an offensive operation against Donbas, scheduled for March 2022."
Error processing example 13443: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "People who have had cancer in the past, they get the COVID jab and now, they're getting cancer two to three, four months later and it's the same cancer they had except much worse."
Error processing example 13444: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "David McCormick paid for attacks on Donald Trump."
Error processing example 13445: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Vice President Kamala Harris laughs at the plight of Ukraine; at idea of America offering assistance"
Error processing example 13446: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 365.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mo Brooks "voted to cut off funding to destroy ISIS terrorists in the middle of the fight."
Error processing example 13447: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Russian airstrike on a maternity hospital in Mariupol, Ukraine, had "the makings of yet another false flag operation" led by the U.S.
Processing examples:  23%|██▎       | 455/2000 [14:06<1:09:07,  2.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 456/2000 [14:08<1:05:40,  2.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 457/2000 [14:14<1:30:46,  3.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 458/2000 [14:16<1:18:51,  3.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 459/2000 [14:21<1:32:26,  3.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 460/2000 [14:31<2:20:08,  5.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 461/2000 [14:36<2:14:13,  5.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 462/2000 [14:45<2:49:16,  6.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 463/2000 [14:50<2:36:57,  6.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 464/2000 [14:53<2:10:46,  5.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 465/2000 [14:56<1:51:11,  4.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13448: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Jan. 6 defendants "are being held in prison without being charged."
Error processing example 13449: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows "Russian soldiers using weapons that have been in a cupboard since Soviet times."
Error processing example 13450: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 365.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.22 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Beto O'Rourke's Plan to Take 'Trans' Children from Parents."
Error processing example 13451: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "She was asked about gasoline prices ... and she tried to defer to President Duda, the Polish president."
Error processing example 13452: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 877.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The presence of apes calls into question the concept of evolution.
Error processing example 13453: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 889.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.71 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A West Virginia inspection sticker on an Uber" proves the car was not in Ukraine.
Error processing example 13454: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Biden "wants to buy lithium from China for electric cars."
Error processing example 13455: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 115.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.46 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Make no mistake: The current spike in gas prices is largely the fault of Vladimir Putin."
Error processing example 13456: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wladimir Klitschko has auctioned off his 1996 Atlanta Olympic Games gold medal to raise money for the children of Ukraine."
Error processing example 13457: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Documents were published confirming Moderna created the COVID-19 Virus."
Error processing example 13458: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Comparing the price of oil and gas in June 2008 to March 2022 shows that oil companies are "price gouging."
Processing examples:  23%|██▎       | 466/2000 [14:56<1:18:48,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  23%|██▎       | 467/2000 [14:57<1:03:01,  2.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▎       | 470/2000 [14:57<28:01,  1.10s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▎       | 471/2000 [15:02<50:03,  1.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▎       | 472/2000 [15:02<39:26,  1.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▎       | 473/2000 [15:02<30:41,  1.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 475/2000 [15:06<34:54,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 476/2000 [15:06<29:23,  1.16s/it]Error processing example 13459: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Virginia women are paid 80 cents for every dollar paid to Virginia men."
Error processing example 13460: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "What is happening here is not so different from what we're seeing happening in Russia, where you have got state TV and controlled messaging across the board."
Error processing example 13461: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "As soon as Biden took office, he eliminated all subsidies for fossil fuels."
Error processing example 13462: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A recent on-air protest can’t be real because "there are no live on air television broadcasts in Russia. None. Ever."
Error processing example 13463: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ukrainian President Volodymyr Zelensky wore a Nazi cross symbol.
Error processing example 13464: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 853.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.74 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Ron Johnson "is supporting the Republican plan that phases out Social Security and Medicare."
Error processing example 13465: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The price of insulin has been jacked up by Big Drug Corporations."
Error processing example 13466: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "David McCormick fired Pennsylvanians and bragged about shipping their jobs to Asia."
Error processing example 13467: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In April 1997, there was a ‘gas out’ conducted nationwide in protest of gas prices. Gasoline prices dropped 30 cents a gallon overnight."
Error processing example 13468: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United Nations sent out an email instructing staff not to use the words "invasion" or "war’" when referring to Ukraine.
Error processing example 13469: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 478/2000 [15:07<19:59,  1.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 481/2000 [15:08<15:50,  1.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 484/2000 [15:08<09:56,  2.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 486/2000 [15:09<10:03,  2.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Gas prices are high "due to the Democrats’ war on fossil fuels."
Error processing example 13470: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Gas prices have been going up since the president’s first day in office, starting with his move to shut down the Keystone XL pipeline."
Error processing example 13471: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Space Foundation has erased the honors previously bestowed on Yuri Gagarin, the first man to ever be in space. His name was stripped ‘in light of current events.’"
Error processing example 13472: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Virginia Tech swimmer Reka Gyorgy tweeted, "My finals spot was stolen by Lia Thomas, who is a biological male."
Error processing example 13473: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Judge Ketanji Brown Jackson "gave child porn offenders sentences below the guidelines and below what the prosecutors were requesting," showing she is "soft on child pornography offenders."
Error processing example 13474: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Only 2% of K-12 students would benefit from Iowa’s school voucher bill.
Error processing example 13475: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Judge Jackson doesn’t think perverts should have to register (as sex offenders). She says it ‘stigmatizes’ THEM."
Error processing example 13476: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] At the start of the COVID-19 pandemic, Judge Ketanji Brown Jackson advocated that "each and every criminal defendant in D.C. Corrections custody should be released."
Error processing example 13477: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pat McCrory appointed the ‘Republican’ judge who sided with Democrats in the partisan Democrat lawsuit/power-grab over redistricting."
Error processing example 13478: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ketanji Brown Jackson "says she gave pedophiles lighter sentences (because) it’s different when they use computers vs mail to get volumes of child porn. This makes ‘total sense’ according to Jackson."
Error processing example 13479: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ketanji Brown Jackson "gave a lecture talking about critical race theory as one of the components to consider when you are making decisions on the bench."
Processing examples:  24%|██▍       | 487/2000 [15:09<10:17,  2.45it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  24%|██▍       | 490/2000 [15:09<06:36,  3.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▍       | 492/2000 [15:10<05:18,  4.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▍       | 494/2000 [15:11<07:11,  3.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▍       | 497/2000 [15:11<04:47,  5.23it/s]Error processing example 13480: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Judge Ketanji Brown Jackson called former President George W. Bush and former Defense Secretary Donald Rumsfeld as "war criminals in a legal filing."
Error processing example 13481: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo shows a Home Depot worksheet on "privilege" used to train its employees.
Error processing example 13482: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ketanji Brown Jackson "believes in abortion on demand up to the moment of birth."
Error processing example 13483: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A baby’s immune system can’t handle antigens from vaccines, because new food must be introduced slowly to identify allergic reactions.
Error processing example 13484: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you look at windmills" and the greenhouse gas emissions from building to retiring them, "the overall return is negative."
Error processing example 13485: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In cutting greenhouse gas emissions, the United States is "the leader in the world by far."
Error processing example 13486: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ketanji Brown Jackson has "a weak record — defund police, abolish ICE and now a completely open border policy."
Error processing example 13487: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Amy Coney Barrett overturned a $6.7M verdict after a prison guard repeatedly raped a 19-year-old inmate who was 8 months pregnant."
Error processing example 13488: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 2021 Georgia Senate runoff and the 2020 presidential election "were stolen."
Error processing example 13489: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration is attempting "to criminalize free speech."
Error processing example 13490: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▍       | 499/2000 [15:13<11:23,  2.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▌       | 501/2000 [15:13<08:44,  2.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▌       | 503/2000 [15:18<23:48,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▌       | 505/2000 [15:20<22:48,  1.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▌       | 506/2000 [15:20<21:38,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▌       | 507/2000 [15:21<17:56,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Republicans "nominated the 1st Black woman to the SCOTUS & she was BLOCKED & filibustered by… Joe Biden."
Error processing example 13491: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Mike Gibbons said "military service doesn't count as real work."
Error processing example 13492: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] According to the Census Bureau, "5 million more people voted than were registered to vote" in the 2020 election.
Error processing example 13493: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "As your Lieutenant Governor…I cut your taxes."
Error processing example 13494: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Over the course of two months, Tony Evers refused to remove (Curtis Schmitt Jr.) from the (state’s veterans) Board despite having the authority to do (so)."
Error processing example 13495: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Pfizer staged the Will Smith and Chris Rock confrontation at the Academy Awards because it "has a new alopecia medication coming out."
Error processing example 13496: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "You go to a store, they don’t have bread."
Error processing example 13497: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A secret backroom deal" gave "away hundreds of millions of tax dollars to a Soros-owned company."
Error processing example 13498: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Cities in notoriously ‘tough on crime’ states like Texas and Florida saw a more significant increase in violent crime than cities in NYS."
Error processing example 13499: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Senate Republicans’ plan" would "end Social Security" and "end Medicare."
Error processing example 13500: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Want a healthy, happy baby? Skip the Well Baby visits. They… make your baby sick."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  25%|██▌       | 509/2000 [15:21<12:01,  2.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 511/2000 [15:23<19:03,  1.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 512/2000 [15:23<16:02,  1.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 514/2000 [15:24<10:48,  2.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 515/2000 [15:26<18:53,  1.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 517/2000 [15:26<12:28,  1.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13501: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We have (people detained by ICE) that have to be seen within 24 hours, while veterans are seen within months.
Error processing example 13502: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Democrat’s New Plan: Tax billionaires to lower the deficit by $1 TRILLION and fund clean energy independence. Republican’s New Plan: Raise taxes on households earning less than $50,000 by an ADDITIONAL $4,500 after CUTTING taxes for the rich."
Error processing example 13503: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "California introduces new bill that would allow mothers to kill their babies up to 7 days after birth."
Error processing example 13504: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The price of insulin increases as waistlines increase."
Error processing example 13505: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Strategic Petroleum Reserve has "been mostly empty" for decades.
Error processing example 13506: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Mike Lee advised Trump's legal challenges to overturn our election" and "was one of only two senators who was in on the scheme."
Error processing example 13507: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ron Johnson is trying to let insurance companies deny coverage to people with pre-existing conditions like cancer, depression, pregnancy, diabetes—or even COVID."
Error processing example 13508: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 2021, "11,201 pounds of fentanyl were seized by Customs and Border Protection, which is enough to kill every American nearly seven times over."
Error processing example 13509: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] John D. Rockefeller coined the term "fossil fuel" to trick people into thinking that his product was scarce and drive up the price when fossil fuels are actually "the second most abundant liquid on the planet."
Error processing example 13510: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The House voted to cap insulin prices to $35/month, just $420 each year! Richard Hudson voted NO."
Processing examples:  26%|██▌       | 518/2000 [15:30<30:43,  1.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 519/2000 [15:30<24:15,  1.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 520/2000 [15:31<26:29,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 521/2000 [15:32<25:43,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 522/2000 [15:33<21:19,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▌       | 524/2000 [15:33<12:53,  1.91it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▋       | 525/2000 [15:33<10:25,  2.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  26%|██▋       | 526/2000 [15:34<14:35,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13511: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] 2004 video shows Anthony Fauci is ignoring science on natural immunity with regard to COVID-19.
Error processing example 13512: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former Gov. Pat McCrory appointed liberals to the North Carolina Textbook Commission, which "mandated textbooks" pushing critical race theory.
Error processing example 13513: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Pfizer paid "$2.8 million bribe payment" to the FDA for COVID-19 vaccine approval.
Error processing example 13514: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden changes his mind and now wants to resume building Trump’s wall."
Error processing example 13515: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The economy was "dead in the water when we got here. Virtually no jobs created."
Error processing example 13516: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A video taken from a car driving through Bucha, Ukraine, shows a corpse "moving his arm," and then "in the rear view mirror the ‘corpse’ sits down."
Error processing example 13517: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sudden death in adults and Sudden Infant Death Syndrome are caused by vaccines.
Error processing example 13518: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Schools are putting litter boxes in bathrooms to accommodate kids who identify as furries.
Error processing example 13519: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says New Zealand Prime Minister Jacinda Ardern said her country doesn’t have to deal with the "rage of older white men" because "we’ve never allowed Rupert Murdoch to set up a media outlet here."
Error processing example 13520: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The proposed (Virginia) House budget cuts hundreds of millions in funding for public education…"
Processing examples:  26%|██▋       | 528/2000 [15:34<09:17,  2.64it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 531/2000 [15:37<14:58,  1.64it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 533/2000 [15:37<10:59,  2.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 536/2000 [15:37<07:11,  3.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 537/2000 [15:43<28:35,  1.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13521: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Twitter employee named Jackson Mulholland resigned in a tweet after Elon Musk joined the company’s board.
Error processing example 13522: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Sens. Lisa Murkowski, Susan Collins and Mitt Romney "are pro-pedophile."
Error processing example 13523: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The FBI lost Hunter Biden’s laptop.
Error processing example 13524: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mehmet Oz "called for a New Zealand-style gun ban" and said "Americans should have less access to guns."
Error processing example 13525: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden’s open border" means that there are "more Democrat voters pouring into this country."
Error processing example 13526: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Josh Mandel doctored an image in a campaign ad but "forgot to Photoshop the hands that clearly show he put his face on the body of a Black soldier."
Error processing example 13527: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "With the money Elon Musk spent to buy Twitter shares, he could have given every American family $100,000 and still had enough left over to cancel all student debt."
Error processing example 13528: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "While the initial increase in oil prices resulted in a near instantaneous increase in gas prices for consumers, the subsequent decrease in crude oil prices has failed to meaningfully provide relief for Georgia's families at the pump."
Error processing example 13529: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Colorado Governor Jared Polis has just signed into law a bill legalizing abortions through all nine months, up until the moment of birth."
Error processing example 13530: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "John Fetterman's a self-described democratic socialist."
Processing examples:  27%|██▋       | 538/2000 [15:43<25:34,  1.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 541/2000 [15:44<16:17,  1.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 542/2000 [15:45<18:05,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 544/2000 [15:46<14:41,  1.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 546/2000 [15:49<23:22,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 547/2000 [15:49<19:35,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  27%|██▋       | 548/2000 [15:50<20:50,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13531: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York City is "wasting taxpayer money on" billboards in Florida.
Error processing example 13532: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Suggests ivermectin is an effective treatment for COVID-19
Error processing example 13533: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Poland palace that Biden visited is fake.
Error processing example 13534: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "German perverts open bestiality brothels."
Error processing example 13535: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video of a Q&A response from Amazon’s Alexa confirms that chemtrails are being used "to reduce the human population."
Error processing example 13536: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Most Americans favor keeping Title 42 in place."
Error processing example 13537: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "As Ohio treasurer, Josh Mandel loaned your money to Chinese business interests, and Mandel opposed a bill holding Communist China accountable when they cheated and stole American jobs."
Error processing example 13538: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If Wisconsin Gov. Tony Evers "had passed his (gas) tax increases ... fuel prices would be at least a dime higher.
Error processing example 13539: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Brian Kemp "dismissed concerns about voter fraud in the 2020 election" and "widespread illegal ballot harvesting continued, electing two Democrat senators."
Error processing example 13540: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Disney+ has had over 350,000 cancellations in the last five days.
Error processing example 13541: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "They’re going to be giving these people who come across the border, giving them smartphones."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 550/2000 [15:50<13:30,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 552/2000 [15:51<09:22,  2.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 553/2000 [15:54<24:21,  1.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 555/2000 [15:55<18:06,  1.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 556/2000 [15:55<16:44,  1.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 557/2000 [15:56<16:06,  1.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 558/2000 [15:57<16:58,  1.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13542: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The chairman of Virginia’s Republican Party made a "shockingly racist post" about Defense Secretary Lloyd Austin.
Error processing example 13543: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you earn $100 and pay $33 income tax you’re left with $67. You then buy $67 worth of fuel and in doing so pay a 48% fuel tax (fuel tax = $32.16 + $6.70 GST). This means that the government just got $71.86 tax from your hard earned $100."
Error processing example 13544: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] George Soros injected "$33 million" into Black Lives Matter.
Error processing example 13545: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In a recent year, the Milwaukee Police Department "collected more guns per capita off the streets than in New York."
Error processing example 13546: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It’s been proven that sugar triggers and causes cancer. … So the best thing to do to prevent cancer is to avoid sugar."
Error processing example 13547: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The COVID-19 vaccines contain "HIV lipid wrappers."
Error processing example 13548: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden is giving smartphones away to illegal border crossers."
Error processing example 13549: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Just passed into law today: You cannot go 5 miles over speed limit" in North Carolina.
Error processing example 13550: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID-19 is a synthetic version of "snake venom" that evil forces are spreading through remdesivir, the COVID-19 vaccines and drinking water to "make you a hybrid of Satan."
Error processing example 13551: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] March 2022 moon crash was orchestrated by the government and visible from Earth.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 560/2000 [15:57<12:21,  1.94it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 561/2000 [15:57<10:06,  2.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 562/2000 [15:59<19:02,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 564/2000 [15:59<11:55,  2.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 566/2000 [15:59<08:08,  2.94it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 567/2000 [16:01<16:42,  1.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  28%|██▊       | 569/2000 [16:02<10:56,  2.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13552: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cannabis reform "is supported by the majority of the residents of our state, including a majority of Republicans."
Error processing example 13553: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Reddit post shows a parent admitting to forcing their child to continue transitioning by sneaking hormone medication.
Error processing example 13554: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Brooklyn subway attack was a false flag.
Error processing example 13555: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "New York leads the U.S. in population loss."
Error processing example 13556: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says he has "always" believed "in a woman’s right to choose."
Error processing example 13557: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A bird pooped on Joe Biden during his speech" in Iowa.
Error processing example 13558: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pfizer tries to hide 2.4K full-time hires handling vax injuries."
Error processing example 13559: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Inflation has gone up every month of the Biden presidency and just hit another 40-year high."
Error processing example 13560: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Dave McCormick is "liberal, pro-Biden, pro-China."
Error processing example 13561: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Fuel containing 15% ethanol will ruin a car engine.
Error processing example 13562: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "J.D. Vance called for higher taxes."
Processing examples:  28%|██▊       | 570/2000 [16:04<20:28,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▊       | 572/2000 [16:04<13:17,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▊       | 574/2000 [16:04<09:07,  2.61it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▉       | 577/2000 [16:07<15:32,  1.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▉       | 579/2000 [16:13<30:15,  1.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13563: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Leftist justice reform" is the reason that the NYC subway shooting suspect wasn’t in jail at the time of the attack.
Error processing example 13564: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ukraine was responsible for the Kramatorsk train station bombing.
Error processing example 13565: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden's spending "has sent prices skyrocketing."
Error processing example 13566: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The state crime lab under Attorney General Josh Kaul "is testing significantly less items than former AG Brad Schimel and is still taking longer to test many categories of key items in comparison to Schimel, including DNA."
Error processing example 13567: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "For four years, I was a full professor at the University of Pennsylvania."
Error processing example 13568: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "America (is) trying to overthrow the Ukraine government just as the shadow government has infiltrated and overthrown the US government."
Error processing example 13569: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "No one visiting Disney can get in" because of protest.
Error processing example 13570: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] MSNBC’s Mika Brzezinski said, "Elon Musk is trying to control how people think. That’s our job."
Error processing example 13571: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The President of Ukraine owns a $35 million home in Florida and has $1.2 billion in a overseas bank account."
Error processing example 13572: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 343.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Bath and Body Works products are not recommended during pregnancy because they are suspected of "damaging fertility or the unborn child."
Processing examples:  29%|██▉       | 580/2000 [16:17<43:10,  1.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▉       | 582/2000 [16:17<29:28,  1.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▉       | 585/2000 [16:19<21:05,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▉       | 586/2000 [16:20<21:52,  1.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  29%|██▉       | 589/2000 [16:20<14:27,  1.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13573: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Australian Technical Advisory Group on Immunisation advised that the COVID-19 vaccine may be given to patients under sedation for unrelated procedures without consent as a way to force compliance.
Error processing example 13574: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Red Cross sells your donated blood to hospitals for $150 and then that hospital charges you thousands for a blood transfusion."
Error processing example 13575: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Finnish Court rules sex with children is permitted."
Error processing example 13576: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Images show the Ukrainian flag being replaced by the Soviet Union flag in Mariupol in 2022.
Error processing example 13577: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Gov. Tony Evers "wanted to increase your taxes by $1 billion just for heating your homes. Instead, Republicans cut your taxes by more than $2 billion."
Error processing example 13578: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] After Russia invaded Ukraine, Google Maps "opened all military and strategic facilities in Russia."
Error processing example 13579: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Dr. Anthony Fauci said lockdowns are a method for coercing people to comply with COVID-19 vaccinations.
Error processing example 13580: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Since taking office, Hochul has passed more than 400 new bills
Error processing example 13581: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Matt Dolan sponsored Michael Bloomberg's "red-flag gun confiscation law, letting the people reporting your social media posts send SWAT teams to your house to take your guns."
Error processing example 13582: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Tylenol is the #1 cause of acute liver failure in the US with 27% of people dying & kills at least 100,000 per year from its use."
Processing examples:  30%|██▉       | 590/2000 [16:21<14:04,  1.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|██▉       | 591/2000 [16:22<18:31,  1.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|██▉       | 594/2000 [16:23<11:49,  1.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|██▉       | 597/2000 [16:23<08:31,  2.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|██▉       | 599/2000 [16:24<08:09,  2.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 600/2000 [16:29<28:09,  1.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13583: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "DeSantis erases Disney’s tax exempt law. Will cost Disney $200 Mil in taxes. Per year!"
Error processing example 13584: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Brian Kemp and Secretary of State Brad Raffensperger "perhaps in collusion with the Radical Left Democrats" are attempting to "unseat" U.S. Rep. Marjorie Taylor Greene.
Error processing example 13585: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mehmet Oz "was a spokesman for a group who wanted to defund the police."
Error processing example 13586: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Hepatitis outbreak in kids may be related to Johnson & Johnson COVID-19 vaccine.
Error processing example 13587: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Fires at food processing plants are an "attempt to starve us."
Error processing example 13588: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Videos show that Ukraine President Volodymyr Zelenskyy uses cocaine.
Error processing example 13589: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Paying taxes is optional!!"
Error processing example 13590: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Under Greg Abbott, property taxes have gone up $20 billion."
Error processing example 13591: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Canada’s new law allows government to take children away if parents don’t accept kids’ ‘gender identity.’"
Error processing example 13592: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A consent decree approved by Gov. Brian Kemp "invalidated all voter ID law" and "allowed fraudulent ballots to be accepted" in the 2020 race.
Error processing example 13593: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 341.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Congratulations to Ruben, knighted by the Queen. Now goes by the name, Sir Ruben."
Processing examples:  30%|███       | 601/2000 [16:31<30:52,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 602/2000 [16:31<25:01,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 603/2000 [16:33<27:26,  1.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 604/2000 [16:37<44:04,  1.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 606/2000 [16:39<32:53,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 607/2000 [16:39<25:58,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  30%|███       | 609/2000 [16:39<16:21,  1.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13594: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A STOLEN ELECTION: State totals minus illegal ballot trafficking numbers give President Trump decisive victories in AZ, GA, MI, PA, and WI."
Error processing example 13595: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Blinken, Austin trip to Kyiv may have been staged
Error processing example 13596: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Mark Kelly voted "for benefits to illegals."
Error processing example 13597: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "With 40 billion dollars, Elon Musk could have given each of the 330M people living in America a million dollars and still had $7B left over."
Error processing example 13598: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Title 42 has fueled the border crisis. In 2019, 7 percent of illegal border crossings were repeated crossings. Today the rate is 27 percent."
Error processing example 13599: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "mRNA Vaccines Show No Mortality Benefit - Danish Study"
Error processing example 13600: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When there is "a noose on a college dorm of a Black student" or a racial slur "on a dormitory building, the odds are overwhelming that a Black student actually did that."
Error processing example 13601: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says J.D. Vance said, "People who voted for Trump voted for him for racist reasons."
Error processing example 13602: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden to teachers: ‘They're not somebody else's children. They're yours when you're in the classroom.’"
Error processing example 13603: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Conservatives gained Twitter followers after the company announced its deal with Elon Musk because "Twitter lifted a broad anti-conservative, anti-Trump shadow ban."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 612/2000 [16:41<17:29,  1.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 615/2000 [16:41<11:22,  2.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 616/2000 [16:46<26:10,  1.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 617/2000 [16:47<23:26,  1.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 618/2000 [16:50<37:20,  1.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 620/2000 [16:51<23:48,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 621/2000 [16:51<19:18,  1.19it/s]Error processing example 13604: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Kari Lake has been appearing at rallies with neo-Nazis," while "two federal juries have found" Katie Hobbs "guilty of racial discrimination."
Error processing example 13605: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We have 76% of Americans living paycheck to paycheck, meaning they are one paycheck away from poverty."
Error processing example 13606: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The tax carve out (Ron) Johnson spearheaded overwhelmingly benefited the wealthiest, over small businesses."
Error processing example 13607: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The student debt that is out there, almost 60% of it is graduate school debt."
Error processing example 13608: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Texas, Republicans passed a law allowing rapists to sue their victims for getting an abortion."
Error processing example 13609: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 345.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "CO2 is not a pollutant."
Error processing example 13610: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Space X’s Starlink internet constellation has angered Russia as it was reported that the Starlink satellite constellation was used to guide the fire" on the Russian ship Moskva.
Error processing example 13611: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Gov. Tony Evers is "a big proponent of this defund the police movement."
Error processing example 13612: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "White House physician demands Biden’s immediate resignation following his ‘obvious’ cognitive decline."
Error processing example 13613: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] On Gov. Tony "Evers’ watch…the efficiency of the state crime lab has plummeted."
Error processing example 13614: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███       | 624/2000 [16:51<10:30,  2.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███▏      | 626/2000 [16:58<33:04,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███▏      | 628/2000 [17:00<28:54,  1.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  31%|███▏      | 629/2000 [17:00<25:19,  1.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "There has never been a leak anything like this" draft of a Supreme Court opinion that would overturn Roe v. Wade. "There’s never been a leak of a vote — much less an actual opinion, much less in a case of this significance."
Error processing example 13615: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republican politicians are pushing a plan "that could raise taxes on almost one in three Wisconsinites and sunset Social Security and Medicare in five years."
Error processing example 13616: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "... Justices Gorsuch, Kavanaugh, and Barrett assured the Senate Judiciary Committee and the American people that Roe v. Wade was ‘established precedent.’"
Error processing example 13617: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] NBC News "covered up Hunter Biden laptop story."
Error processing example 13618: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 343.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An adenovirus is a combination of viruses, gastrointestinal virus, pneumonia and COVID-19 mixed together to "kill our children."
Error processing example 13619: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Margaret Sanger, founder of Planned Parenthood, referred to Black women as weeds that needed to be plucked from their garden, and was a supporter of the Ku Klux Klan and a Democrat.
Error processing example 13620: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When the New York State Senate voted to legalize abortion in 1970, 12 Republican senators voted in favor of it.
Error processing example 13621: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Roe v. Wade happened because a woman lied about being raped by black men."
Error processing example 13622: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Republicans DO NOT want to throw doctors" in jail.
Error processing example 13623: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "79% of Planned Parenthood clinics are in minority neighborhoods. This is not by accident. That is by its founder, Margaret Sanger's, eugenicist design."
Error processing example 13624: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There are 63 million abortions a year in this country."
Processing examples:  32%|███▏      | 632/2000 [17:01<15:48,  1.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 633/2000 [17:06<34:55,  1.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 634/2000 [17:08<37:09,  1.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 638/2000 [17:08<17:52,  1.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 640/2000 [17:08<13:18,  1.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 642/2000 [17:12<20:41,  1.09it/s]Error processing example 13625: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It costs more to live in many Florida cities than New York City."
Error processing example 13626: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 343.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Our Supreme Court has never taken away a constitutional right."
Error processing example 13627: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The vast majority of individuals that we’re releasing out are not showing up for hearings."
Error processing example 13628: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mehmet Oz is "pro-abortion."
Error processing example 13629: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Senator Grassley hopped off the Finance Committee, where he could be helpful to Iowa and the nation, in favor of the Judiciary Committee"
Error processing example 13630: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Georgia Secretary of State Brad Raffensperger used "$50 million of Zuckerberg money" in 2020 to "tip the scales."
Error processing example 13631: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Amber Heard plagiarized lines from the movie "The Talented Mr. Ripley" in her opening statement during the Johnny Depp libel trial.
Error processing example 13632: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Lee Zeldin "voted for Cuomo’s Billion Dollar Tax hike and every Cuomo budget."
Error processing example 13633: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Supreme Court Justice Clarence Thomas and his wife Ginni posted a photo of themselves enjoying a $5,135 bottle of wine "as American women despair about abortion rights being taken away from them."
Error processing example 13634: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Texas GOP lawmaker recently introduced a bill that would allow the death penalty for women who have an abortion.
Error processing example 13635: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 644/2000 [17:12<15:06,  1.50it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 646/2000 [17:15<20:18,  1.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  32%|███▏      | 648/2000 [17:15<14:43,  1.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 651/2000 [17:18<16:38,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Supreme Court Justices Amy Coney Barrett and Samuel Alito said that the U.S. needs a "domestic supply of infants" to meet the needs of parents seeking to adopt.
Error processing example 13636: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows abortion rights activist urging women to "run on down to the abortion clinic" if they get pregnant.
Error processing example 13637: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Is ANYONE asking how Ron DeSantis was worth $340,000 in 2020 and is now worth $52 million?"
Error processing example 13638: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Due to U.S. Senate rules, Democrats need "more than a majority ... to codify Roe vs. Wade."
Error processing example 13639: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Tennessee just banned Plan B and made it a crime punishable by a $50,000 fine to order it."
Error processing example 13640: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "GOP Sen. Marsha Blackburn has proposed that birth control should only be legal for MARRIED couples."
Error processing example 13641: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We’re one of the seven nations in the world that allow abortion on demand at 20 weeks."
Error processing example 13642: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Biden administration is bringing amendments that would propose that all nations of the earth cede their sovereignty over national health care decisions to the WHO."
Error processing example 13643: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Zack Nunn saved over 2,000 Americans abandoned in Afghanistan by leading private rescue missions."
Error processing example 13644: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "David Perdue voted with liberals" for $10 trillion "in new debt, his out-of-control federal spending is driving inflation through the roof."
Error processing example 13645: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The NewProfilePic app is Russian spyware.
Processing examples:  33%|███▎      | 653/2000 [17:18<12:36,  1.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 655/2000 [17:22<21:23,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 659/2000 [17:22<13:19,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 660/2000 [17:24<16:37,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 661/2000 [17:29<30:57,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13646: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "My Treasury Department is planning to pay down the national debt this quarter, which never happened under my predecessor."
Error processing example 13647: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "J.D. Vance profits off Russia propaganda."
Error processing example 13648: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Suggests Megan Thee Stallion was not shot in the foot.
Error processing example 13649: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The infant formula shortage was purposely created because Mark Zuckerberg and Bill Gates invested in artificial breast milk.
Error processing example 13650: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Kathy Barnette wants to build a statue of Barack Obama right next to the one of Abraham Lincoln on Capitol Hill."
Error processing example 13651: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The U.S. government’s inaction over buffalo killing that harmed Native American populations in the 1800s shows it is now "is creating food shortages" so that Americans are more "dependent on the government."
Error processing example 13652: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In the immediate wake of a fire and vandalism at the Madison headquarters of an anti-abortion group, Democrats had not condemned "activists who are engaging in this repugnant illegal activity."
Error processing example 13653: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says people can follow a 1960 recipe for homemade baby formula as a workaround during the current shortage.
Error processing example 13654: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says the Biden administration is choosing to send baby formula to the border as one of its "out-of-touch priorities."
Error processing example 13655: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Homemade baby formula recipes offer a ‘viable option’ during U.S. shortage
Processing examples:  33%|███▎      | 663/2000 [17:29<21:40,  1.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 664/2000 [17:30<22:20,  1.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 666/2000 [17:30<15:05,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 668/2000 [17:30<10:35,  2.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  33%|███▎      | 669/2000 [17:32<13:15,  1.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▎      | 671/2000 [17:32<09:26,  2.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▎      | 672/2000 [17:34<17:10,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▎      | 673/2000 [17:34<13:53,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13656: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "When President Biden took office … there was no vaccine available."
Error processing example 13657: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Amazon will ship baby formula to U.S. addresses if you change your preferred country on its website from the U.S. to Canada.
Error processing example 13658: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Agriculture is the No. 1 industry here in Pennsylvania."
Error processing example 13659: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden "inherited inflation of 1.7%. Now we're well over 8% and growing."
Error processing example 13660: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Merrick Garland "sicced the police on parents when they were at the school boards simply trying to be heard for the safety of their children."
Error processing example 13661: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Mars Rover photo captured by NASA shows a doorway, suggesting extraterrestrial life.
Error processing example 13662: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 2020, when Raphael Warnock ran a campaign ad featuring himself with a beagle, "that wasn’t his dog."
Error processing example 13663: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kathy Barnette "pretends she's America First, but repeatedly attacked President Trump."
Error processing example 13664: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cheri Beasley "vacated" a man’s death sentence and "threw out" the indictment in a child assault case.
Error processing example 13665: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says she never tweeted comments linking pedophilia and Islam.
Error processing example 13666: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Agriculture "is still the No. 1 sector of the economy" in Georgia.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 676/2000 [17:34<07:30,  2.94it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 678/2000 [17:36<10:03,  2.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 679/2000 [17:37<12:13,  1.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 681/2000 [17:38<11:49,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 682/2000 [17:40<19:04,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 684/2000 [17:40<12:59,  1.69it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13667: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Fox News and Candace Owens tweeted that "New studies show that 68% of those who used ivermectin to treat or prevent covid are suffering long-term bowel and urinary incontinence."
Error processing example 13668: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Democrats want open borders so they can bring in and amnesty tens of millions of illegal aliens — that’s their electoral strategy."
Error processing example 13669: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United States can’t import baby formula because of the U.S.-Mexico-Canada Agreement.
Error processing example 13670: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The mass shooting in Buffalo, New York was a "false flag event."
Error processing example 13671: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "When unemployment goes down, inflation goes up."
Error processing example 13672: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Due to the baby formula shortage Tricare will now pay to have it shipped directly to you."
Error processing example 13673: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Studies show plastics can help combat climate change.
Error processing example 13674: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] State Sen. Kirk deViere voted against Medicaid expansion, the governor’s budget and teacher raises.
Error processing example 13675: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] There’s a "‘positive correlation’ between higher mask usage and COVID-19 deaths."
Error processing example 13676: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The problem with mail ballots in Lancaster County is a sign of "voter fraud."
Error processing example 13677: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Baby boomers didn’t have autism, seizures, allergies and other ailments when they were kids.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 686/2000 [17:44<22:41,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 689/2000 [17:44<13:51,  1.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  34%|███▍      | 690/2000 [17:50<33:27,  1.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▍      | 691/2000 [17:56<50:13,  2.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▍      | 692/2000 [17:56<40:10,  1.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▍      | 693/2000 [17:56<31:21,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▍      | 694/2000 [18:00<45:27,  2.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13678: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Public schools are now as segregated by race and class as they were in the 1960s."
Error processing example 13679: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Iran, if a 12-year-old girl is raped and impregnated by her father, she must carry the baby to term, or be thrown in prison for life. Wait, sorry, no, That's Alabama."
Error processing example 13680: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Georgia Gov. Brian Kemp "allowed 7.5 million ballots to be sent to every registered voter."
Error processing example 13681: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] They passed the flu off as COVID-19, and "they’re going to do the same thing with monkeypox and shingles."
Error processing example 13682: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "U.S. Orders MILLIONS Of Smallpox Vaccines Amid Global Monkeypox Outbreak."
Error processing example 13683: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 329.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Nearly 60% of all student loan debt is held by the rich and upper-middle class," so forgiveness would give the wealthy a "financial windfall" but not really help low-income people.
Error processing example 13684: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 831.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.76 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 59.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The current monkeypox outbreak was planned by government and industry leaders.
Error processing example 13685: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Suggests a January traffic accident involving lab monkeys was responsible for spreading monkeypox to humans.
Error processing example 13686: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The World Health Assembly was canceled "after the Geneva airport was coincidentally blown up."
Error processing example 13687: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Stacey Abrams "supported the MLB boycott."
Error processing example 13688: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▍      | 696/2000 [18:00<29:00,  1.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▍      | 698/2000 [18:02<22:33,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▌      | 700/2000 [18:02<16:39,  1.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▌      | 702/2000 [18:03<13:21,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▌      | 704/2000 [18:04<12:42,  1.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▌      | 706/2000 [18:05<11:29,  1.88it/s]
[CLAIM] "Swedish health authorities considering introducing restrictions because of monkeypox outbreak."
Error processing example 13689: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Bill Gates planned a smallpox-type outbreak.
Error processing example 13690: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A CNN story reported that Rice Krispies added a transgender mascot.
Error processing example 13691: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Increased encounters at the southwest border in April 2022 compared to April 2020 are attributable to changes in border policy.
Error processing example 13692: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Uvalde, Texas, school shooter was a "transsexual leftist illegal alien named Salvatore Ramos."
Error processing example 13693: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] ICE is in Uvalde, Texas, "so undocumented parents have to consider arrest deportation before they go check to see if their kids are alive."
Error processing example 13694: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Most of these killers tend to be 18, 19 years old."
Error processing example 13695: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We know from past experiences that the most effective tool for keeping kids safe is armed law enforcement on the campus."
Error processing example 13696: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] This image shows the victims of the school shooting in Uvalde, Texas.
Error processing example 13697: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "When we passed the assault weapons ban, mass shootings went down. When the law expired, mass shootings tripled."
Error processing example 13698: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The official pride flag was altered to include Ukrainian colors.
Error processing example 13699: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  35%|███▌      | 709/2000 [18:05<07:08,  3.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 711/2000 [18:05<05:30,  3.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 713/2000 [18:07<09:18,  2.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 715/2000 [18:07<06:56,  3.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 717/2000 [18:09<13:01,  1.64it/s]
[CLAIM] "50% of the guns sold in Texas, because of the loopholes, do not pass through a background check."
Error processing example 13700: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The N.C. GOP came out in support of Sen. [Rick] Scott's extremist policy to ‘Rescue America.’"
Error processing example 13701: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "90% of Americans, regardless of political party, want universal background checks."
Error processing example 13702: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Texas is "50th in the nation in mental health care access."
Error processing example 13703: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "More people die from hands, fists, feet, than rifles."
Error processing example 13704: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There hasn't been a single of these mass shootings that have been purchased at a gun show or on the internet."
Error processing example 13705: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We need to realize that people who think that ‘well, maybe if we could just implement tougher gun laws, it’s going to solve it,’ Chicago and LA and New York disprove that thesis."
Error processing example 13706: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The leading cause of death among children is a firearm."
Error processing example 13707: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Recent research suggests that 15% of abortions are the result of coercion.
Error processing example 13708: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] ABC News altered the Uvalde, Texas, school shooter’s photo to appear more Caucasian.
Error processing example 13709: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] With the $53 billion spent in Ukraine aid, the U.S. "could pay five SWAT members $80,000 each and have them at EVERY school front door."
Error processing example 13710: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 719/2000 [18:09<09:30,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 721/2000 [18:09<07:08,  2.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▌      | 723/2000 [18:11<08:38,  2.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  36%|███▋      | 726/2000 [18:11<07:00,  3.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Active shooter drill at high school in Uvalde, Texas, two months before the elementary school shooting shows the event was a false flag.
Error processing example 13711: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 2013, under a Democratic Senate, "a majority of the Senate voted in favor of" gun-related legislation sponsored by Republican Sens. Chuck Grassley and Ted Cruz, "but the Democrats filibustered it."
Error processing example 13712: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Nobody in any election in America gets 74% of the votes. Ever. It doesn’t happen."
Error processing example 13713: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The RNC is "a $200 million economic infusion into our communities."
Error processing example 13714: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says U.S. Sen. Ron Johnson, his prospective opponent, has received $1.2 million from the gun lobby.
Error processing example 13715: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Costco gas pump screen said "Don’t blame us. Blame Joe Biden."
Error processing example 13716: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The man who killed the Uvalde, Texas, school shooter "wasn't even an on-duty police officer."
Error processing example 13717: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pfizer CEO says it’s their dream to reduce the population by 50% in 2023."
Error processing example 13718: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] $3 million per day in "your tax dollars" are being spent to guard "unused border wall materials."
Error processing example 13719: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] TV interviews with two different men who are each identified as Uvalde victim's father show the Robb Elementary School mass shooting was staged.
Error processing example 13720: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Aaron Salter Jr. was killed by a mass shooter because he was working on creating a water-powered car engine.
Processing examples:  36%|███▋      | 728/2000 [18:11<05:23,  3.93it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 731/2000 [18:13<07:34,  2.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 732/2000 [18:14<09:54,  2.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 734/2000 [18:15<08:16,  2.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 736/2000 [18:16<09:09,  2.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 738/2000 [18:16<07:55,  2.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13721: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Illinois high school to implement race-based grading system in 2022-2023 school year
Error processing example 13722: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Zimbabwe, people have started selling their toes for thousands of dollars due to (the) high cost of living."
Error processing example 13723: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There are fewer Iowans working today than when Gov. Reynolds took office."
Error processing example 13724: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Dead people always vote Democrat."
Error processing example 13725: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "A quarter of the entire acreage in the country that is under hemp production is here in North Carolina."
Error processing example 13726: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gun manufacturers are "the only industry in the country" that have immunity from lawsuits.
Error processing example 13727: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Since I took office, Michigan has added more than 25,000 auto jobs."
Error processing example 13728: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Biden gave Americans "the cheapest gas prices on Earth."
Error processing example 13729: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Blake Masters "unironically quoted a Nazi, called World War II an unjust war, called the Unabomber one of his intellectual influencers, and shared an article attacking Israel as the North Korea of the Middle East."
Error processing example 13730: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The U.S. Justice Department "conservatively estimated that guns are used 1.5 million times per year to save lives."
Error processing example 13731: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A BP oil executive named Brice Cromwell blamed the Biden administration for high gas prices.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 740/2000 [18:18<13:06,  1.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 743/2000 [18:19<08:31,  2.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 744/2000 [18:20<10:35,  1.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 747/2000 [18:20<06:45,  3.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  37%|███▋      | 748/2000 [18:26<26:55,  1.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13732: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The impact of President Biden being beholden to the Green New Deal radicals in his party has electricity costs through the roof in Texas."
Error processing example 13733: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] White supremacists are going to be "shooting up all Walmarts and will kill Blacks and Mexicans" in San Bernardino, California.
Error processing example 13734: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Waiting periods for gun purchases may not make a difference, because "If somebody's decided that they're gonna take their life, they're gonna take their life."
Error processing example 13735: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ukraine adopted "laws banning the Russian language."
Error processing example 13736: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo shows U.S. Rep. Lauren Boebert with two scantily dressed men.
Error processing example 13737: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cheri Beasley tossed the conviction of a man seeking sex with a boy online and "set free" a child porn offender.
Error processing example 13738: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "USDA is predicting egg prices will be $12 a dozen by fall 2022."
Error processing example 13739: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Bennie Thompson actively cheer-led riots in the ’90s."
Error processing example 13740: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] 95-item list of food-destroying incidents demonstrates that "you are duped if you think they aren’t planning a food shortage."
Error processing example 13741: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 839.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.75 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Gavin Newsom reportedly intervened at the request of Nancy Pelosi and directly ordered the California Highway Patrol to drop all charges" against Paul Pelosi for his DUI arrest.
Processing examples:  37%|███▋      | 749/2000 [18:29<30:56,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 751/2000 [18:29<21:47,  1.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 753/2000 [18:29<14:57,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 755/2000 [18:33<23:03,  1.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 757/2000 [18:33<16:43,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 758/2000 [18:34<17:46,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13742: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Americans aren’t required to show IDs to vote.
Error processing example 13743: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "U.S. House of Reps. votes 226-194 to criminalize disassembling, cleaning, and re-assembling your gun without a firearm manufacturer’s license, including 8 Republicans!"
Error processing example 13744: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Jan. 6, 2021, attack on the U.S. Capitol "was a dust-up."
Error processing example 13745: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden’s margin of the less than 21,000 votes in Wisconsin was the tightest of any state in 2020."
Error processing example 13746: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Donald Trump authorized up to 20,000 National Guard troops to protect the Capitol" before Jan. 6, 2021, but was "rejected" by Nancy Pelosi and Chuck Schumer.
Error processing example 13747: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Bennie Thompson objected to the 2004 Presidential election."
Error processing example 13748: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Not a single person in the crowd on January 6 was found to be carrying a firearm. Not one."
Error processing example 13749: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Justin Bieber’s Ramsay Hunt syndrome and Hailey Bieber’s blood clot were caused by COVID-19 vaccines.
Error processing example 13750: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Soviet and Chinese communists have "grabbed control" of U.S. entertainment, movies, television, music, academia, K-12 education and the news media.
Error processing example 13751: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Pentagon "finally comes clean" and "admits...that there are 46 U.S. military-funded biolabs in Ukraine"
Error processing example 13752: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 761/2000 [18:35<10:20,  2.00it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 762/2000 [18:35<08:53,  2.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 763/2000 [18:36<14:18,  1.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 764/2000 [18:37<12:00,  1.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 766/2000 [18:38<11:18,  1.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 768/2000 [18:39<14:22,  1.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  38%|███▊      | 770/2000 [18:42<17:02,  1.20it/s]
[CLAIM] Claims the U.S. military is using "polymer drone flies" that look and act like a large fly.
Error processing example 13753: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When Joe Biden "walked into this administration … 20 million people were on unemployment insurance benefits."
Error processing example 13754: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We’ve got a parole board right now that has released 20 cop killers in the last two years under Hochul."
Error processing example 13755: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wisconsin's archaic abortion ban is older than 20 states."
Error processing example 13756: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A doctor at the Mayo Clinic is misdiagnosing pregnancies of Trump supporters so they have abortions.
Error processing example 13757: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Adam Laxalt "wants to restrict" and "worked to limit access to birth control."
Error processing example 13758: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Zach Nunn took money from Adventureland’s CEO, then sponsored legislation to loosen amusement park safety rules. A child died. Zach Nunn put his big donor first, ahead of the safety of Iowa families."
Error processing example 13759: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Three mass shootings were meant to distract from Hillary Clinton controversies.
Error processing example 13760: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Adam Laxalt "supports eliminating Nevada's protections for legal abortions."
Error processing example 13761: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cheri Beasley voted to reverse the conviction of an armed kidnapper, release a double murderer early.
Error processing example 13762: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] J.D. Vance "wants to defund law enforcement."
Error processing example 13763: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▊      | 772/2000 [18:44<19:53,  1.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▊      | 774/2000 [18:44<13:53,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▉      | 775/2000 [18:44<11:49,  1.73it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▉      | 777/2000 [18:45<11:26,  1.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▉      | 780/2000 [18:46<06:53,  2.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "The four big meat packers are raking in record profits … "
Error processing example 13764: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In every other major industrial country in the world … inflation is higher."
Error processing example 13765: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Virginia, Black people are eight times (8X) more likely than white people to die of gun homicide."
Error processing example 13766: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "American oil is … more affordable" than foreign oil.
Error processing example 13767: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "When you have unemployment below 4% and inflation above 4%, recession always follows within two years."
Error processing example 13768: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] According to International Olympic Committee data, "an average of 29 athletes under the age of 35 suffered sudden death per year from 1966 to 2004. From March 2021 to March 2022, 769 athletes have died or suffered cardiac arrest."
Error processing example 13769: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Supreme Court ruled that border agents can enter anyone’s home within a 100-mile radius of the border without a warrant for any reason and federal courts cannot do anything about it."
Error processing example 13770: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] No one "with a concealed carry permit ever committed a mass shooting."
Error processing example 13771: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Peter Meijer voted to "create the Adam Schiff-led January 6th Commission."
Error processing example 13772: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The majority of citizens in the country will have no change at all because … most of the population resides in states that have already protected abortion rights."
Error processing example 13773: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Plan B can cause an abortion. It’s right there on the box."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▉      | 782/2000 [18:46<05:15,  3.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▉      | 784/2000 [18:47<06:51,  2.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  39%|███▉      | 787/2000 [18:47<04:41,  4.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|███▉      | 790/2000 [18:47<03:17,  6.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13774: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Supreme Court decision overturning Roe v. Wade "made the United States an outlier among developed nations in the world" on abortion rights.
Error processing example 13775: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Founding Fathers of the United States intended that "the church is supposed to direct the government."
Error processing example 13776: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Planned Parenthood clinics are "closing down all over the country" because Roe v. Wade was overturned.
Error processing example 13777: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] This is the 911 call placed by the migrants trapped in the truck found in San Antonio, Texas.
Error processing example 13778: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Charlie Crist is pro-life."
Error processing example 13779: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video claims Moscow turned into "sea of fire after being attacked by two mysterious missiles."
Error processing example 13780: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Civics Secures Democracy Act "would allow the Biden administration to buy off states with $6 billion" if they adopt critical race theory.
Error processing example 13781: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows the aftermath of a mass shooting in Harrisburg, Pennsylvania.
Error processing example 13782: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The cost of a Fourth of July cookout in 2022 increased by 67.2% because of inflation.
Error processing example 13783: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 339.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Images show Highland Park shooter had antifa connection
Error processing example 13784: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida Gov. Ron DeSantis "signs bill requiring Florida students, professors to register political views with (the) state."
Processing examples:  40%|███▉      | 792/2000 [18:52<13:58,  1.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|███▉      | 794/2000 [18:57<23:59,  1.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|███▉      | 797/2000 [18:57<15:45,  1.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|████      | 800/2000 [19:03<23:23,  1.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|████      | 801/2000 [19:04<24:41,  1.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13785: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] House Democrats have kicked Rep. Ilhan Omar out of Congress
Error processing example 13786: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Every pro-life Governor up for election in November won – by a large margin.”
Error processing example 13787: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “All ‘gender affirming care’ for children is 100% experimental. There are zero long term studies on the effects the drugs and surgeries will have on them.”
Error processing example 13788: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Historically, our spring elections (including for state Supreme Court) have a smaller turnout associated with them."
Error processing example 13789: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In 2010, (Wisconsin Republicans) inherited a multi-Billion dollar deficit … Today we are looking at a projected surplus approaching 7 Billion."
Error processing example 13790: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Florida's water is dirtier and sicker than when Gov. DeSantis first took office."
Error processing example 13791: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID-19 vaccines "provide zero benefit relative to risk for the young and healthy."
Error processing example 13792: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] There have been “1,598 athlete cardiac arrests since Jan 2021. 69% fatal."
Error processing example 13793: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Pope Benedict XVI and the Vatican requested that President Biden not attend Benedict’s funeral."
Error processing example 13794: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Buffalo Bills player Damar Hamlin’s collapse was caused by the COVID-19 vaccine.
Error processing example 13795: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|████      | 803/2000 [19:05<20:27,  1.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|████      | 806/2000 [19:06<15:32,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  40%|████      | 809/2000 [19:07<10:26,  1.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 812/2000 [19:07<07:12,  2.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Since a new immigration program was implemented, the number of Venezuelans trying to enter the U.S. illegally decreased “from about 1,100 per day to less than 250 per day on average.”
Error processing example 13796: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Let’s be clear: ‘drag shows’ are strip shows.”
Error processing example 13797: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cellphone data used to arrest the Idaho quadruple-murder suspect proves that criticism of the “2000 Mules” movie’s use of cell data is unfounded.
Error processing example 13798: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] As president, Donald Trump “marshaled the full power of government to stop deadly drugs, opioids, and fentanyl from coming into our country. As a result, drug overdose deaths declined nationwide for the first time in nearly 30 years."
Error processing example 13799: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Americans who worked through the pandemic are being paid up to $26,000 by qualifying for the Employee Retention Credit.”
Error processing example 13800: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Fifty-one percent of our taxes are paid by 2% of New Yorkers.”
Error processing example 13801: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A “default on our debt” would be unprecedented in American history.
Error processing example 13802: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] MMA fighter Victoria Lee died because of a COVID-19 vaccine.
Error processing example 13803: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Parents that go to school board meetings" and question the curriculum are being "flagged by the Department of Justice and the FBI for attending a meeting."
Error processing example 13804: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Lula's staff infiltrated and caused damage while the Brazilian Patriots cleaned up.”
Error processing example 13805: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The World Health Organization has been working since 1974 on vaccines to create permanent sterility.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 814/2000 [19:09<11:08,  1.77it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 815/2000 [19:09<09:47,  2.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 817/2000 [19:12<13:51,  1.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 819/2000 [19:12<10:06,  1.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 821/2000 [19:12<07:32,  2.61it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████      | 823/2000 [19:13<07:34,  2.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13806: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The election in Brazil “was very clearly a rigged election.”
Error processing example 13807: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden drained America's Strategic Petroleum Reserves to the lowest level since 1984."
Error processing example 13808: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Energy costs are the highest in 15 years."
Error processing example 13809: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “When the Republicans were in power the last time for eight years, do you know how much discretionary spending increased in those eight years? Zero.”
Error processing example 13810: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden as vice president didn't "have the right to declassify" documents.
Error processing example 13811: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The White House is now attempting to ban all gas ovens and burners.”
Error processing example 13812: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Harvard Medical School students “learn how to care for LGBTQIA+ infants.”
Error processing example 13813: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “More contraception availability increases abortion demand."
Error processing example 13814: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Take-home pay for workers is going up.”
Error processing example 13815: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Joe Biden’s war on American energy caused this crisis and his only response has been to drain our strategic petroleum reserve to its lowest level since 1983.”
Error processing example 13816: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Social Security has nothing to do with the deficit or the national debt."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████▏     | 825/2000 [19:14<10:19,  1.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████▏     | 827/2000 [19:15<08:28,  2.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  41%|████▏     | 829/2000 [19:17<12:09,  1.61it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 832/2000 [19:17<08:37,  2.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 833/2000 [19:18<08:21,  2.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13817: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The cost of eggs has gone up 700%.
Error processing example 13818: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "New government-funded 'food pyramid' says Lucky Charms are healthier than steak."
Error processing example 13819: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Abbott “signed a bill last year eliminating the requirements for Texas schools to teach MLK’s ‘I have a dream’ speech.”
Error processing example 13820: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Every single Republican blocked Ketanji Brown Jackson’s SCOTUS appointment.”
Error processing example 13821: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Climate change is a naturally occurring phenomenon that can be tracked throughout prehistoric eras."
Error processing example 13822: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Almost every study now has said with these new boosters, you're more likely to get infected with the bivalent booster."
Error processing example 13823: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Glenn Youngkin is “the first sitting Governor in the modern era to have their party have less legislative seats than when they were elected.”
Error processing example 13824: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The average American has anywhere from 5 to 15 pounds” of stool stuck in their gut.
Error processing example 13825: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In California, there were “10 million mail ballots unaccounted for” in the November midterm election.
Error processing example 13826: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Document shows that Hunter Biden paid Joe Biden $50,000 a month in rent.
Error processing example 13827: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 835/2000 [19:18<06:11,  3.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 836/2000 [19:20<13:19,  1.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 839/2000 [19:21<08:16,  2.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 840/2000 [19:24<18:26,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 841/2000 [19:24<15:19,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 842/2000 [19:26<21:15,  1.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Bill Gates tweeted that “vaccines in our food supply solves the problem of vaccine hesitancy.”
Error processing example 13828: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Wisconsin, even multimillionaires can be on BadgerCare "because we don't even ask people what their income level is to qualify for free health care."
Error processing example 13829: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Transportation Security Administration spent “a couple hundred million dollars” on gender-neutral screening technology.
Error processing example 13830: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Chickens aren’t laying eggs because RNA is being added to commercial chicken feed.”
Error processing example 13831: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If New York’s proposed limits on natural gas in buildings take effect, the grid today can’t handle the increased electric load.
Error processing example 13832: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “One quarter” of today’s $31.4 trillion federal debt “was accumulated in the four years of my predecessor,” Donald Trump.
Error processing example 13833: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Classified documents can never be taken out of a (sensitive compartmented information facility), ever.”
Error processing example 13834: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The U.S. chicken and egg shortage is linked to Bill Gates.
Error processing example 13835: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The sinking of the Titanic was orchestrated by the Rothschilds and J.P. Morgan to kill prominent businessmen who opposed the creation of the Federal Reserve.
Error processing example 13836: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The USA has a gun homicide rate 26 times higher than our peers.”
Error processing example 13837: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Virginia tax receipts “in just the last four years alone have grown 50%.”
Processing examples:  42%|████▏     | 845/2000 [19:27<13:07,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 846/2000 [19:28<14:05,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▏     | 847/2000 [19:30<18:25,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  42%|████▎     | 850/2000 [19:32<17:16,  1.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 853/2000 [19:33<10:33,  1.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 855/2000 [19:36<18:00,  1.06it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13838: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Under Biden, we have seen over 4.6 million encounters at our southern border. Most have been released into the interior and will stay here indefinitely."
Error processing example 13839: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] People not vaccinated against COVID-19 “came out the best."
Error processing example 13840: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Electrically charged stones discovered in Congo.
Error processing example 13841: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “NASA stopped exploring the ocean.”
Error processing example 13842: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wisconsin has seen over 63 billion dollars of federal pandemic-related funding funnel into our state over (Gov. Tony Evers') tenure."
Error processing example 13843: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Diamond and Silk pundit died from COVID-19
Error processing example 13844: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Female student-athletes in Florida need to provide their schools with detailed information about their periods.”
Error processing example 13845: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Two years ago this week, 18 million people were out of work needing unemployment benefits. Today, that number is under 1.6 million, the lowest in decades.”
Error processing example 13846: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wind farm construction is a possible cause of dead whales washing ashore in New Jersey.
Error processing example 13847: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Global warming is a “fraud.”
Error processing example 13848: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United States of America “no longer exists” because President Joe Biden “signed away our sovereignty as a nation.”
Processing examples:  43%|████▎     | 856/2000 [19:37<15:22,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 859/2000 [19:37<09:21,  2.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 861/2000 [19:40<15:16,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 863/2000 [19:42<16:41,  1.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 864/2000 [19:43<15:43,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13849: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Same-day voter registration "does not allow election officials to verify the validity and accuracy of voter information."
Error processing example 13850: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Right now, driving 100 miles in an electric vehicle is more expensive than driving an internal combustion engine car.”
Error processing example 13851: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Pregnancy tests include a “hidden Plan B” pill.
Error processing example 13852: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The largest contributor to the debt ceiling, or to our deficit, has been the Trump tax cuts.”
Error processing example 13853: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Doctors say colon cancer is on the rise due to toxins absorbed while cooking food (in) the microwave.”
Error processing example 13854: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "I got a list of 20 jurisdictions that defunded the police to the tune of over $1 billion total."
Error processing example 13855: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An ingredient in some of Chick-fil-A’s dipping sauces can cause serious health problems.
Error processing example 13856: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Pfizer just got caught doing gain of function to mutate covid intentionally for vaccines.”
Error processing example 13857: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "(Ron) DeSantis' bill would remove: background checks, instruction, training and oversight."
Error processing example 13858: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We used to rank No. 1 in the world in research and development, now we rank No. 9. China used to rank No. 8, now it ranks No. 2.”
Error processing example 13859: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 867/2000 [19:45<14:41,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  43%|████▎     | 868/2000 [19:45<12:36,  1.50it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▎     | 870/2000 [19:45<09:03,  2.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▎     | 871/2000 [19:46<10:49,  1.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▎     | 874/2000 [19:46<06:20,  2.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 876/2000 [19:47<07:57,  2.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “More classified documents (were) found at the University of Delaware,” the “equivalent of a tractor trailer worth.”
Error processing example 13860: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The whole green energy agenda involves more than doubling 10 types of mining.”
Error processing example 13861: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Backyard chicken flocks have stopped laying eggs, and the feed is responsible.
Error processing example 13862: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “You know how many children are dying across Virginia because of fentanyl poisoning? A heck of a lot more than are dying at the hands of guns.”
Error processing example 13863: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden said, “The idea that we’re going to send in tanks to Ukraine, that’s called World War III.”
Error processing example 13864: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Illinois “received $5.1 billion at an elementary school there that used it for equity and diversity.”
Error processing example 13865: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Biden administration is floating a new” climate strategy: Don't leave your house."
Error processing example 13866: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Brown v. Board of Education “was never about sending Black children to white schools. It was about parents being able to decide where to send their children to school.”
Error processing example 13867: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York City Mayor Eric Adams proposes to “ban chocolate milk.”
Error processing example 13868: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We created more new jobs in two years than any president did in their entire term.”
Error processing example 13869: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Unusual cancers” are “exploding right into stage 4" and those cancer cells have “the spike protein."
Processing examples:  44%|████▍     | 877/2000 [19:48<06:55,  2.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 880/2000 [19:48<04:19,  4.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 883/2000 [19:52<11:47,  1.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 884/2000 [19:52<11:27,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 885/2000 [19:52<09:48,  1.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 886/2000 [19:56<21:11,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13870: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “‘Satellites’ are on balloons and not in space.”
Error processing example 13871: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "It's President Biden who is proposing to cut Medicare Advantage."
Error processing example 13872: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The moon is a habitable place.”
Error processing example 13873: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “No administration has ever looked less like America, just by the numbers, than the Biden administration.”
Error processing example 13874: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Earthquake in Turkey and Syria is "a systemic attack against Kurdish people."
Error processing example 13875: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Lifetime earnings of someone with a college degree is over a million dollars more than those entering the workforce without one.”
Error processing example 13876: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Studies show that children who go to preschool are nearly 50% more likely to finish high school and go on to earn a two- or four-year degree, no matter their background they came from."
Error processing example 13877: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 87,000 new IRS agents who were supposed to “only target the rich” are now “coming after waitresses’ tips.”
Error processing example 13878: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The earthquake in Turkey is a "scripted" use of geo engineering weather modification HAARP
Error processing example 13879: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden “cheated on his taxes and got away with it.”
Error processing example 13880: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Joe Biden's wide open southern border" is to blame for fentanyl deaths in the U.S.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  44%|████▍     | 890/2000 [19:56<11:10,  1.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 891/2000 [19:56<09:44,  1.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 892/2000 [19:57<08:15,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 893/2000 [20:02<26:59,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 894/2000 [20:03<24:03,  1.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 896/2000 [20:03<14:56,  1.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 898/2000 [20:03<09:58,  1.84it/s]Error processing example 13881: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In the past two years, democracies have become stronger, not weaker. Autocracies have grown weaker, not stronger.”
Error processing example 13882: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden closed schools during the COVID-19 pandemic.
Error processing example 13883: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under a GOP bill on cash bail, "a large quantity of people who are accused of crimes, including anyone who even witnesses certain crimes, could now be detained and subject to bail."
Error processing example 13884: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ron DeSantis voted to cut $473 billion from Medicare."
Error processing example 13885: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “North Carolina ranks last in the country in K-12 funding.”
Error processing example 13886: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 329.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The youth suicide rate has increased exponentially alongside trans affirmation. Trans affirmation causes the suicide rate, not the other way around.”
Error processing example 13887: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “One man was actually killed” during 2020 protests of police brutality in Raleigh
Error processing example 13888: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The dog dewormer fenbendazole can cure cancer in humans.
Error processing example 13889: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] 53% of Americans made aware of the Hunter Biden laptop story would have changed their vote in 2020.
Error processing example 13890: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Energy Secretary Jennifer Granholm met "with the radical green energy group behind the gas stove ban," which "has ties to the" Chinese Communist Party.
Error processing example 13891: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▍     | 899/2000 [20:05<14:51,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▌     | 902/2000 [20:05<08:10,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▌     | 904/2000 [20:05<07:02,  2.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▌     | 906/2000 [20:06<06:30,  2.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  45%|████▌     | 907/2000 [20:09<16:18,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “Humans are not causing climate change.” Magnetic pole shifting “is causing havoc worldwide in weather, climate, earthquakes, tsunamis, etc.”
Error processing example 13892: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Canada is to thank for American football.
Error processing example 13893: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] At the World Economic Forum, Moderna CEO Stéphane Bancel admitted that his company produced 100,000 doses of COVID-19 vaccines in 2019, before the pandemic started.
Error processing example 13894: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Michigan State University suspect is 22-year-old ‘Raymone Jordan.’”
Error processing example 13895: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Michigan State University shooter is a 21-year-old man named Lynn Dee Walker.
Error processing example 13896: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Obama imposed stricter rules on trains carrying toxins. Trump killed them.”
Error processing example 13897: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The “average tax” for billionaires is “about 3%,” which is “a lower tax than a schoolteacher or a firefighter."
Error processing example 13898: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Businesses large and small have led Texas to be ranked the number one state for business every year that I’ve been governor."
Error processing example 13899: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Seven ecological disasters in less than 2 weeks. … We are under attack.”
Error processing example 13900: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wake County is the only large county in North Carolina that elects its commissioners at-large countywide.
Error processing example 13901: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The train that derailed in Ohio was carrying “over 300,000 gallons of a chemical that was banned in 1974.”
Processing examples:  45%|████▌     | 909/2000 [20:09<11:27,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 910/2000 [20:11<13:26,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 912/2000 [20:11<10:48,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 914/2000 [20:11<07:36,  2.38it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 916/2000 [20:12<05:27,  3.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 918/2000 [20:19<24:32,  1.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13902: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo shows “what railways in Ohio look like while we send $40 billion to Ukraine.”
Error processing example 13903: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Mickey Mouse to be replaced as official Disney mascot.”
Error processing example 13904: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Ohio, “there are 75,000 acres of farmland, fertile farmland, that are all now being poured down with acid rain.”
Error processing example 13905: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Apple will pay customers $700 for a slowed-down iPhone.
Error processing example 13906: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Richard Belzer died because of a COVID-19 vaccine.
Error processing example 13907: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Biden admin negotiates deal to give WHO authority over US pandemic policies.”
Error processing example 13908: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Canada has only “had 11 school shooting deaths … in history.”
Error processing example 13909: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Muslims by the millions are converting to Christianity.”
Error processing example 13910: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The laws governing Agenda 2030 land development allows the government to seize polluted lands and move their residents to … smart cities.”
Error processing example 13911: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Australia, a “17% increase in heart attack deaths” in the first eight months of 2022 is linked to COVID-19 vaccination.
Error processing example 13912: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Student ID cards are a “source of fraud” at the polls.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 921/2000 [20:19<16:05,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 922/2000 [20:20<14:38,  1.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▌     | 924/2000 [20:20<10:18,  1.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▋     | 925/2000 [20:21<11:44,  1.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▋     | 926/2000 [20:25<25:06,  1.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▋     | 928/2000 [20:25<16:14,  1.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  46%|████▋     | 930/2000 [20:26<10:56,  1.63it/s]Error processing example 13913: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Because the Covid shots are still manufactured under the Emergency Use Authorization, they can change up to 49% of the ingredients without FDA approval."
Error processing example 13914: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I never claimed to be Jewish.”
Error processing example 13915: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Marco Rubio signed a 2021 letter that “supports waivers that would reduce visual track inspections.”
Error processing example 13916: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A $2 million grant for Georgia’s DeKalb County amounts to a “private takeover of election offices.”
Error processing example 13917: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Tammy Baldwin wants to take away” Medicare and Social Security.
Error processing example 13918: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "WOW county swing suburban voters (in Wisconsin) don't vote for Donald Trump.”
Error processing example 13919: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The left is determined to erase Mount Rushmore from our history books.”
Error processing example 13920: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Digital IDs were given to residents in East Palestine, Ohio, “to track long term health problems like difficulty breathing” before the Feb. 3 train derailment.
Error processing example 13921: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo of electric charging station “powered by diesel generator” is emblematic of the electric vehicle movement.
Error processing example 13922: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] DO Black credit card is part of a government plot to control people’s lives.
Error processing example 13923: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 932/2000 [20:28<15:57,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 935/2000 [20:29<09:51,  1.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 937/2000 [20:31<11:38,  1.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 940/2000 [20:31<07:31,  2.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Drinking or using tap water treated with chlorine is dangerous because “chlorine is a poison.”
Error processing example 13924: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The officers who beat Tyre Nichols targeted him because he was “allegedly dating” one of their wives.
Error processing example 13925: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We cut Black child poverty in half in 2021 because of the child tax credit.”
Error processing example 13926: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We’re just burning 300 gallons of jet fuel to de-ice this clean energy wind turbine … that needs 80 gallons of synthetic oil to operate."
Error processing example 13927: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 2020 election could not have been fair because “Los Angeles County agreed, during litigation, that there were 1.2 million ineligible voters after the election.”
Error processing example 13928: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Every single county in New York has experienced a federal climate disaster between 2011-2021.”
Error processing example 13929: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photos of the Statue of Liberty taken in 1920 and 2020 show that the sea level hasn’t changed.
Error processing example 13930: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York Gov. Kathy Hochul wants “quarantine camps” and “imprisonment” if you’re suspected of having a disease.
Error processing example 13931: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Half the kids in this country, when they graduate, can’t read their diploma.”
Error processing example 13932: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows that certain drinks can “test positive” for COVID-19 when using at-home COVID-19 tests.
Error processing example 13933: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Justice Department has been “calling parents that are concerned about what their kids are being taught, they are labeling them terrorists.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 942/2000 [20:34<13:23,  1.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 943/2000 [20:35<14:23,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 944/2000 [20:36<14:37,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 946/2000 [20:36<09:59,  1.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  47%|████▋     | 949/2000 [20:36<05:58,  2.93it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 951/2000 [20:38<07:38,  2.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13934: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats "want to ban gas powered cars and gas stoves.”
Error processing example 13935: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “USA gives final warning to Uganda to legalize homosexuality,” or no aid.
Error processing example 13936: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Ukrainian President Volodymyr Zelenskyy said he wants America’s “sons and daughters to go die in Ukraine.”
Error processing example 13937: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We banned transgender” people from serving in the military.
Error processing example 13938: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ron DeSantis wants to raise the retirement age to 70."
Error processing example 13939: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Violent crime in Manhattan is "now at a record level."
Error processing example 13940: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Walgreens’ position “has always been” to “dispense Mifepristone in any jurisdiction where it is legally permissible to do so.”
Error processing example 13941: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Capitol Police officers “helped” QAnon Shaman Jacob Chansley and “acted as his tour guides.”
Error processing example 13942: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "$1 of every $3 (Ron DeSantis) spends comes from the federal government."
Error processing example 13943: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “A proposal in Syracuse would pay gang members $100-$200 per week to stay out of trouble.”
Error processing example 13944: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Because of daylight saving time, “NO alarm set for the 2:00 a.m. hour will go off” on March 12.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 954/2000 [20:43<16:17,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 955/2000 [20:44<16:18,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 956/2000 [20:44<15:02,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 958/2000 [20:46<13:21,  1.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 959/2000 [20:50<25:45,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 961/2000 [20:50<17:00,  1.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13945: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The money for upgrades to the Milwaukee Brewers stadium in Evers’ budget is "a bipartisan plan."
Error processing example 13946: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The First Amendment doesn’t allow you to willingly lie."
Error processing example 13947: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] At a Wisconsin school, a 12-year-old girl "was transitioned into a boy by school officials without parental consent," and Protasiewicz supports it.
Error processing example 13948: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “When a basketball player or football or baseball player from another team plays in Wisconsin, that one game's salary, they pay Wisconsin income tax on it. ... So, if for some reason we do not have the Brewers in Wisconsin all of those player salaries that generate dollars for the State of Wisconsin go away."
Error processing example 13949: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 45.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] As a sitting Supreme Court justice, Daniel Kelly recused himself from a case, then “he pocketed $20,000 in contributions” from the plaintiff and family members and “unrecused himself so he could judge the case.”
Error processing example 13950: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “More cops are killed responding to domestic violence calls than anything else.”
Error processing example 13951: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sullivan County “has the highest opioid death rate in all of New York state” yet “it’s the only county in the Hudson Valley not included in the federal High-Intensity Drug Trafficking Area” program.
Error processing example 13952: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 69.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows Elon Musk saying he bought Twitter and unblocked Andrew Tate’s Twitter account because “we need to escape the suppression from the matrix and expose the global elites.”
Error processing example 13953: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “No one got (a) dime” from the $70 million raised for the Florida Disaster Fund.
Error processing example 13954: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Silicon Valley Bank “donated $73M to 'BLM Movement'”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 964/2000 [20:50<09:52,  1.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 966/2000 [20:55<19:34,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 967/2000 [20:55<16:29,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 968/2000 [20:59<26:55,  1.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  48%|████▊     | 970/2000 [21:01<21:26,  1.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13955: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says President Joe Biden was recorded talking about bank collapse.
Error processing example 13956: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When discussing education savings accounts, "We would be 31st in the nation to have these policies going forward on behalf of kids.”
Error processing example 13957: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Many law enforcement agencies didn’t submit their (hate crime) data to the FBI.”
Error processing example 13958: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Manhattan District Attorney Alvin Bragg “has downgraded over 50% of the felonies to misdemeanors.”
Error processing example 13959: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Individuals released from incarceration are 129 times more likely to die of a drug overdose during the first two weeks after release.”
Error processing example 13960: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “You can get vaccinated by sleeping with somebody who is already jabbed” with an mRNA vaccine. “It’s called shedding.”
Error processing example 13961: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We have a national debt the size of our nation’s economy for the first time since World War II.”
Error processing example 13962: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump “instructed all the governors to shut down” because of the COVID-19 pandemic.
Error processing example 13963: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Gun violence is the number one killer of children and teens — it has overtaken cars.”
Error processing example 13964: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Abortion pills are dangerous and “1 in 5 women will suffer a complication.”
Error processing example 13965: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▊     | 973/2000 [21:01<12:19,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▉     | 975/2000 [21:01<09:07,  1.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▉     | 977/2000 [21:03<10:35,  1.61it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▉     | 980/2000 [21:03<06:51,  2.48it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Sen. Tommy Tuberville is preventing military promotions “because he objects to women within the military getting access to reproductive care.”
Error processing example 13966: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In the U.S., “you’re not allowed to own a machine gun.”
Error processing example 13967: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “For every dollar in federal taxes my constituents pay, we get a mere 93 cents back.”
Error processing example 13968: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In the next two years, we'll spend more on interest on our national debt than we do on the national defense.”
Error processing example 13969: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Actions of the Tennessee state legislators who protested against gun violence were “at least equivalent” to the actions of the Jan. 6 insurrectionists.
Error processing example 13970: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Report proves that COVID-19 vaccines caused 300,000 excess deaths in 2022.
Error processing example 13971: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video clip shows that protesters in Tennessee “made their way onto the state Capitol floor.”
Error processing example 13972: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The 5 states with the highest gun homicide rates in the nation all have loose gun laws” while “the 5 states with the lowest rates have some of the toughest laws.”
Error processing example 13973: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Vitamins aren’t FDA approved, but Twinkies are.”
Error processing example 13974: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “President Trump is officially a political prisoner.”
Error processing example 13975: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Regarding Trump’s January 2021 telephone call with Georgia election officials, “nobody found anything wrong with that perfect call until a book promotion tour many months later.”
Processing examples:  49%|████▉     | 983/2000 [21:03<04:47,  3.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▉     | 985/2000 [21:08<14:50,  1.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▉     | 987/2000 [21:11<16:04,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  49%|████▉     | 989/2000 [21:12<15:10,  1.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|████▉     | 991/2000 [21:13<11:56,  1.41it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|████▉     | 993/2000 [21:13<09:42,  1.73it/s]Error processing example 13976: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Fentanyl is the leading cause of death for Americans 18-45.”
Error processing example 13977: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Minority Leader Robert Reives “sent out a statement … encouraging us to resign and/or (face) a primary challenge.”
Error processing example 13978: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 835.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New Kansas law will “authorize genital inspections of children in order for kids to play sports.”
Error processing example 13979: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Canada just made it illegal to protest against the LGBT!”
Error processing example 13980: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Scientists struggle to understand why Antarctica hasn’t warmed for over 70 years despite rise in CO2."
Error processing example 13981: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Lead paint was not banned to protect children’s health, but because it would protect people from radiation.
Error processing example 13982: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When migrants arrived on Martha’s Vineyard, “They called out the National Guard and deported them within 24 hours.”
Error processing example 13983: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. public schools don't teach the Declaration of Independence.
Error processing example 13984: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] There is “$500 billion of unspent COVID money” that can be rescinded.
Error processing example 13985: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “For African American families in particular, the homeownership rate remains relatively unchanged since 1968, the year the Fair Housing Act was signed into law.”
Error processing example 13986: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|████▉     | 996/2000 [21:14<06:59,  2.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|████▉     | 997/2000 [21:15<09:05,  1.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|████▉     | 998/2000 [21:15<07:47,  2.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|████▉     | 999/2000 [21:15<06:41,  2.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|█████     | 1000/2000 [21:16<09:08,  1.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|█████     | 1003/2000 [21:16<05:12,  3.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] The RESTRICT Act would allow the U.S. government to “charge you with a felony and $1 million for having a VPN.”
Error processing example 13987: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The abortion pill has been linked to the deaths of at least 28 women in the U.S. alone.”
Error processing example 13988: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida has “the second lowest tax burden per capita in the United States.”
Error processing example 13989: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Fewer people get hurt playing rugby” than American football.
Error processing example 13990: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “One in 5 Americans has lost a family member to gun violence.”
Error processing example 13991: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In the 1960s, liberals emptied our psych wards."
Error processing example 13992: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A video proves that “our own government is acknowledging that they’re spraying the skies” with toxic chemicals.
Error processing example 13993: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Missouri’s attorney general has a website “where people can report trans individuals and the people who help them.”
Error processing example 13994: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Las personas atraídas a los menores de edad “ya tienen su bandera del orgullo”.
Error processing example 13995: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “1,900 food production companies have been destroyed” as part of conspiracy.
Error processing example 13996: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "U.N. calls for decriminalizing sex with minors.”
Error processing example 13997: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The ocean is "flat" and "contained" by land features that jut up from a flat (not spherical) earth.
Processing examples:  50%|█████     | 1005/2000 [21:16<03:49,  4.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|█████     | 1005/2000 [21:27<03:49,  4.33it/s]Processing examples:  50%|█████     | 1007/2000 [21:28<33:17,  2.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  50%|█████     | 1009/2000 [21:28<23:19,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████     | 1011/2000 [21:29<16:27,  1.00it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████     | 1013/2000 [21:31<17:03,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 13998: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Latino small businesses are 1 in 4 new businesses, but only 1% of venture capital funding goes to Latino businesses."
Error processing example 13999: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Buyers with good credit scores will pay even more to cover for those with bad credit."
Error processing example 14000: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Ron DeSantis sealed "all beaches" in Florida.
Error processing example 14001: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] FedNow is a new “monetary system” that’s “going to cost you.”
Error processing example 14002: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Healthy pilots are suffering from myocarditis and dropping dead on flights” because of the COVID-19 vaccines.
Error processing example 14003: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats “introduced a bill that would ban detaining gay illegal immigrants.”
Error processing example 14004: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Social Security is a “legal Ponzi scheme.”
Error processing example 14005: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Face masks may raise risk of stillbirths, testicular dysfunction and cognitive decline, study warns.”
Error processing example 14006: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] BlackRock owns 59 million shares of Dominion Voting Systems and 45.7 million shares of Fox News. BlackRock sued itself and fired Tucker Carlson as part of the lawsuit.
Error processing example 14007: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The COVID vaccine “has been proven to have negative efficacy.”
Error processing example 14008: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Infertility is treated differently than other issues and "often excluded from insurance coverage"
Processing examples:  51%|█████     | 1016/2000 [21:31<10:35,  1.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████     | 1018/2000 [21:32<10:15,  1.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████     | 1021/2000 [21:32<06:39,  2.45it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████     | 1023/2000 [21:33<06:58,  2.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████▏    | 1025/2000 [21:35<09:57,  1.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14009: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Addressing the impact of the House GOP debt-ceiling bill on veterans’ programs, "I'm dead serious that we're not cutting veterans, and I mean it."
Error processing example 14010: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Fauci, DOD and CDC funded deadly pathogen research at Sudan lab
Error processing example 14011: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Title 42 and other Trump-era holdovers are forcing migrants into dangerous, overcrowded conditions in Mexico.”
Error processing example 14012: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows people collapsing because of COVID-19 vaccines.
Error processing example 14013: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Joe Biden was older on his first day as president than Ronald Reagan was on his last day.”
Error processing example 14014: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Ron DeSantis signed “a last-minute exception into an anti-discrimination law for anyone who also operates a theme park more than 25 acres in Florida,” benefiting Walt Disney Co.
Error processing example 14015: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We pay by far the highest prices in the world for prescription drugs, in some cases 10 times more than the people of any other country."
Error processing example 14016: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Florida elections bill “guts everything” “instead of getting tough, and doing what the people want (same day voting, Voter ID, proof of Citizenship, paper ballots, hand count, etc.).”
Error processing example 14017: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video showing celebrities, politicians and other notable figures with black eyes proves they are part of the Illuminati.
Error processing example 14018: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Texas Republicans just voted to give a Republican appointee the power to single-handedly CANCEL election results in the state’s largest Democratic county.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████▏    | 1027/2000 [21:36<08:29,  1.91it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  51%|█████▏    | 1029/2000 [21:36<06:18,  2.56it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1031/2000 [21:38<08:07,  1.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1032/2000 [21:39<11:48,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1033/2000 [21:40<09:56,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1035/2000 [21:40<06:41,  2.40it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14019: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “mRNA in food is being tested by USDA now.”
Error processing example 14020: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] House Speaker “Kevin McCarthy has quietly implemented a pay raise for members (of Congress) that could be $30,000+ per person. It circumvents the Constitution by instead reimbursing their rent, utilities, & meals.”
Error processing example 14021: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Four North Carolina Republicans made “campaign promises” to protect reproductive rights
Error processing example 14022: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida lawmakers danced “after passing anti-trans legislation.”
Error processing example 14023: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Donald Trump himself wrote a book where he was talking about the need to increase the age of eligibility for Social Security to 70.”
Error processing example 14024: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A chart on Arctic sea ice provides evidence that Al Gore was wrong when he said in 2009 that the north polar ice cap would lose all of its ice “within the next five to seven years.”
Error processing example 14025: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “This year, there have been 13,000 deaths by gun violence. The vast majority, 95% of them, are in inner cities, where you have some of the strictest gun control laws in the country.”
Error processing example 14026: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Texas Gov. Greg Abbott “cut $211 million in mental health funding.”
Error processing example 14027: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California has approved $1.2 million in reparations for every Black resident.
Error processing example 14028: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Duke Health, UNC Health and ECU Health “in North Carolina are diagnosing toddlers who play with stereotypically opposite gender toys as having gender dysphoria and are beginning to transition them!!”
Processing examples:  52%|█████▏    | 1036/2000 [21:41<08:58,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1039/2000 [21:41<05:01,  3.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1041/2000 [21:47<18:23,  1.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1042/2000 [21:47<16:09,  1.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1044/2000 [21:53<25:33,  1.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1045/2000 [21:58<36:28,  2.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1046/2000 [22:01<38:34,  2.43s/it]Error processing example 14029: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Both murderers in Texas were not only illegals, but gang members” and were “most likely tied to MS-13.”
Error processing example 14030: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Chelsea Clinton said, “It’s time to force-jab every unvaccinated child in America.”
Error processing example 14031: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Amazon’s “Utopia” TV series predicted the COVID-19 pandemic and an intentional effort to sterilize humans, showing the pandemic was planned.
Error processing example 14032: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The New York state budget covers 96% of eligible preschoolers
Error processing example 14033: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An Instagram video shows a man confronting President Joe Biden about an inappropriate relationship with a 13-year-old girl.
Error processing example 14034: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Republican plan would cut federal law enforcement officers — 30,000 — including 11,000 FBI agents, 2,000 border agents, DEA agents, and so on."
Error processing example 14035: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Many of these teachers are soldiers, ex-soldiers, ex-policeman.”
Error processing example 14036: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In Congress Ron DeSantis pushed a 23% national sales tax.”
Error processing example 14037: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “(Donald) Trump ran up more debt than any other president in American history.”
Error processing example 14038: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 835.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.76 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] This video shows empty shelves at a Florida grocery store because truckers said “they were not delivering anything to Florida !!!”
Error processing example 14039: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▏    | 1048/2000 [22:04<33:29,  2.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  52%|█████▎    | 1050/2000 [22:06<26:56,  1.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1053/2000 [22:06<15:32,  1.02it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1054/2000 [22:06<13:09,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1056/2000 [22:11<19:39,  1.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "Chinese communist” troops on the ground at U.S. southern border.
Error processing example 14040: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Americans will get $1.1 B in rebates from health insurance companies this year cuz of a provision I wrote in the ACA.”
Error processing example 14041: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Barack Obama said that Iran has a “right” to nuclear technology, including weapons.
Error processing example 14042: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Pop-Tarts and other foods “have antifreeze in them.”
Error processing example 14043: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “U.S. military is now testing soldiers for AIDS, after DOD database reports 500% increase in HIV since the COVID vaccine rollout.”
Error processing example 14044: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] North Korean leader Kim Jong-un “just ordered the evacuation of the capital city.”
Error processing example 14045: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The NEXT pandemic is here and children are the target, says Bill Gates.”
Error processing example 14046: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says a report shows that “Ventilators Killed Nearly ALL COVID Patients”
Error processing example 14047: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A printer error in Nassau County, New York, turned every registered voter Democratic.
Error processing example 14048: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "NYC mayor plans to track and limit meat and dairy."
Error processing example 14049: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Just last year,” Miami-Dade Public Schools “had over 14,000 new children, 10,000 of which came from four countries of Cuba, Nicaragua, Venezuela and Haiti.”
Error processing example 14050: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1059/2000 [22:11<11:52,  1.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1061/2000 [22:11<08:46,  1.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1063/2000 [22:13<10:35,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1065/2000 [22:14<10:53,  1.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1067/2000 [22:15<08:42,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Said he was “able to convince India, Australia, Japan, and the United States to form an organization called the Quad to maintain stability in the Indian Ocean and the South China Sea."
Error processing example 14051: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An Arizona judge was “forced to overturn” an election and ruled “274,000 ballots must be thrown out.”
Error processing example 14052: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] COVID-19 vaccines administered during pregnancy drove an increase in cases of myocarditis in babies.
Error processing example 14053: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The share of working-age men choosing to work is the lowest it has ever been.”
Error processing example 14054: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows boxes of books removed from a middle school library that were “banned by the DeSantis regime in Florida.”
Error processing example 14055: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “AIDS-associated diseases and cancers have increased by 338x since the rollout of the COVID-19 vaccines, according to the CDC and foreign government bodies."
Error processing example 14056: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Arizona BANS Electronic Voting Machines.”
Error processing example 14057: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Ron DeSantis voted against the wall.”
Error processing example 14058: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] 30 tons of lost ammonium nitrate on a California-bound train suggest an orchestrated conspiracy.
Error processing example 14059: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There's not been a single book banned in the state of Florida.”
Error processing example 14060: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Ron DeSantis said Puerto Rico’s flag can’t be displayed in Florida.
Processing examples:  53%|█████▎    | 1068/2000 [22:15<07:36,  2.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  53%|█████▎    | 1069/2000 [22:17<12:01,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▎    | 1070/2000 [22:17<09:44,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▎    | 1071/2000 [22:19<14:54,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▎    | 1073/2000 [22:19<09:20,  1.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1075/2000 [22:19<06:13,  2.48it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1076/2000 [22:20<08:18,  1.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1077/2000 [22:20<06:51,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14061: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump "wanted to amnesty 2 million illegal aliens in 2018 when he was president."
Error processing example 14062: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Miami-Dade County has banned” Amanda Gorman’s poem “from elementary schools.”
Error processing example 14063: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 339.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The average amount of assistance for the Supplemental Nutrition Assistance Program is $6 a day.
Error processing example 14064: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Every study has shown” that when work requirements are tied to federal safety-net programs, “it puts more people to work.”
Error processing example 14065: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Wisconsin has historically … and I think largely continues to be, a blue state.”
Error processing example 14066: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A movement representing "minor attracted people" has adopted a flag that is being integrated into LGBTQ+ advocacy.
Error processing example 14067: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The European Union is now warning pregnant women not to get the COVID-19 vaccine due to the possibility of infertility and miscarriage.”
Error processing example 14068: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “New tapes” of Nancy Pelosi show that the Jan. 6, 2021, attack on the U.S. Capitol “was all planned.”
Error processing example 14069: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Having "biological boys … in their locker rooms” is a reason why “a third of our teenage girls seriously contemplated suicide last year."
Error processing example 14070: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Atrazine in the water supply is contributing to "sexual dysphoria" in kids.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1080/2000 [22:22<06:54,  2.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1082/2000 [22:22<05:13,  2.93it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1083/2000 [22:24<10:22,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1085/2000 [22:25<08:14,  1.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1086/2000 [22:25<08:19,  1.83it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1087/2000 [22:26<08:15,  1.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14071: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] During the spring 2020 election in Milwaukee, a reduction to only five polling locations “came days before the election and was done without proper notice to voters.”
Error processing example 14072: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump “never changed one immigration law.”
Error processing example 14073: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Pfizer funneled $12 million to CNN anchor Anderson Cooper as part of a deal to promote mRNA COVID jabs to the American public.”
Error processing example 14074: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] This video shows a cartel member in Mexico carrying a Javelin that the U.S. sent to Ukraine.
Error processing example 14075: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “A place like California, they give (immigrants in the country illegally) benefits. They give unemployment checks.”
Error processing example 14076: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Particulate matter, found in wildfire smoke, has “no effect” on human health and is “total junk science.”
Error processing example 14077: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The family separation policy actually began under the Obama administration.”
Error processing example 14078: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “They are trying to throw Trump in jail for the rest of his life for having a couple dozen documents that he had authority to declassify.”
Error processing example 14079: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In the U.S., “a person can be married in the morning and thrown out of a restaurant for being gay in the afternoon.”
Error processing example 14080: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] World Economic Forum “calls for AI to rewrite Bible, create ‘religions that are actually correct’”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  54%|█████▍    | 1089/2000 [22:26<05:22,  2.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▍    | 1090/2000 [22:26<05:30,  2.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▍    | 1093/2000 [22:27<04:01,  3.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▍    | 1094/2000 [22:33<21:19,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▍    | 1095/2000 [22:34<20:42,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▍    | 1097/2000 [22:35<13:16,  1.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14081: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Displaying the progress pride flag between two American flags during a White House pride event was “in breach of US Flag Code.”
Error processing example 14082: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Per capita, more Floridians move to California than Californians moving to Florida.”
Error processing example 14083: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The White House Gift Shop is putting out a commemorative coin for Donald Trump's indictment.”
Error processing example 14084: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Monkey virus DNA found in COVID-19 shots”
Error processing example 14085: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Twenty-six out of every 100 students” in K-12 speak Spanish.
Error processing example 14086: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Burisma Energy accountant, who blew the whistle on Biden bribery scheme, found dead.”
Error processing example 14087: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Videos and photos of military vehicles are proof of “war prep” and looming military action within the U.S.
Error processing example 14088: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Protests of (Sisters of Perpetual Indulgence) at a Los Angeles Dodgers game led to a "virtually empty stadium for the game itself.”
Error processing example 14089: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Young Americans across this country are no longer proud to be American.”
Error processing example 14090: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “New California bill charges parents with child abuse for refusing to ‘affirm’ their kid’s gender identity.”
Error processing example 14091: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▌    | 1100/2000 [22:36<09:45,  1.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▌    | 1104/2000 [22:36<05:28,  2.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  55%|█████▌    | 1107/2000 [22:36<03:50,  3.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “Think about parents who show up at school board meetings. By this DOJ, they’re called domestic terrorists.”
Error processing example 14092: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Justin Trudeau is being investigated by the (Royal Canadian Mounted Police).”
Error processing example 14093: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] J.P. Morgan y la familia Rothschild planificaron que el Titanic se hundiera para eliminar a Jacob Astor y crear la Reserva Federal.
Error processing example 14094: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 2020 the ballots that were counted "were fake ballots."
Error processing example 14095: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Median income in this country is … $35,000 a year. Because of Joe Biden's inflation, the average monthly expenses have increased by $500, almost.”
Error processing example 14096: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Audio is of “the knocking they caught from the submarine” that imploded on the way to view the Titanic wreckage.
Error processing example 14097: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video suggests COVID-19 vaccines are responsible for breast cancer deaths in women under age 45 rising from 26,000 in 2021 to 297,000 in 2023.
Error processing example 14098: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] West Virginia University has “increased the amount of unfunded institutional aid provided to students and expects this amount to exceed $134 million in 2024.”
Error processing example 14099: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The fact is, today abortion law in the United States is more aligned with China and North Korea than with Western nations in Europe.”
Error processing example 14100: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] China sends “fentanyl into America killing 80,000 Americans a year.”
Error processing example 14101: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says that under his presidency, the unemployment rate has “been below 4% for the longest stretch in 50 years in American history.”
Processing examples:  55%|█████▌    | 1109/2000 [22:38<07:34,  1.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1111/2000 [22:39<06:36,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1113/2000 [22:43<12:05,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1115/2000 [22:43<08:57,  1.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1117/2000 [22:45<10:47,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1119/2000 [22:45<08:01,  1.83it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14102: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump is an “affirmative action advocate.”
Error processing example 14103: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Bill Gates is behind the five recent cases of malaria in the United States.
Error processing example 14104: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “A lot of your chain-food restaurants have been using (lab-grown meat) for years.”
Error processing example 14105: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gender Queer on NEA Summer Reading List for Kids.
Error processing example 14106: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Deaths of four alternative medicine doctors show they are being targeted for their ideas.
Error processing example 14107: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows that Attorney General Merrick Garland “cries like kid after Taylor Greene calls out stupid ‘Trump’s Arraignment.’”
Error processing example 14108: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] AMC is intentionally deterring people from seeing 'Sound of Freedom' with no A/C, fire alarms and other issues.
Error processing example 14109: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Federal “agencies and government have grown 50% since 2019.”
Error processing example 14110: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "If you get pulled over driving in Florida and your license is from Vermont, Delaware, RI, Connecticut or Hawaii, your license is invalid."
Error processing example 14111: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The percentage of students that are going to college (in West Virginia) has been declining for five years.”
Error processing example 14112: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There have been more illegal encounters under Biden than the previous two administrations combined.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1122/2000 [22:45<05:14,  2.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▌    | 1124/2000 [22:49<11:18,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▋    | 1126/2000 [22:49<08:38,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▋    | 1127/2000 [22:53<15:44,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▋    | 1128/2000 [22:53<13:54,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  56%|█████▋    | 1129/2000 [22:58<25:00,  1.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14113: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under the “2023 Solar Incentive Program,” U.S. homeowners can “now qualify to get a full, state-of-the-art solar system at absolutely no cost from the government.”
Error processing example 14114: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Stop getting mammograms immediately as they are outdated and dangerous!” Thermography “is a safer way to test for breast cancer!”
Error processing example 14115: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The sand storm headed our way is an attempt by Bill Gates to block out the sun.”
Error processing example 14116: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “COVID-19 is targeted to attack Caucasians and Black people. The people who are most immune are Ashkenazi Jews and Chinese.”
Error processing example 14117: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Helicopter released deadly mosquitoes” at Baltimore’s AFRAM Festival.
Error processing example 14118: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “You can work at some McDonald’s in this country for more money than … signing up for the military.”
Error processing example 14119: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida has “the highest” homeowners insurance in the nation.
Error processing example 14120: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Biden's America, 7 million people illegally crossed the border and are given a free cellphone and airline tickets."
Error processing example 14121: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Videos show Planned Parenthood “aiding sex traffickers in their crimes.”
Error processing example 14122: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The Inflation Reduction Act led to higher taxes.”
Error processing example 14123: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1131/2000 [22:58<15:51,  1.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1133/2000 [22:58<10:35,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1134/2000 [22:59<11:25,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1136/2000 [22:59<07:56,  1.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1137/2000 [23:01<12:25,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1138/2000 [23:03<15:50,  1.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1141/2000 [23:04<09:05,  1.58it/s]
[CLAIM] Immigrants are supposed to “apply for asylum in the country where they are, and then they try to come into the United States.”
Error processing example 14124: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In some liberal states, you actually have post-birth abortions.”
Error processing example 14125: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Prosecutor Jack Smith “only targets Republicans.”
Error processing example 14126: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Borax can be used to treat a variety of health problems, including joint pain, kidney stones, osteoporosis and chronic fatigue.
Error processing example 14127: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Tens of millions of Americans live in communities where they cannot find a doctor while others have to wait months to be seen.”
Error processing example 14128: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows a van “‘Queer testing’ kids without parental approval … (that) involves clothing removal.”
Error processing example 14129: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida “decided middle school students will be taught that enslaved people benefited from slavery.”
Error processing example 14130: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Two pipeline workers in the Chicago area were given fentanyl-laced water bottles.
Error processing example 14131: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There is no such thing as a trans kid.”
Error processing example 14132: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Some states have “tried to ban teaching Latino and Hispanic history.”
Error processing example 14133: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] These images show LeBron James dressed in pink for the “Barbie” movie.
Error processing example 14134: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1142/2000 [23:05<10:11,  1.40it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1144/2000 [23:05<08:08,  1.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1146/2000 [23:06<06:35,  2.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  57%|█████▋    | 1148/2000 [23:08<08:19,  1.71it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1151/2000 [23:08<05:09,  2.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “Luke Bryan pulls his videos from CMT: ‘Time for the Bud Light treatment.’”
Error processing example 14135: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Under ‘Crooked Joe’ Biden, there has been a catastrophic increase in shortages of essential medicines.”
Error processing example 14136: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Doctors in the Netherlands “ordered” to euthanize “citizens with autism and other minor disabilities, even if the patient does not currently express any desire to die."
Error processing example 14137: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The government just stated under oath that they are in possession of UFOs and nonhuman alien bodies.”
Error processing example 14138: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Potato chips, KitKat bars and Viagra are not taxed in Wisconsin because they are considered "essential."
Error processing example 14139: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The No. 1 weather-related killer is heat. 600 people die annually from its effects, more than from floods, hurricanes and tornadoes in America combined.”
Error processing example 14140: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Barack Obama “removed 500,000 pedophiles from background check database.”
Error processing example 14141: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wages are up after being adjusted for inflation."
Error processing example 14142: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Trump administration “built nearly 500 miles of border wall.”
Error processing example 14143: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Gov. Ron DeSantis “opposed my China tariffs and he heartlessly opposed” $28 billion for farm relief, which was paid for by China.
Error processing example 14144: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photos of Barack Obama golfing with bandaged fingers are proof of his involvement in Tafari Campbell’s death.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1153/2000 [23:08<03:59,  3.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1155/2000 [23:09<05:08,  2.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1156/2000 [23:09<04:36,  3.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1157/2000 [23:13<13:39,  1.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1160/2000 [23:13<07:45,  1.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14145: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Illinois is now letting illegal aliens become police officers."
Error processing example 14146: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Consumer confidence is the highest it’s been in years.”
Error processing example 14147: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There are polls that also say I have great approval ratings.”
Error processing example 14148: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Judge Tanya Chutkan “is the only federal judge in Washington, D.C., who has sentenced Jan. 6 defendants to sentences longer than the government had requested.”
Error processing example 14149: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Douglass Mackey was convicted “because he made a meme in the 2016 election.”
Error processing example 14150: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "FBI director Wray confirms Joe Biden is under criminal investigation for Ukrainian bribes by the U.S. attorney in Delaware then tries to backpedal.”
Error processing example 14151: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’ve eliminated critical race theory in our K through 12 schools.”
Error processing example 14152: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “They are trying to make it illegal to question the results of a bad election.”
Error processing example 14153: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "WEF chairman Klaus Schwab openly calls for AI technology to replace Democratic elections!”
Error processing example 14154: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “During Biden’s first 30 months in office, just 2.1 million new jobs were created, and by contrast, during my first 30 months in office we created 4.9 million new jobs.”
Error processing example 14155: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1163/2000 [23:14<06:52,  2.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1166/2000 [23:14<04:33,  3.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1168/2000 [23:14<03:36,  3.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  58%|█████▊    | 1170/2000 [23:21<13:51,  1.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▊    | 1172/2000 [23:24<15:45,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] A bill sponsored by U.S. Sen. Bernie Sanders would “increase government control” of pharmacy benefit managers and is a “radical health care takeover.”
Error processing example 14156: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I’ve already” declared a national climate emergency
Error processing example 14157: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The FDA is now saying that it’s OK to take ivermectin if you have COVID.”
Error processing example 14158: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Wildfires do not completely burn out cars, glass and all, yet leaving nearby trees and utility poles still standing upright.”
Error processing example 14159: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Maui fires are part of an intentional effort to rebuild the island into a “smart island.”
Error processing example 14160: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In three short years, we achieved energy independence.”
Error processing example 14161: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Gov. Roy Cooper is blocking a pioneering school choice bill that would give thousands of low-income parents the funds to allow their kids to get a high-quality education.”
Error processing example 14162: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Credit card debt is above $1 trillion for the FIRST TIME EVER."
Error processing example 14163: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Democrat-sponsored bill proposes a “1,000% gun tax on most firearms.”
Error processing example 14164: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “A higher percentage of American workers are working today than ever before.”
Error processing example 14165: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Fulton County District Attorney Fani Willis “campaigned and raised money on, ‘I will get Trump.’”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▊    | 1174/2000 [23:24<11:31,  1.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1175/2000 [23:25<12:02,  1.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1177/2000 [23:25<08:36,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1178/2000 [23:26<10:02,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1180/2000 [23:26<06:47,  2.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1183/2000 [23:31<12:39,  1.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14166: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows Hawaii Gov. Josh Green “wants to turn Lahaina Maui into state lands. All planned for smart city."
Error processing example 14167: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When elected Georgia governor in 2018, “Brian Kemp was over $7 million in debt.” Kemp “cut a deal” to select Dominion Voting Systems as Georgia’s voting machine vendor and “has never been in debt since.”
Error processing example 14168: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Since Biden took office ... real wages have fallen monthly.”
Error processing example 14169: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Foreign languages are “not a high priority” nationally
Error processing example 14170: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “After China sent a balloon over strategic sites in the United States … President Biden literally sent the secretary of state hat in hand to go kowtow and ask for a meeting.”
Error processing example 14171: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Official statement — Putin assassinated in the Kremlin!”
Error processing example 14172: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Jim Justice "opposes the Right to Work”
Error processing example 14173: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Jim Justice “supports banning sporting rifles for law-abiding citizens.”
Error processing example 14174: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Los Angeles Dodgers Stadium is flooded after mocking God!”
Error processing example 14175: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Rick Scott “wants to … end Social Security and Medicare coverage.”
Error processing example 14176: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Smart-city plans are behind wildfires in Tenerife.
Processing examples:  59%|█████▉    | 1184/2000 [23:31<11:46,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1185/2000 [23:32<10:39,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1187/2000 [23:32<08:21,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  59%|█████▉    | 1189/2000 [23:34<10:15,  1.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1190/2000 [23:35<08:35,  1.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1191/2000 [23:37<14:03,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1192/2000 [23:37<11:09,  1.21it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1193/2000 [23:38<09:08,  1.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14177: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “#Bidenomics has led to mortgage rates hitting a 21-year high → over 7%.”
Error processing example 14178: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Norway, Finland, Sweden, Holland, and the UK have now banned gender transition surgery for minors.”
Error processing example 14179: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "I actually pushed the Deficit Reduction Act. That was the last time we actually reduced the national debt in the United States, when I was the leader of House conservatives."
Error processing example 14180: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Thanks to negotiating drug prices "our seniors, instead of paying $500 a month for drugs, are paying no more than 35 cents."
Error processing example 14181: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Americans “currently are making less than at the height of the Great Depression.”
Error processing example 14182: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The reality is more people are dying of bad climate change policies than they are of actual climate change.”
Error processing example 14183: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Under Jim Justice,” West Virginia had “the lowest unemployment rate in state history.”
Error processing example 14184: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Since Joe Biden “has taken office, unemployment in our (Latino) community has been cut in half."
Error processing example 14185: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] GOP-authored elections bill “requires valid votes to be tossed out … if a computer rejects a signature.”
Error processing example 14186: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Fewer than 2% of students are impacted whatsoever by the decisions we're making” to cut the budget.
Processing examples:  60%|█████▉    | 1194/2000 [23:41<18:16,  1.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1195/2000 [23:42<19:16,  1.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1196/2000 [23:43<15:50,  1.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1197/2000 [23:44<15:00,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|█████▉    | 1198/2000 [23:46<19:21,  1.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|██████    | 1200/2000 [23:46<10:54,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|██████    | 1201/2000 [23:47<09:50,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|██████    | 1203/2000 [23:47<06:08,  2.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14187: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows Donald Trump believes “there has to be some form of punishment” for women who have abortions.
Error processing example 14188: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A discrepancy in the number of ballots and voters in the 2022 general election in New York “appears to be a federal crime.”
Error processing example 14189: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "America’s C40 cities will ban meat, dairy, new clothes, private cars by 2030.”
Error processing example 14190: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Mike Pence on Jan. 6, 2021, could have approved the 2020 electoral votes on the condition that Congress pass an overhaul of election law.
Error processing example 14191: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The WHO is developing a pandemic treaty that will remove current human rights protections, enforce surveillance and censorship and eliminate freedom of speech.
Error processing example 14192: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “A few years back,” Mitchell Stadium in Bluefield, W.Va., “was voted the best high school football field in America.”
Error processing example 14193: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. inflation is "now down close to 3%, the lowest among the world’s leading economies."
Error processing example 14194: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “According to the Congressional Budget Office,” the Inflation Reduction Act “will save the federal government $160 billion over the next 10 years because Medicare will be paying less for the prescription drugs they’re making available to seniors.”
Error processing example 14195: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Gas prices have skyrocketed since Joe Biden took office and continue to do so everyday.”
Error processing example 14196: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A new Florida law says “a father must submit a DNA test before signing the birth certificates.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|██████    | 1206/2000 [23:48<05:54,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  60%|██████    | 1210/2000 [23:48<03:23,  3.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████    | 1213/2000 [23:58<15:56,  1.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14197: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "(President Joe) Biden is trying to limit you to two beers per week.”
Error processing example 14198: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Bill Gates-funded research into genetically engineered cattle ticks is responsible for red-meat allergies from alpha-gal syndrome in the U.S.
Error processing example 14199: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Under the Obama-Biden administration, we invested hundreds of millions of dollars in the state of Florida replacing wooden power poles with steel poles and we buried these electric lines."
Error processing example 14200: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Derrick Van Orden is one of the only members of Congress who participated in the deadly January 6th insurrection. He now serves in the building he tried to burn down."
Error processing example 14201: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A video shows Florida Gov. Ron DeSantis dropping out of the 2024 presidential race.
Error processing example 14202: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Just in the last four years, $80 billion in federal funding has come to West Virginia — $22 billion more than we've ever received.”
Error processing example 14203: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Burning Man was a national emergency because it was flooded.”
Error processing example 14204: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Joe Manchin’s “support for the (Inflation Reduction Act) could cost West Virginia 100,000 fossil fuel jobs.”
Error processing example 14205: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under Gov. Ron DeSantis, “in the last five years, spending has gone up 30%" in Florida.
Error processing example 14206: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] E. Gordon Gee’s university projects have increased “WVU’s debt load by 55%.”
Processing examples:  61%|██████    | 1214/2000 [24:01<19:51,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████    | 1215/2000 [24:02<17:54,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████    | 1216/2000 [24:02<15:03,  1.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████    | 1217/2000 [24:04<18:29,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████    | 1220/2000 [24:05<10:00,  1.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████    | 1222/2000 [24:05<07:03,  1.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14207: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 105.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] During E. Gordon Gee's presidency of West Virginia University, "student enrollments have steadily decreased."
Error processing example 14208: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Modified mosquitoes have officially made it to Ohio.”
Error processing example 14209: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 113.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.47 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Donald Trump spent 9/11 marching into a war zone of fire and ashes with an army of his own men to save Americans.”
Error processing example 14210: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin Republicans are backing “a nonpartisan redistricting plan based off the Iowa Model.  ... Republicans, Democrats, and the Governor pushed this plan last time redistricting happened in 2020.”
Error processing example 14211: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “2022 was the biggest tourism year ever in West Virginia”
Error processing example 14212: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We know the bank records show that nearly $20 million in payments were directed to the Biden family members and associates through various shell companies.”
Error processing example 14213: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Hawaii wildfire was intentionally set because there’s “120 million tons of lithium in Maui.”
Error processing example 14214: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Giant human skeletons as large as 36 feet have been “unearthed and documented” in historical records.
Error processing example 14215: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The repeal of Roe v. Wade has exacerbated the state’s OB-GYN shortage as residency programs report a decline in enrollment due to the inability of residents to receive training in-state."
Error processing example 14216: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I taught at the University of Pennsylvania for four years.”
Processing examples:  61%|██████    | 1224/2000 [24:06<06:52,  1.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████▏   | 1225/2000 [24:06<05:53,  2.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████▏   | 1226/2000 [24:06<05:08,  2.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  61%|██████▏   | 1227/2000 [24:07<06:51,  1.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1230/2000 [24:07<03:41,  3.47it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1232/2000 [24:07<02:53,  4.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1234/2000 [24:10<06:44,  1.89it/s]Error processing example 14217: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I really don’t know who gave (Dr. Anthony Fauci) the commendation.”
Error processing example 14218: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Florida sort of had a mandate because they were giving the vaccine, they were demanding everybody take the vaccine.”
Error processing example 14219: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 111.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.23 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Tony Evers and Democrats "rejected our (Iowa model redistricting) proposal to enact the very plan they originally endorsed."
Error processing example 14220: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “CEOs are now making 400 times more than their average worker.”
Error processing example 14221: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The price of "bacon is up five times" under President Joe Biden.
Error processing example 14222: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An emergency broadcast system test will “send a specific high-frequency signal … with the intention of activating graphene oxide” in people.
Error processing example 14223: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Texas teacher fired for reading Diary of Anne Frank to class.”
Error processing example 14224: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York City is spraying unsafe pesticides to combat West Nile virus.
Error processing example 14225: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 109.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. aid to Ukraine will continue during a government shutdown, but programs for food stamps, transportation, education, justice, environmental protection and homelessness will not.
Error processing example 14226: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden "put 20 million people on Medicaid who aren’t even eligible, then stopped states from taking them off."
Error processing example 14227: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1237/2000 [24:10<04:16,  2.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1240/2000 [24:12<06:19,  2.00it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1241/2000 [24:13<06:14,  2.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1243/2000 [24:14<07:15,  1.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “Joe Biden grants voting rights to 500,000 invaders.”
Error processing example 14228: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The life span of a wind tower generator lasts just three to four years.
Error processing example 14229: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The North Carolina state budget includes “the largest cut in our personal income tax that we’ve ever seen.”
Error processing example 14230: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We are more energy independent today” under President Joe Biden.
Error processing example 14231: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Ron DeSantis is against fracking. He is against drilling.”
Error processing example 14232: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 107.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.48 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When I took office, “the auto industry was on its knees gasping its last breath.” Trump administration “tariffs and taxes saved the American auto industry from extinction."
Error processing example 14233: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Being transgender is a "mental health disorder."
Error processing example 14234: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Washington, D.C., allows noncitizens to vote in (municipal elections)" so a Russian citizen, after living in Washington for 30 days can "vote for mayor in Washington, D.C. And they don’t even need to show their ID to do it.”
Error processing example 14235: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] James Biden’s text message to Hunter Biden that says, “I can work with you(r) father alone,” shows that Joe Biden was “in business” with Hunter Biden.
Error processing example 14236: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Six million or 7 million people have “come illegally under Biden.”
Error processing example 14237: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Since the mid-'90s, this country has been governed by revolving continuing resolution and omnibus spending bill(s). … That is the reason we're $33 trillion in debt."
Processing examples:  62%|██████▏   | 1245/2000 [24:16<09:27,  1.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1247/2000 [24:17<06:51,  1.83it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  62%|██████▏   | 1249/2000 [24:17<05:02,  2.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  63%|██████▎   | 1251/2000 [24:19<07:49,  1.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  63%|██████▎   | 1253/2000 [24:19<05:41,  2.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14238: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York Attorney General Letitia James and Judge Arthur Engoron valued Mar-a-Lago at $18 million.
Error processing example 14239: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Flu shot ingredients include aborted human cell cultures, monkey kidney cells and antifreeze.
Error processing example 14240: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows a family learning their debt was resolved by the “American Debt Relief program” for American citizens and legal residents who “have over $20,000” in credit card debt.
Error processing example 14241: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] There are "about a half a dozen" transgender athletes competing in Wisconsin K-12 schools.
Error processing example 14242: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Milwaukee Brewers “are complaining about a lack of transit going to the ballpark at the same time as they are testifying in favor (of) taking $135 million away from Milwaukee County, which funds transit."
Error processing example 14243: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration “wanted Israel to stand down after the attack" by Hamas.
Error processing example 14244: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Sadly, American taxpayer dollars helped fund these attacks, which many reports are saying came from the Biden administration.”
Error processing example 14245: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The same people that raided Israel are pouring into our once beautiful USA, through our TOTALLY OPEN SOUTHERN BORDER, at Record Numbers.”
Error processing example 14246: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Biden is “funding every angle” of the Israel-Hamas war.
Error processing example 14247: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] It has been U.S. policy “for at least 79 years" to require Americans who intend to be evacuated from overseas to sign a promissory note.
Processing examples:  63%|██████▎   | 1255/2000 [24:20<06:01,  2.06it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  63%|██████▎   | 1258/2000 [24:20<03:50,  3.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  63%|██████▎   | 1260/2000 [24:22<04:48,  2.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  63%|██████▎   | 1263/2000 [24:22<03:21,  3.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14248: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] China is “the largest developer of neuro-strike weapons, weapons engineered to change the brain activity of military commanders and segments of the population.”
Error processing example 14249: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Kids who don’t even know what day it is or their colors can decide to permanently change themselves” through medical gender transition.
Error processing example 14250: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows North Korean President Kim Jong Un blaming President Joe Biden for the Israel-Hamas war and Russia’s invasion of Ukraine, and saying, “I support Donald Trump for President in 2024.”
Error processing example 14251: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "95% of over-the-counter pharmaceuticals" in the U.S. come from China.
Error processing example 14252: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republicans in Congress are “trying to wipe out federal funding to end the HIV epidemic.”
Error processing example 14253: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Nikki Haley argues in support of bringing Gaza refugees to America.”
Error processing example 14254: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Democrats are pushing “to take in a million Palestinian refugees.”
Error processing example 14255: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Since 1981, the state Senate has only rejected five executive appointments. The GOP today is more than doubling that number.”
Error processing example 14256: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Some individuals or businesses in Wisconsin are charging $2 or $2.25 an hour for child care.
Error processing example 14257: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In the last 11 months, Customs and Border Patrol has averaged 200,000 apprehensions a month, and you know how many beds we have to detain people at the border? Thirty-eight thousand."
Processing examples:  63%|██████▎   | 1265/2000 [24:25<06:55,  1.77it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  63%|██████▎   | 1269/2000 [24:25<04:07,  2.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▎   | 1271/2000 [24:27<06:26,  1.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▎   | 1273/2000 [24:27<05:36,  2.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14258: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Russia has hypersonic missile capabilities ahead of that of the U.S.”
Error processing example 14259: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Campaign finance disclosures showed Fulton County District Attorney Fani Willis took part in a “massive money laundering and election fraud” scheme.
Error processing example 14260: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The FDA is required to take the COVID vaccines off the market” because they are “adulterated.”
Error processing example 14261: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gasoline prices are "up 63%" under President Joe Biden.
Error processing example 14262: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “When I came into office, Iran had $70 billion in foreign exchange reserves. … By the time I left, they had nothing. They were broke.”
Error processing example 14263: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “This map was drawn entirely in secret by an Ohio consultant … with taxpayer dollars.”
Error processing example 14264: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The Republicans and Trump funded $400 million in March of 2020 for mail ballots.”
Error processing example 14265: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “It is harder to buy Sudafed than an AR-15 in Maine.”
Error processing example 14266: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. House Speaker Mike Johnson "opposes Social Security benefits."
Error processing example 14267: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “New court documents reveal that George Floyd died of a fentanyl overdose.”
Error processing example 14268: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Ron DeSantis “actually sponsored the bill to make Puerto Rico a state.”
Processing examples:  64%|██████▍   | 1276/2000 [24:28<03:47,  3.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▍   | 1278/2000 [24:28<03:00,  4.00it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▍   | 1280/2000 [24:30<05:48,  2.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▍   | 1282/2000 [24:30<04:32,  2.64it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▍   | 1284/2000 [24:33<07:40,  1.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▍   | 1286/2000 [24:33<05:39,  2.10it/s]Error processing example 14269: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Because of our economic policies we now are reducing inflation.”
Error processing example 14270: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Gen Z is divided 50-50 on whether they support Hamas or Israel.”
Error processing example 14271: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Haaretz investigation reveals discrepancies in Israel’s reporting on October 7th death toll.”
Error processing example 14272: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ohio’s Issue 1 “allows for abortion after viability for…financial reasons.”
Error processing example 14273: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Over the past four decades, we’ve lost over 400,000 farms in America (and) over 140 million acres of farmland. And that’s an area roughly equal to the size of Minnesota, North and South Dakota combined.”
Error processing example 14274: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin elections administrator Meagan Wolfe “allowed for the use of illegal drop boxes and ballot harvesting.”
Error processing example 14275: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “It’s been revealed that Fauci brought COVID to the Montana one year before COVID broke out in the U.S.!”
Error processing example 14276: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin elections administrator Meagan Wolfe “refuses to clean up our voter rolls”
Error processing example 14277: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin elections administrator Meagan Wolfe “permitted the ‘Zuckerbucks’ influence money.”
Error processing example 14278: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Jim Justice “broke his pledge, raised the gas tax.”
Error processing example 14279: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  64%|██████▍   | 1288/2000 [24:35<07:58,  1.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▍   | 1291/2000 [24:36<06:30,  1.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▍   | 1292/2000 [24:41<13:59,  1.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▍   | 1295/2000 [24:42<10:26,  1.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “Only in Washington can you cut funding, add a pay-for to a new spending measure, and they say it's terrible for the deficit.”
Error processing example 14280: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We produce a lot more wind energy in Texas than California does.”
Error processing example 14281: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Acapulco destroyed by yet another directed weather attack!”
Error processing example 14282: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We sell arms to 60% of the world's autocrats. We are the world's largest arms exporters."
Error processing example 14283: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "On most campuses, you cannot destroy a chalked message unless you were the one that wrote it. You can't dump water on it, you can't even walk over it."
Error processing example 14284: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Bucks star Giannis Antetokounmpo signed a three-year contract for $186 million. He and his family, being African American, could receive benefits under affirmative action.”
Error processing example 14285: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Looking at “American aid to other countries … the vast majority of it is military aid.”
Error processing example 14286: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] An exemption in a new Florida law “allowed Chinese nationals to buy land within a 20-mile radius of a military base, lobbied for by one of (Gov. Ron DeSantis’) donors.”
Error processing example 14287: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says U.S. Sen. Tammy Baldwin “voted to send hundreds of millions to Iran” to bankroll “radicals like Hezbollah and Hamas.”
Error processing example 14288: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ron DeSantis “wants to cut Social Security and Medicare.”
Error processing example 14289: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In 1992, this country made a promise to Ukraine. We said if you return nuclear missiles that were part of the old Soviet Union to Russia, and they invade you, we will protect you.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▍   | 1298/2000 [24:43<06:59,  1.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▍   | 1299/2000 [24:46<11:52,  1.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▌   | 1300/2000 [24:46<10:08,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▌   | 1302/2000 [24:48<10:00,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▌   | 1304/2000 [24:49<09:35,  1.21it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▌   | 1305/2000 [24:50<09:41,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14290: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Just like the Cuban regime, the Biden administration is trying to put their political opponents in jail, shutting down free speech.”
Error processing example 14291: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New York’s fraud case against Trump “has nothing to do with his home in Mar-a-Lago.”
Error processing example 14292: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says expanding Medicaid in Wisconsin would save the state $530 million, which could pay for 3,287 K-12 teachers, or 7.8 educators per district.
Error processing example 14293: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Unfortunately during COVID, the clerks in Madison and Milwaukee told their constituents that they were actually indefinitely confined and did not have to come and vote, which was incorrect."
Error processing example 14294: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] North Carolina Attorney General and gubernatorial candidate Josh Stein “supports taxpayer-funded abortion on demand with no restrictions up until birth.”
Error processing example 14295: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Footage from Israeli helicopter shows the IDF killing many people at October 7 concert in Israel.”
Error processing example 14296: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In order to be a poll worker on the conservative side, you have to register through the Republican Party of Wisconsin."
Error processing example 14297: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida Gov. Ron DeSantis “did something six months ago” akin to Nikki Haley recruiting Chinese companies to South Carolina.
Error processing example 14298: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rise in U.S. infant mortality rate connected to COVID-19 vaccines.
Error processing example 14299: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] CNN and The Associated Press “admitted” to knowing about Hamas’ Oct. 7 attack in advance but didn’t tell anyone.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  65%|██████▌   | 1308/2000 [24:51<06:52,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1310/2000 [24:54<09:43,  1.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1311/2000 [24:55<09:33,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1313/2000 [24:55<06:40,  1.71it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1317/2000 [25:00<10:31,  1.08it/s]Error processing example 14300: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New Hampshire has “a worse drug problem per capita than any other state."
Error processing example 14301: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] These 13 books “have been banned from schools in Florida.”
Error processing example 14302: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Half of American high school students are not familiar with the Holocaust."
Error processing example 14303: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “With school-based professional social workers, the recommendation is 400-to-1, and in Wisconsin, we’re almost triple that.”
Error processing example 14304: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Last year alone, natural disasters in America caused $178 billion in damages.”
Error processing example 14305: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I have been a state legislator for 11 years, passing over 170 bills into law and delivering for Wisconsin families.”
Error processing example 14306: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Abraham Lincoln challenged a guy to a sword fight.”
Error processing example 14307: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “New police bodycam footage” of George Floyd’s arrest “changes the narrative”
Error processing example 14308: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A "ghost bus … filled with FBI informants dressed as Trump supporters deployed onto our Capitol on January 6th.”
Error processing example 14309: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under New York’s Clean Slate Act “violent crimes … will now be automatically sealed after a set time - like they never happened.”
Error processing example 14310: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 343.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1318/2000 [25:00<09:33,  1.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1319/2000 [25:05<15:59,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1321/2000 [25:05<11:54,  1.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1323/2000 [25:10<16:44,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▌   | 1324/2000 [25:13<19:50,  1.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▋   | 1325/2000 [25:15<19:29,  1.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▋   | 1326/2000 [25:15<16:21,  1.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▋   | 1327/2000 [25:17<16:44,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] The federal government and the Federal Reserve were “raising inflation,” something that “had nothing to do with … getting our economy back in line” but rather to fund a “war against a bunch of civilians.”
Error processing example 14311: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Florida, “any woman who has an abortion after six weeks — and any doctor who gives her care — will be guilty of a felony.”
Error processing example 14312: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Because wages are rising, this Thanksgiving dinner is the fourth-cheapest ever as a percentage of average earnings.”
Error processing example 14313: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Clips of Nikki Haley speaking about Hillary Clinton show Haley supports Clinton and is “not who she says she is.”
Error processing example 14314: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In every single war that America has fought, we have never asked for land afterwards, except for enough to bury the Americans who gave the ultimate sacrifice for that freedom we went in for.”
Error processing example 14315: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republicans’ “plan would cut Social Security benefits. … Average benefit cut would be 13%.”
Error processing example 14316: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 345.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] China “had the largest naval fleet in the world. They had 370 ships. They'll have 400 ships in two years. We won't even have 350 ships in two decades."
Error processing example 14317: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Your minor child can go to California without your knowledge or without your consent, and get hormone therapy, puberty blockers and a sex change operation.”
Error processing example 14318: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "When (Ron DeSantis) was in Congress, he supported amnesty."
Error processing example 14319: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Mysterious pneumonia” from China is now in Ohio.
Error processing example 14320: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] With Apple’s NameDrop, anyone placing an iPhone near your child’s “will automatically receive their contact information.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▋   | 1329/2000 [25:17<10:19,  1.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  66%|██████▋   | 1330/2000 [25:17<08:13,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1332/2000 [25:18<07:31,  1.48it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1333/2000 [25:18<06:09,  1.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1336/2000 [25:22<10:11,  1.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1338/2000 [25:22<07:11,  1.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14321: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Our government, wrongly, is saying if you’re born in this country you’re automatically an American citizen.”
Error processing example 14322: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “With the historic Abraham Accords, I even made peace in the Middle East.”
Error processing example 14323: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Wisconsin utilities have charged ratepayers more than $1.9 billion of increases since 2019.”
Error processing example 14324: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’ve created close to 800,000 manufacturing jobs since I’ve taken office.”
Error processing example 14325: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We have our government giving people who came into this country illegally $5,000 gift cards.”
Error processing example 14326: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Discussing gender-affirming surgical care for minors, said that Sweden “shut it down.”
Error processing example 14327: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nikki Haley “raised taxes.”
Error processing example 14328: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The great replacement theory is not some grand right-wing conspiracy theory. But a basic statement of the Democratic Party's platform.”
Error processing example 14329: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Jan. 6 now does look like it was an inside job.”
Error processing example 14330: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Wisconsin Republicans have passed multiple billion dollar middle-class tax cuts this year, but each time we have, Governor Tony Evers has vetoed the relief."
Error processing example 14331: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] China is allowed to do crypto mining in the U.S.
Processing examples:  67%|██████▋   | 1339/2000 [25:23<07:52,  1.40it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1340/2000 [25:24<06:27,  1.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1342/2000 [25:24<04:24,  2.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1343/2000 [25:28<12:36,  1.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1345/2000 [25:28<08:58,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1346/2000 [25:29<08:04,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  67%|██████▋   | 1348/2000 [25:29<06:07,  1.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14332: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats have also used alternate slates of electors "repeatedly in all kinds of different states."
Error processing example 14333: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida “had a year-over-year reduction” in overdose deaths.
Error processing example 14334: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In the past year, Michigan, Minnesota and Illinois passed "highly popular” policies such as paid family leave, one of several issues that have been stymied in Wisconsin by Republicans.
Error processing example 14335: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under the Trump administration's trade deal, “China's only fulfilled one-third of the promises of agricultural products that they were supposed to buy from our farmers.”
Error processing example 14336: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Gasoline prices are now $5, $6, $7 and even $8 a gallon.”
Error processing example 14337: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’ve had more Americans die of fentanyl than the Iraq, Afghanistan, and Vietnam wars, combined.”
Error processing example 14338: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. tax dollars “have gone to promote transgenderism in Bangladesh.”
Error processing example 14339: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Almost a million people, mostly Democrats, have been kicked off the voter roll here in Florida” because of efforts to place a proposed amendment on abortion access on the state’s 2024 ballot.
Error processing example 14340: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “They want to make our Army tanks all electric.”
Error processing example 14341: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Illegal immigrants now have the right to vote in New York."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1351/2000 [25:30<05:07,  2.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1354/2000 [25:30<03:15,  3.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1356/2000 [25:31<02:35,  4.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1358/2000 [25:33<05:49,  1.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1359/2000 [25:35<08:50,  1.21it/s]Error processing example 14342: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Migrants who enter the U.S. illegally get U.S. passports immediately after leaving a border processing center.
Error processing example 14343: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Ron DeSantis “called China ‘Florida’s most important trading partner.’”
Error processing example 14344: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump “deported less, believe it or not, than Barack Obama even did."
Error processing example 14345: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Palm Springs, California, “transgender residents to receive a monthly payment of $900.”
Error processing example 14346: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Right now, over 70% of federal employees are still working from home three years after COVID."
Error processing example 14347: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In actuality, there is no evidence Joe Biden won” the 2020 election, citing a report’s allegations from five battleground states.
Error processing example 14348: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says FBI Director Christopher Wray told Congress “that with the border wide open and a war in Israel, Hamas can just walk right in.”
Error processing example 14349: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nikki Haley “opposed Trump’s border wall” and “Trump’s travel ban.”
Error processing example 14350: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Florida, “income growth is top of the chart.”
Error processing example 14351: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Epstein List” shows 166 people connected to convicted sex offender Jeffrey Epstein.
Error processing example 14352: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1361/2000 [25:36<06:11,  1.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1363/2000 [25:36<04:26,  2.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1365/2000 [25:37<04:52,  2.17it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1366/2000 [25:38<05:45,  1.83it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1367/2000 [25:40<09:24,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1368/2000 [25:40<08:16,  1.27it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1369/2000 [25:41<06:30,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  68%|██████▊   | 1370/2000 [25:42<07:37,  1.38it/s]
[CLAIM] “We’ve had 8 million” immigrants come to the U.S. illegally under Biden “and they only sent back 142,000.”
Error processing example 14353: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “That’s why they are allowing these people to come in — people that don’t speak our language — they are signing them up to vote.”
Error processing example 14354: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nikki Haley is ineligible to be president or vice president.
Error processing example 14355: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nikki Haley “says the retirement age at 65 is way too low, it must be much higher.”
Error processing example 14356: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The law in Wisconsin (on abortion time limit) is 20 weeks. That is way outside the international bounds.”
Error processing example 14357: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The racial wealth gap is the smallest it’s been in 20 years.”
Error processing example 14358: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Nikki Haley refused to call illegals criminals" and "repeatedly pushed amnesty for illegals."
Error processing example 14359: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Police made ‘no arrests’ of protesters who blocked Durham Freeway
Error processing example 14360: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Bryan Steil is part of a group that wants to cut Social Security and Medicare, that would be devastating.”
Error processing example 14361: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Biden’s let in 8 million people just in four years.”
Error processing example 14362: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Haley’s plan cuts Social Security benefits for 82% of Americans.”
Error processing example 14363: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▊   | 1371/2000 [25:42<05:57,  1.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▊   | 1372/2000 [25:45<14:39,  1.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▊   | 1374/2000 [25:45<08:23,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1377/2000 [25:46<04:33,  2.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1379/2000 [25:47<04:53,  2.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1381/2000 [25:47<03:40,  2.81it/s]
[CLAIM] Women "store DNA" from all their male sexual partners.
Error processing example 14364: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Abraham Lincoln’s coin is copper because he ended slavery. Figures on other U.S. coins “face left because they turned their backs to him in protest.”
Error processing example 14365: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “This is the third time we’ve won but this is the biggest win” in Iowa caucus.
Error processing example 14366: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “New proposed bill in Maine says the state can take custody of a kid if the parents oppose” gender-affirming care
Error processing example 14367: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump “proposed when he was president” that “he wanted to raise the gas tax up to 25 cents."
Error processing example 14368: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Every booster you take, you’re more likely to get COVID as a result of it.”
Error processing example 14369: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Florida state government (has the) lowest number of state employees per capita in the country.”
Error processing example 14370: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Department of Homeland Security Secretary Alejandro Mayorkas' border policy is "personally responsible" for fentanyl crossing the border
Error processing example 14371: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden changed the State of the Union Address from summer to March.
Error processing example 14372: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Wisconsin had over 1,400 opioid overdose deaths in 2022.”
Error processing example 14373: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "(The) top issue for college students is the economy."
Error processing example 14374: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1384/2000 [25:48<03:34,  2.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1385/2000 [25:48<03:10,  3.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1386/2000 [25:48<02:46,  3.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1387/2000 [25:51<07:36,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  69%|██████▉   | 1388/2000 [25:54<14:21,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|██████▉   | 1390/2000 [25:55<10:03,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Chinese scientists “created (a) new COVID-19 strain (with) 100% fatality.”
Error processing example 14375: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Winston Churchill said, “Success is not final, failure is not fatal: it is the courage to continue that counts.”
Error processing example 14376: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says President Joe Biden said that Democrats should not vote in the New Hampshire primary.
Error processing example 14377: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Today in America, 1 in 3 women of reproductive age live in a state with an abortion ban.”
Error processing example 14378: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We have technology and medical advancements today that can tell you if you are pregnant the day after conception."
Error processing example 14379: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Social media post claims judge in Trump’s defamation trial engaged in “election interference” by delaying the trial to New Hampshire’s primary day.
Error processing example 14380: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “US hospitals caught injecting experimental Ebola vaccine that sheds!”
Error processing example 14381: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "After Roe was dismantled, extremists evoked a law from 1849 to stop abortion”  in Wisconsin.
Error processing example 14382: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In April last year, our state unemployment rate hit a record low of 2.4%. Last year, Wisconsin had an all-time lowest number of unemployed workers ever in modern history.”
Error processing example 14383: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats “used COVID to cheat” in the 2020 election.
Error processing example 14384: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Tens of thousands of auto jobs were lost nationwide during Trump's presidency,” but during Biden’s presidency, “we've created more than 250,000 auto jobs.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|██████▉   | 1393/2000 [25:55<05:36,  1.80it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|██████▉   | 1395/2000 [25:55<04:05,  2.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|██████▉   | 1397/2000 [25:59<08:30,  1.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|██████▉   | 1399/2000 [26:02<10:43,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|███████   | 1401/2000 [26:03<09:04,  1.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14385: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Rep. Dean Phillips received almost twice as many votes as Biden did in the New Hampshire primary.”
Error processing example 14386: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “North Carolina has the longest voting period in the country … and we have the most ways of voting.”
Error processing example 14387: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "There are a thousand billionaires now and you know what their average tax rate is? 8%."
Error processing example 14388: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Last time Democrats removed a Republican from the ballot” was Abraham Lincoln in 1860.
Error processing example 14389: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’ve seen an 800% increase in the Swanton sector, which is the part of the northern border that I represent, in illegal crossings.”
Error processing example 14390: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Ilhan Omar said her "top priority is to put Somalia first and expand its territory.”
Error processing example 14391: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Postpartum Medicaid coverage expanded from three states to 43 states because of the Biden administration.
Error processing example 14392: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “TSA found with metal detectors nearly 7,000 firearms at airports last year — an all-time high.”
Error processing example 14393: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Stanley cups tested positive for lead “inside of the cup where the drink is.”
Error processing example 14394: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “George Soros worked for Hitler. Klaus Schwab’s father also worked for Hitler.”
Error processing example 14395: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|███████   | 1404/2000 [26:03<05:38,  1.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|███████   | 1406/2000 [26:04<05:19,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|███████   | 1408/2000 [26:05<05:05,  1.94it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|███████   | 1409/2000 [26:06<05:15,  1.88it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  70%|███████   | 1410/2000 [26:11<14:39,  1.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████   | 1411/2000 [26:13<14:30,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “We think the Earth’s warming” because “we’re literally cooking the books” with flawed temperature data.
Error processing example 14396: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If Texas says, “‘We don’t want to be part of America anymore’ … that’s their decision to make.”
Error processing example 14397: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Minnesota incarcerates “about a third of what Wisconsin does.”
Error processing example 14398: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US.”
Error processing example 14399: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Bidenomics is costing the typical American household over $11,400 more a year to buy the basics.”
Error processing example 14400: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Removing gun-related homicides in Chicago, Detroit, Los Angeles, Philadelphia and St. Louis would drop the U.S. to 189th out of 193 countries in gun violence.
Error processing example 14401: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "NYC to give ILLEGALS $53 Million in Prepaid Credit Cards."
Error processing example 14402: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Biden’s strategy is to “get as many illegals in the country as possible” and “legalize them to create a permanent majority.”
Error processing example 14403: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 855.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.74 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 59.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Marijuana is currently classified in the same category of drugs as heroin and a more dangerous category than fentanyl or cocaine.”
Error processing example 14404: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under President Joe Biden, “Black unemployment is the lowest in American history.”
Error processing example 14405: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Inflation has been at the pre-pandemic level of 2% over the last half year.”
Processing examples:  71%|███████   | 1413/2000 [26:13<09:10,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████   | 1416/2000 [26:13<05:12,  1.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████   | 1419/2000 [26:14<05:00,  1.94it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████   | 1421/2000 [26:16<05:13,  1.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████   | 1422/2000 [26:16<05:11,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14406: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Senate’s border bill “accepts 5,000 illegal immigrants a day.”
Error processing example 14407: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Paper shows “mRNA COVID-19 vaccines caused more deaths” than lives saved.
Error processing example 14408: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “NYC will give $1,000 taxpayer-funded credit cards to migrants.”
Error processing example 14409: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Oklahoma, "I won 77 out of 77 counties. Ronald Reagan is second with 56."
Error processing example 14410: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Each year, more than $1 billion is stolen from” workers in New York state through wage theft.
Error processing example 14411: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Half of the cities with the highest rates of child poverty are in” New York state.
Error processing example 14412: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Arson, not climate change, caused Canada’s record 2023 wildfire season.
Error processing example 14413: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Since Russia invaded Ukraine, “not a single Western journalist has bothered to interview Putin.”
Error processing example 14414: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ukraine puts Tucker Carlson on a ‘hit list’ after an interview with Vladimir Putin.”
Error processing example 14415: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] On classified documents in his possession, “none of it was high classified.”
Error processing example 14416: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I cooperated far more” than Joe Biden in the classified documents investigations.
Processing examples:  71%|███████   | 1424/2000 [26:16<03:46,  2.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████▏  | 1425/2000 [26:18<05:52,  1.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  71%|███████▏  | 1429/2000 [26:19<04:24,  2.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1431/2000 [26:25<11:21,  1.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1432/2000 [26:26<10:49,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14417: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 357.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Food “costs 40%, 50%, 60% more than it did just a few years ago.”
Error processing example 14418: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We're having the largest classes of correctional officers we've ever had before.”
Error processing example 14419: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Bipartisan Safer Communities Act led to “a 12% reduction in urban homicides in this country.”
Error processing example 14420: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If a woman in Texas takes “abortion pills voluntarily,” she would be “charged with murder.”
Error processing example 14421: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 2022 CHIPS and Science Act “attracted $640 billion in private companies’ investments.”
Error processing example 14422: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The Ukraine supplemental includes a hidden impeachment clause against President Trump.”
Error processing example 14423: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under Joe Biden, there are “record numbers of new Black entrepreneurs.”
Error processing example 14424: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 345.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The United States is an outlier, one of only about half a dozen countries, without any guarantee of paid leave for new parents and/or other health care needs.”
Error processing example 14425: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] After a 2022 law, “the vast majority of colleges in New York State do not have on-campus poll sites.”
Error processing example 14426: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Kids Online Safety Act “would require everyone to upload your government ID in order to use most sites on the internet.”
Error processing example 14427: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1435/2000 [26:27<07:42,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1438/2000 [26:28<06:05,  1.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1439/2000 [26:30<07:26,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1440/2000 [26:30<06:43,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1441/2000 [26:31<05:36,  1.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1442/2000 [26:33<08:28,  1.10it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] There is “clear scientific consensus” that “hormonal birth control makes you fat, doubles risk of depression and triples risk of suicide.”
Error processing example 14428: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden “directed New York AG Witch Hunt” into Donald Trump’s real estate.
Error processing example 14429: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The biggest threat to your unions is millions of people coming across the border, because you're not gonna have your jobs anymore.”
Error processing example 14430: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nevada database glitch showing voters cast ballots when they didn’t is evidence that voting by mail is “the largest source of potential voter fraud.”
Error processing example 14431: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Cloud seeding program connected to recent California storms.
Error processing example 14432: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo shows Minnesota Sen. Amy Klobuchar and Minnesota Attorney General Keith Ellison posing at “Defund the Police” event.
Error processing example 14433: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Republican U.S. Senate candidate Eric Hovde claims he "understands the tragedy of children being trafficked through Central America because he owns three homes there."
Error processing example 14434: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 355.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump’s fine in the New York fraud case “is a form of Navalny, it is a form of communism or fascism.”
Error processing example 14435: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “One of my responsibilities as Secretary of State is to countersign acts passed by the legislature & signed into law by Gov. Evers.”
Error processing example 14436: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Chicken farm fire that killed 12 million birds intentionally set.
Error processing example 14437: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If you went “anywhere in the world,” you could get a prescription filled for 40% to 60% less than it costs in the U.S.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1446/2000 [26:33<04:37,  2.00it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1447/2000 [26:33<04:01,  2.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1448/2000 [26:34<05:04,  1.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  72%|███████▏  | 1449/2000 [26:35<04:15,  2.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1451/2000 [26:39<10:51,  1.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1452/2000 [26:40<10:43,  1.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1455/2000 [26:43<08:55,  1.02it/s]Error processing example 14438: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In New York, there are no barriers to law enforcement to work with the federal government on immigration laws, and there are 100 crimes where migrants can be handed over.
Error processing example 14439: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Olympic boxing committee “decided this year that men can box against women in the Olympics.”
Error processing example 14440: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 103.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Study reveals Bill Gates’ Fake Meat causes ‘turbo cancers’ in humans.”
Error processing example 14441: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Milwaukee, we “never defunded the police.”
Error processing example 14442: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Nearly a quarter million birds in New York City, and more than one billion across the country, die each year from collisions with buildings.”
Error processing example 14443: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats don’t deport undocumented migrants “because every illegal is a highly likely vote at some point.”
Error processing example 14444: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The government did more to stop the distribution of ivermectin and hydroxychloroquine than to stop the distribution of fentanyl.
Error processing example 14445: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump called his wife "Mercedes" instead of Melania.
Error processing example 14446: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Citizens Property Insurance “is not solvent.”
Error processing example 14447: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Your legal name is in fact a corporation. This is why you always see your name written in ALL CAPS.”
Error processing example 14448: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1456/2000 [26:44<08:31,  1.06it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1457/2000 [26:49<17:57,  1.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1458/2000 [26:50<15:13,  1.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1461/2000 [26:50<07:53,  1.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1463/2000 [26:50<05:31,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1464/2000 [26:52<07:05,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1465/2000 [26:52<06:23,  1.40it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] "There are less than two weeks until the deadline to register to vote in Wisconsin."
Error processing example 14449: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In New York City “local elections, illegal immigrants can vote.”
Error processing example 14450: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 345.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Great Lakes account for “over 20% of the world’s freshwater and over 80% of North America’s freshwater.”
Error processing example 14451: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The CDC officially agrees with ‘conspiracy theorists’ from 2020” who wanted to treat COVID-19 like the flu.
Error processing example 14452: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Eighty-two percent of the country understands that (the 2020 election) was a rigged election.”
Error processing example 14453: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden was “rushed to hospital unexpectedly.”
Error processing example 14454: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “When it comes to how many votes (President) Joe Biden won, it was literally less than a few votes per ward.”
Error processing example 14455: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We just had 100% increase in layoffs.”
Error processing example 14456: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The average commute time for people who drive to work in (New York City) is 43 minutes, the longest in the nation.”
Error processing example 14457: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A new Missouri bill “would put teachers on the sex offense registry if they ‘contribute to social transition’” of transgender students, including by using gender-affirming pronouns.
Error processing example 14458: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Since May of last year, we have removed or returned more individuals than in any year since 2015.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1467/2000 [26:58<13:06,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  73%|███████▎  | 1468/2000 [27:03<19:56,  2.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▎  | 1470/2000 [27:03<12:33,  1.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▎  | 1474/2000 [27:03<06:12,  1.41it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14459: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden's executive orders meant "for the first time in American history we have a president who will not detain the people who enter this country illegally."
Error processing example 14460: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 345.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration seeks to end voter ID requirements to allow immigrants in the country illegally to vote.
Error processing example 14461: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Since the end of the 2020 fiscal year, President Biden and Senator Baldwin have added over $7.3 trillion of debt, more than the first 228 years of our nation’s history combined.”
Error processing example 14462: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “This is the era where people are given smartphones when they cross the border and enter our country.”
Error processing example 14463: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says U.S. Sen. Tammy Baldwin “was one of the staunchest supporters of the Iran deal which sent billions of dollars and plane loads of cash to Iran.”
Error processing example 14464: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Texas wildfires are a “deliberate” attack on U.S. food supply.
Error processing example 14465: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Only immigrants in the country illegally are eligible for an interest-free California housing loan program.
Error processing example 14466: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Canada is passing a bill to essentially ban the idea of Christianity.”
Error processing example 14467: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’ve had 12 elections in 24 years in Wisconsin that have been decided by less than 30,000 votes.”
Error processing example 14468: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’re facing situations these days where you have to have two officers in the evidence room in case there’s an accidental (fentanyl) exposure.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▍  | 1477/2000 [27:05<05:31,  1.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▍  | 1480/2000 [27:06<05:06,  1.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▍  | 1482/2000 [27:07<04:23,  1.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▍  | 1485/2000 [27:07<03:01,  2.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14469: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Trump administration "added more to the national debt than any presidential term in American history."
Error processing example 14470: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We cut the federal deficit by $160 billion because Medicare will no longer have to pay those exorbitant prices to Big Pharma.”
Error processing example 14471: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We’ve already cut the federal deficit by over $1 trillion."
Error processing example 14472: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden said “do whatever the hell they want,” paused, and then said, “I guess I should clear my mind here a little bit.”
Error processing example 14473: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “300,000 illegal immigrants were able to use a simple app to get a free flight to our country.”
Error processing example 14474: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A woman was “sex trafficked by the cartels starting at the age of 12 … We wouldn't be OK with this happening in a Third World country. This is the United States of America.”
Error processing example 14475: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Billionaires “rigged” the California Senate primary election through “dishonest means.”
Error processing example 14476: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Since 1989 and a new age of globalization began, 51 million jobs have been created in America. 49 million, 96%, have been created under Democratic presidents.”
Error processing example 14477: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Valid IDs are not required for voting.
Error processing example 14478: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Jan. 6 committee “suppressed testimony” from former Deputy Chief of Staff Anthony Ornato that proves former President Donald Trump pushed for 10,000 National Guard troops at the Capitol.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▍  | 1487/2000 [27:11<06:47,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  74%|███████▍  | 1490/2000 [27:11<04:33,  1.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▍  | 1492/2000 [27:17<09:27,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▍  | 1493/2000 [27:18<08:35,  1.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▍  | 1495/2000 [27:18<06:35,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14479: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Biden has implemented a formal policy that illegal aliens who intrude into the United States are granted immunity from deportation.”
Error processing example 14480: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In February alone, nearly 1 million jobs held by native-born Americans disappeared."
Error processing example 14481: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The stock market “hit an all-time high under President Biden and not under President Trump."
Error processing example 14482: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Russian company secured Trump’s bond in E. Jean Carroll case.
Error processing example 14483: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “On Joe Biden’s watch, Wisconsin has lost 6,000 manufacturing jobs and 455 dairy farms in the last year.”
Error processing example 14484: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Emergency officials warn to “stock up on food, water, and fuel ahead of the eclipse.”
Error processing example 14485: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says “100% of net job creation under the Biden administration has gone to the foreign-born.”
Error processing example 14486: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Chinese government “owns” TikTok’s parent company, ByteDance.
Error processing example 14487: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Miami’s immigration courts have the “largest backlog of any court in this country with 10% of the overall immigration cases,” and “more than 90%” of people lose their asylum claims.
Error processing example 14488: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A U.S. law says that “if a house is not inhabited, we can expropriate it.”
Error processing example 14489: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▍  | 1498/2000 [27:20<05:44,  1.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▍  | 1499/2000 [27:21<06:13,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▌  | 1500/2000 [27:21<06:01,  1.38it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▌  | 1503/2000 [27:22<03:29,  2.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▌  | 1505/2000 [27:22<02:37,  3.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] The Oklahoma National Guard’s assistance for the April 8 eclipse signals something “bigger.”
Error processing example 14490: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Your Social Security will be destroyed by the people coming in. There's too many of them. It's not sustainable."
Error processing example 14491: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gaza has “the highest number of people facing catastrophic hunger ever recorded by the Integrated Food Security Classification system — anywhere, anytime.”
Error processing example 14492: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The $454 million bond is “unprecedented, and practically impossible for ANY Company.”
Error processing example 14493: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Insulin for Medicare beneficiaries "was costing 400 bucks a month on average. It now costs $35 a month."
Error processing example 14494: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Tennessee bill that would ban the release of chemicals into the atmosphere is evidence chemtrails are real.
Error processing example 14495: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The New York civil fraud judgment against Trump and a plan to seize Trump-owned property “is simply a 'taking.' Much like what is done in communist countries."
Error processing example 14496: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A March 7 U.S. security alert … means the U.S. was “behind this gruesome terror attack on Russian civilians.”
Error processing example 14497: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “It is a fact that Obama created ISIS.”
Error processing example 14498: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The current Congress is “the least productive in our lifetime.”
Error processing example 14499: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Montana Democrats are "suing to say, ‘You should be allowed to be registered to vote in two states.'”
Processing examples:  75%|███████▌  | 1507/2000 [27:24<04:52,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▌  | 1508/2000 [27:24<04:09,  1.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  75%|███████▌  | 1509/2000 [27:30<12:30,  1.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1510/2000 [27:30<10:18,  1.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1511/2000 [27:31<09:19,  1.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1512/2000 [27:32<09:51,  1.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1513/2000 [27:33<08:07,  1.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1514/2000 [27:33<07:14,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14500: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Donald Trump “told Americans all they had to do was inject bleach in themselves. Just take a shot of UV light.”
Error processing example 14501: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video of the ship colliding with the Baltimore bridge proves the bridge collapse wasn’t “an accident.”
Error processing example 14502: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 341.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The captain of the container ship Dali…is a Ukrainian.”
Error processing example 14503: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We passed 27 bills last year, which is the fewest since the Depression.”
Error processing example 14504: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A March 26 bridge fire in Ohio is related to the collapse of Baltimore's Francis Scott Key Bridge.
Error processing example 14505: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Three busses just loaded up with illegal invaders at Detroit Metro" airport.
Error processing example 14506: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Nothing to see here… just the Mayor of Baltimore threatening White people.”
Error processing example 14507: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Since Joe Biden took office, crime has skyrocketed across our country.”
Error processing example 14508: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says President Joe Biden purposely chose Easter Sunday to mark Transgender Day of Visibility.
Error processing example 14509: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Joe Biden banned ‘religious themed’ eggs at the White House’s Easter Egg design contest for kids.”
Error processing example 14510: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The much-trumpeted job growth in the last year was ENTIRELY part-time jobs."
Processing examples:  76%|███████▌  | 1518/2000 [27:34<02:56,  2.73it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1520/2000 [27:36<04:55,  1.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1522/2000 [27:36<03:32,  2.25it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▌  | 1524/2000 [27:36<02:36,  3.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▋  | 1527/2000 [27:37<02:38,  2.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14511: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] ProduceMaxx sprayed on organic produce at grocery stores contains pesticides, herbicides, fungicides, insecticides and antibiotics.
Error processing example 14512: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Brad Knott was a lawyer for Joe Biden.”
Error processing example 14513: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “President Biden is the first candidate in history, the first president in history, that has used the federal agencies to censor political speech ... to censor his opponent."
Error processing example 14514: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] You can vote in-person absentee at your clerk’s office on Monday before Election Day.
Error processing example 14515: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 2020, Trump “did much better” in the Wisconsin election than 2016, and “after the wrongdoing was found, people said, ‘Well, he actually did win.’”
Error processing example 14516: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "EPA regulators just banned most new gas-powered cars."
Error processing example 14517: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Social Security Administration database shows “how many people each of the 43 states are registering to vote who DO NOT HAVE IDs.”
Error processing example 14518: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Crime is down in Venezuela by 67% because they're taking their gangs and their criminals and depositing them very nicely into the United States.”
Error processing example 14519: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] More than 2 million people registered to vote so far this year without photo ID in Arizona, Pennsylvania and Texas.
Error processing example 14520: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 95.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump’s plan is “cutting Social Security for you.”
Processing examples:  76%|███████▋  | 1528/2000 [27:37<02:23,  3.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  76%|███████▋  | 1530/2000 [27:39<04:12,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1531/2000 [27:40<03:35,  2.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1534/2000 [27:40<02:07,  3.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1536/2000 [27:41<02:36,  2.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14521: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] For health risk exceptions in Florida’s abortion amendment, “You're not even talking to a doctor to make that determination.”
Error processing example 14522: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Kohl’s has contributed $1,000,000 to the (Black Lives Matter) movement and related causes since 2020."
Error processing example 14523: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] On Jan. 6, 2021, U.S. Capitol “protestors carried no weapons.”
Error processing example 14524: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Support for Roe is higher today in America than it has ever been."
Error processing example 14525: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Speaking of semiconductor industry jobs, "Know what the average salary is? $110,000. You don't need a college degree."
Error processing example 14526: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "All legal scholars, both sides, wanted, and in fact demanded” that Roe v. Wade be overturned.
Error processing example 14527: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Democrats support abortion measures that result in the “execution” of babies “after birth.”
Error processing example 14528: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Starting in 2025 "no matter what your total bills are for prescription drugs,” Medicare Part D users will never pay “more than $2,000 a year, because some of these cancer drugs are 10(,000 to) 15,000 bucks a year.”
Error processing example 14529: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The GOP has failed to invest in our campuses, making WI fall to 42nd in the nation in state support.”
Error processing example 14530: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “It's very misleading when (President Biden) says (inflation) used to be at 9%. This is compounding. It’s not like it went down from 9% to 3%. This is building month after month. The better way to think about it is that it’s 18%, 19% over the last three years."
Processing examples:  77%|███████▋  | 1538/2000 [27:42<03:17,  2.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1539/2000 [27:42<03:21,  2.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1541/2000 [27:49<10:15,  1.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1542/2000 [27:49<08:38,  1.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1543/2000 [27:51<10:34,  1.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1546/2000 [27:51<05:36,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14531: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 45.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Taylor Swift is doing satanic rituals during her shows.”
Error processing example 14532: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Immigrants in the country illegally cost “Florida taxpayers $566 million for 54,000 hospital visits.”
Error processing example 14533: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Gov. Tony Evers is asking the GOP to release money for PFAS, but "vetoed (a bill outlining) how to spend the money."
Error processing example 14534: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “As many as 75% of school shootings resulted from a gun that was not secured.”
Error processing example 14535: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The 1864 Arizona law forbidding abortions, upheld by the State Supreme Court, also sets the age of consent for females at 10 years.”
Error processing example 14536: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Every Senate Democrat has voted to support unlimited abortions up to the moment of birth.”
Error processing example 14537: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “This trial that I have now, that’s a Biden trial.”
Error processing example 14538: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “School allows students to be terrorized by colleagues who identify as ‘furries.’”
Error processing example 14539: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Dollar Tree, Walgreens, Macy’s, Foot Locker, Gap, Burger King, Regal Cinemas and Kroger have had mass store closures during Joe Biden’s presidency.
Error processing example 14540: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Congress prohibits the NIH from researching the cause of mass shootings.”
Error processing example 14541: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  77%|███████▋  | 1549/2000 [27:52<03:26,  2.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1551/2000 [27:54<05:31,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1553/2000 [27:55<04:02,  1.84it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1555/2000 [27:58<06:16,  1.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1557/2000 [27:58<04:55,  1.50it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1558/2000 [27:59<04:40,  1.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Florida’s Amendment 4 "will mandate abortion until the moment of birth.”
Error processing example 14542: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Said Columbia University has told students, “Well, if you’re Jewish, maybe you do want to stay home.”
Error processing example 14543: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden is paying rent for immigrants illegally in the country.
Error processing example 14544: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says he “got arrested” while protesting in favor of desegregation.
Error processing example 14545: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says U.S. Sen. Tammy Baldwin "believes taxpayer dollars should be used for abortion."
Error processing example 14546: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump “has the worst record of job loss of any president.”
Error processing example 14547: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo shows “clearest image ever taken of Venus.”
Error processing example 14548: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The science is clear — the gray wolf has met and exceeded recovery goals.”
Error processing example 14549: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In his Time magazine interview, Donald Trump “said states have the right to monitor pregnant women to enforce these bans, and states have the right to punish pregnant women for seeking out abortion care.”
Error processing example 14550: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump told Time magazine that “states should monitor women’s pregnancies and prosecute those who violate abortion bans.”
Error processing example 14551: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Bombshell: Biden had pallets of classified docs sent to Mar-a-Lago before FBI raid! Trump was setup.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1561/2000 [27:59<02:46,  2.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1563/2000 [28:00<02:40,  2.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1566/2000 [28:00<01:49,  3.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1568/2000 [28:03<04:17,  1.68it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14552: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Robert F. Kennedy Jr. called voter ID “racist,” calls NRA a “terrorist organization” and supports affirmative action.
Error processing example 14553: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I’m not allowed to testify” because of a gag order.
Error processing example 14554: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Nearly 90% of all UW graduates stay in Wisconsin 5 years after they graduate.”
Error processing example 14555: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Voter fraud investigations are being banned by Michigan lawmakers!”
Error processing example 14556: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Joe Biden flew hundreds of thousands of illegals into Florida last year.”
Error processing example 14557: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Second Boeing Whistleblower Killed In 2 Months!
Error processing example 14558: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “When I worked on the Affordable Care Act, I wrote the amendment that allows all young people to stay on their parents’ health insurance until they turn 26. Overnight, millions of young Americans got health care.”
Error processing example 14559: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Fired Milwaukee election leader “printed 64,000 ballots in a back room at City Hall in Milwaukee and had random employees fill them out,” giving Joe Biden the lead.
Error processing example 14560: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If the Affordable Care Act were terminated, “that would mean over a hundred million Americans will lose protections for preexisting conditions.”
Error processing example 14561: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The millions (of immigrants) that have been paroled can simply go to their local welfare office or the DMV and register to vote.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  78%|███████▊  | 1570/2000 [28:03<03:11,  2.25it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▊  | 1572/2000 [28:04<03:21,  2.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▊  | 1574/2000 [28:04<02:31,  2.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1577/2000 [28:06<03:07,  2.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14562: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The House of Representatives in the U.S. just passed … the Antisemitism Act that proposes to eliminate part of the Bible.”
Error processing example 14563: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Lyme disease became endemic because of U.S. government bioweapons labs.
Error processing example 14564: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Justice Paul Newby is out here removing judges who were elected to their positions by the voters of North Carolina!!!”
Error processing example 14565: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Katie Britt introduced a bill “to create a national registry of pregnant women.”
Error processing example 14566: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Biden wants to immediately stop all aid to Israel.”
Error processing example 14567: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Aurora borealis sightings a “fabricated light show” created by a HAARP experiment.
Error processing example 14568: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "China's in a situation where they have more retired than working."
Error processing example 14569: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Wisconsin, “a legislative committee cannot release funding.”
Error processing example 14570: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Alvin Bragg’s office deleted 3 pages of phone call records of Michael Cohen with Stormy Daniels’ lawyer,” which is evidence tampering.
Error processing example 14571: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] North Carolina mask bill includes “specific carve outs for entities like the KKK to continue wearing hoods in public.”
Error processing example 14572: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Inflation “was 9% when I came to office.”
Processing examples:  79%|███████▉  | 1580/2000 [28:07<03:21,  2.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1581/2000 [28:08<03:14,  2.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1583/2000 [28:08<02:48,  2.48it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1584/2000 [28:09<02:50,  2.44it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1587/2000 [28:10<02:32,  2.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1588/2000 [28:10<02:45,  2.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  79%|███████▉  | 1589/2000 [28:11<03:11,  2.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14573: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Violent crime is near a record 50-year low.”
Error processing example 14574: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Tariffs are a “direct, regressive tax on Americans” and President Joe Biden’s new tariffs on Chinese goods will “hit every family.”
Error processing example 14575: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Under (the Biden) administration we have witnessed the fastest growth of Black-owned small businesses in more than 30 years.”
Error processing example 14576: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Trump administration handed out taxpayer dollars to a foreign company. Hundreds of homes and farms were bulldozed, and over $500 million in taxpayer dollars were wasted preparing for Foxconn."
Error processing example 14577: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says in his three election campaigns, the early polls were "wildly incorrect, all three times."
Error processing example 14578: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump wants to terminate the Affordable Care Act.
Error processing example 14579: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 101.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The number of people in the U.S. illegally is “upwards of 20, 25, maybe 30 million.”
Error processing example 14580: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Biden’s DOJ was authorized to use DEADLY FORCE” in Mar-a-Lago raid. Biden was “locked and loaded” and “ready to take me out.”
Error processing example 14581: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The loss of SALT deductibility cost our state $15 billion in revenue. So New Yorkers are paying more in taxes and receiving fewer services because of Donald Trump.”
Error processing example 14582: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The last administration oversaw the largest increase in murders ever recorded."
Processing examples:  80%|███████▉  | 1590/2000 [28:12<03:27,  1.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|███████▉  | 1593/2000 [28:12<01:55,  3.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|███████▉  | 1594/2000 [28:12<01:49,  3.71it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|███████▉  | 1595/2000 [28:15<06:14,  1.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|███████▉  | 1597/2000 [28:16<04:02,  1.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|███████▉  | 1598/2000 [28:16<04:28,  1.50it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|███████▉  | 1599/2000 [28:22<12:42,  1.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14583: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump “shut down” Nord Stream 2 pipeline, Joe Biden approved it, then Biden shut down U.S. liquified natural gas export capacity.
Error processing example 14584: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ronald Reagan’s Federal Communications Commission abolished the Fairness Doctrine and gave rise to Fox News.
Error processing example 14585: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Judge Juan Merchan “overrules every objection from the defense and sustains every objection from the prosecution” during former President Donald Trump’s New York trial.
Error processing example 14586: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Judge Juan Merchan told New York jurors the verdict in Donald Trump’s trial does not need to be unanimous.
Error processing example 14587: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.24 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden “is letting millions of people from jails, from prisons, from insane asylums, from mental institutions, drug dealers pour in.”
Error processing example 14588: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Trump can’t vote for himself in the November election.”
Error processing example 14589: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Congo, Africa, just released a lot of people, a lot of people from their prisons and jails and brought them into the United States of America.”
Error processing example 14590: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 97.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden wants “to quadruple your taxes.”
Error processing example 14591: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 99.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.49 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Speaking about Hillary Clinton: “I didn’t say, ‘Lock her up.’ ”
Error processing example 14592: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 851.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 28.74 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Due to the arrangements that I've reached with (Mexico) President Obrador, the number of migrants coming … to our shared border unlawfully in recent months has dropped dramatically.”
Processing examples:  80%|████████  | 1600/2000 [28:24<11:58,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|████████  | 1602/2000 [28:24<07:06,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|████████  | 1605/2000 [28:24<03:51,  1.71it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|████████  | 1607/2000 [28:25<03:46,  1.73it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  80%|████████  | 1609/2000 [28:25<02:46,  2.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14593: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Wage increases have exceeded” the cost of inflation.
Error processing example 14594: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republican legislatures “have targeted birth control” and in Wisconsin “refused to schedule a vote on a bill that would have codified a right to birth control.”
Error processing example 14595: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Audio of President Joe Biden’s interview with Special Counsel Robert Hur leaked.
Error processing example 14596: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Joe Biden’s immigration order limiting asylum is “pro-child trafficking.”
Error processing example 14597: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 93.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Lower insulin prices were the work of the Trump administration, not “Crooked Joe Biden. He had NOTHING to do with it.”
Error processing example 14598: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Elective abortions up to and including the moment of birth. Healthy, 9-month-year-old baby killed at the moment of birth. That’s what Jon Tester and the Democrats have voted for.”
Error processing example 14599: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Trump can’t be on the Texas ballot because of our state constitution.”
Error processing example 14600: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Sen. Lindsey Graham said a newly elected Donald Trump “will round up and deport LEGAL immigrants.”
Error processing example 14601: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows Rep. Nancy Pelosi “takes responsibility for not having the National Guard” at the U.S. Capitol on Jan. 6, 2021.
Error processing example 14602: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin had a “record-breaking year” for tourism in 2023.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████  | 1611/2000 [28:28<04:14,  1.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████  | 1612/2000 [28:28<03:43,  1.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████  | 1614/2000 [28:29<03:28,  1.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████  | 1617/2000 [28:29<02:08,  2.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████  | 1618/2000 [28:30<02:45,  2.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14603: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Hunter Biden’s trial and conviction were contrived to divert attention from Biden family crimes.
Error processing example 14604: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 9th Circuit Court of Appeals “just ruled Covid vax mandates unconstitutional.”
Error processing example 14605: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 91.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Saudi Arabia will stop using the US dollar for oil sales and will not renew the 50-year petro-dollar agreement with the U.S.”
Error processing example 14606: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows Biden wandered off from world leaders at G7 skydiving event.
Error processing example 14607: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Trump administration “achieved the lowest African American unemployment rate and the lowest African American poverty rate ever recorded.”
Error processing example 14608: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Crime statistics “no longer include data from 30% of the country including the biggest and most violent cities.”
Error processing example 14609: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The weapons that Ukraine used in the early days of this war to fend off the Russian invasion are the weapons that Donald Trump sent, that Barack Obama and Joe Biden had refused to send.”
Error processing example 14610: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California Assembly Bill 1955 “would allow schools to secretly, socially and perhaps medically transition your child without your knowledge and consent."
Error processing example 14611: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says he “saved Kenosha” and the “governor wouldn’t move” on sending the National Guard.
Error processing example 14612: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 89.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump is “against mail in voting.”
Processing examples:  81%|████████  | 1620/2000 [28:32<04:06,  1.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████  | 1623/2000 [28:34<04:12,  1.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████▏ | 1626/2000 [28:36<04:04,  1.53it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████▏ | 1627/2000 [28:36<03:34,  1.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  81%|████████▏ | 1628/2000 [28:37<03:49,  1.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14613: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Starting from June 2024, the United States will provide a six-month free CDL course.”
Error processing example 14614: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “On (Josh) Stein’s watch, rapes in North Carolina are up 53%.”
Error processing example 14615: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 87.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Planes full of unvetted ‘refugees’ are being accepted at the Milw. & Madison airports!”
Error processing example 14616: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Border Patrol agents take migrants "to the Yuma airport, put them on a plane to any destination they want. ... And they pay their ticket. And then they get reimbursement from FEMA."
Error processing example 14617: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “(Homeland Security Secretary Alejandro) Mayorkas said, ‘Hey you don’t have to detain aggravated felons,’ those are rapists, child predators, you know, murderers.”
Error processing example 14618: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "For 18 months under President Trump, not a single American was harmed in Afghanistan.”
Error processing example 14619: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Biden Executive Order 14019  “requires our taxpayer-funded federal agencies to violate the Hatch Act and engage in illegal vote harvesting.”
Error processing example 14620: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Black women in the U.S. are “three to four times more likely to die in connection with childbirth than other women."
Error processing example 14621: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris "co-sponsored, fully sponsored" the Green New Deal.
Error processing example 14622: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Project 2025 eliminates OSHA and overtime wages.”
Processing examples:  82%|████████▏ | 1630/2000 [28:37<02:39,  2.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1633/2000 [28:37<01:38,  3.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1635/2000 [28:38<02:04,  2.93it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1637/2000 [28:39<01:37,  3.71it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1638/2000 [28:40<02:27,  2.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1639/2000 [28:40<02:05,  2.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14623: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] LGBTQ+ leaders sign World Economic Forum treaty to “Accept Pedophiles as 'Legally Protected Minority’”
Error processing example 14624: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Deadlines have passed in … Wisconsin” to replace President Joe Biden on the ballot.
Error processing example 14625: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In 2020, “only nine NATO allies were spending 2%” of their GDP on defense. “This year, 23 will spend at least 2%.”
Error processing example 14626: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The most recent State Budget increased funding for the Department of Tourism by 98 percent.”
Error processing example 14627: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Project 2025 would end gay marriage.
Error processing example 14628: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The idea that there’s political violence or violence in America like this is just unheard of.”
Error processing example 14629: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump staged the shooting at his rally in Butler, Pennsylvania.
Error processing example 14630: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A Secret Service agent named Jonathan Willis said he “had the assassin in his sights for 3 minutes, but he was told not to engage by the head of the (Secret Service) unit.”
Error processing example 14631: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Milwaukee's Marcus Performing Arts Center – where 'The Daily Show' had been scheduled – “was originally located in the ‘soft perimeter,’ they called it, security-wise” but “was shifted, understandably so, to the ‘hard perimeter.’”
Error processing example 14632: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Just last week, Ruben Gallego voted to let the millions of people who poured into our country illegally cast a ballot in this upcoming election.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1642/2000 [28:44<04:53,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1643/2000 [28:45<04:37,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1644/2000 [28:46<05:21,  1.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1645/2000 [28:48<06:48,  1.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1647/2000 [28:48<04:14,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  82%|████████▏ | 1649/2000 [28:48<02:49,  2.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14633: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] After being shot at a Milwaukee rally in 1912, former President Teddy Roosevelt “finished his speech and he kept fighting.”
Error processing example 14634: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Unlike Mike Pence, (JD) Vance said he would have carried out Trump's plan to overturn the 2020 election."
Error processing example 14635: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 69.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sam Brown “said abortion should be banned without any exceptions for rape or incest.”
Error processing example 14636: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Project 2025 would “end Head Start.”
Error processing example 14637: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Today, America has record energy production and we are energy independent.”
Error processing example 14638: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Project 2025 calls for restricting “access to IVF and contraception.”
Error processing example 14639: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump “took away protections against discrimination for LGBTQ patients under the Affordable Care Act.”
Error processing example 14640: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Thomas Matthew Crooks worked for BlackRock "in the past" and BlackRock owned the building Crooks stood on when he shot at the Trump campaign rally.
Error processing example 14641: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The state of Washington has already sent out their ballots” for the 2024 presidential election.
Error processing example 14642: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden appointed "Kamala Harris to be his border czar to deal with illegal immigration … Harris was put in charge of stopping illegal immigration."
Error processing example 14643: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1651/2000 [28:48<01:57,  2.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1653/2000 [28:50<03:16,  1.77it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1655/2000 [28:50<02:23,  2.41it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1658/2000 [28:52<02:12,  2.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] The death of Rep. Sheila Jackson Lee, D-Texas, six days after the Trump rally shooting was linked to her “oversight of the Secret Service.”
Error processing example 14644: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] There are nine states “where Biden’s name can’t be removed and no one can be added. That’s over 130 electoral votes in the toilet.”
Error processing example 14645: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Eric Hovde “wants to repeal the Affordable Care Act, and kick millions off their health insurance.”
Error processing example 14646: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris “endorsed free taxpayer-funded government health care for all illegal aliens.”
Error processing example 14647: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris “wants to defund the police.”
Error processing example 14648: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Vice President, Kamala Harris cast the tie-breaking vote to cut, as you know, Medicare by $273 billion. She cast a vote to cut Medicare.”
Error processing example 14649: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris “supported abolishing ICE.”
Error processing example 14650: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We’re losing 300,000 people a year to fentanyl that comes through our border. We had it down to the lowest number and now it’s worse than it’s ever been.”
Error processing example 14651: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris “wants to forcibly compel doctors and nurses against their will to give chemical castration drugs to young children.”
Error processing example 14652: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] During the 2020 Minnesota riots after George Floyd’s murder, Gov. Tim Walz didn’t call in the National Guard, “so, I sent in the National Guard to save Minneapolis.”
Error processing example 14653: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows that “JD Vance endorses Project 2025.”
Processing examples:  83%|████████▎ | 1661/2000 [28:52<01:28,  3.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1663/2000 [28:52<01:09,  4.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1667/2000 [28:56<03:05,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  83%|████████▎ | 1669/2000 [28:57<03:05,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▎ | 1670/2000 [29:01<05:36,  1.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14654: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] C-sections are coded as abortions in hospitals.
Error processing example 14655: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Even before the pandemic, America went into a manufacturing recession.”
Error processing example 14656: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows “several masked men stormed a local voting station and stole the ballot boxes” in Venezuela’s 2024 election.
Error processing example 14657: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris is “calling for an end to the child tax credit.”
Error processing example 14658: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Donald Trump “intends to cut” Medicare.
Error processing example 14659: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Donald Trump intends to cut Social Security."
Error processing example 14660: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Nowhere on her birth certificate does it say that she is BLACK OR AFRICAN. …  Kamala Harris is NOT black and never has been.”
Error processing example 14661: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris was “Indian all the way, and then all of a sudden, she made a turn and she went, she became a Black person.”
Error processing example 14662: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In negotiating releases of Americans held abroad, “(I) gave the opposing country NOTHING — and never any cash.”
Error processing example 14663: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Since Vice President Harris and I took office … incomes have risen faster than prices."
Error processing example 14664: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Pompano Beach, Florida, Somali immigrants illegally in the U.S. were in line to get driver’s licenses and “all you need to vote is a driver's license.”
Processing examples:  84%|████████▎ | 1672/2000 [29:01<04:02,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▎ | 1673/2000 [29:01<03:26,  1.59it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1676/2000 [29:06<05:25,  1.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1678/2000 [29:06<03:57,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1682/2000 [29:06<02:12,  2.40it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14665: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris “supports mandatory gun confiscation.”
Error processing example 14666: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The NFL will now use facial recognition at every stadium to verify the identity of everyone at the game.”
Error processing example 14667: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Tim Walz “changed the Minnesota flag so it could resemble the Somalian flag.”
Error processing example 14668: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] JD Vance “literally wrote the foreword for the architect of the Project 2025 agenda."
Error processing example 14669: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Tim Walz signed into law driver's licenses for illegal immigrants in Minnesota."
Error processing example 14670: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Minnesota Gov. Tim Walz signed “legislation giving free college … to illegal immigrants.”
Error processing example 14671: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In 2021, Minnesotans were roughly five times more likely to move to Florida than vice versa” because they were “fleeing” the state under Democratic Gov. Tim Walz.
Error processing example 14672: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Violent crime was up under Donald Trump.”
Error processing example 14673: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “During Covid, Tim Walz rationed access to monoclonal antibody treatments based on skin color.”
Error processing example 14674: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Minnesota Gov. Tim Walz “forced schools to stock tampons in boys’ bathrooms.”
Error processing example 14675: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Tim Walz said he carried weapons in war, but “he has not spent a day in a combat zone.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1684/2000 [29:07<02:20,  2.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1686/2000 [29:08<02:17,  2.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1688/2000 [29:08<02:01,  2.57it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  84%|████████▍ | 1690/2000 [29:08<01:32,  3.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1692/2000 [29:11<02:42,  1.90it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14676: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republican Senate candidate Dave McCormick “wants to outlaw abortions.”
Error processing example 14677: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “When Tim Walz was asked by his country to go to Iraq, do you know what he did? He dropped out of the Army and allowed his unit to go without him.”
Error processing example 14678: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Minnesota Gov. Tim Walz signed a law that would “take children away from their parents if the parents don’t want to consent to sex changes.”
Error processing example 14679: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Rick Scott does not support abortion exceptions.
Error processing example 14680: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ruben Gallego wants to “cut your Social Security.”
Error processing example 14681: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Bob Casey and Kamala Harris want to BAN fracking.”
Error processing example 14682: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says his Jan. 6, 2021, speech on the White House Ellipse drew the “same number of people,” as the 1963 March on Washington where  Martin Luther King Jr. gave his “I Have a Dream” speech.
Error processing example 14683: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I went down in a helicopter with (Willie Brown). We thought maybe this is the end. We were in a helicopter going to a certain location together and there was an emergency landing.”
Error processing example 14684: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Jacky Rosen “said ending taxes on tips would ‘hurt working Nevada families.’”
Error processing example 14685: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mike Rogers “left Michigan to trade on his D.C. connections, helping Chinese tech companies get access to the U.S.”
Processing examples:  85%|████████▍ | 1693/2000 [29:12<03:44,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1694/2000 [29:16<06:15,  1.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1695/2000 [29:16<05:01,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1696/2000 [29:16<04:00,  1.26it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1697/2000 [29:19<06:39,  1.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1698/2000 [29:22<09:11,  1.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▍ | 1699/2000 [29:22<07:22,  1.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▌ | 1701/2000 [29:27<09:31,  1.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14686: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gov. Tim Walz delivered paid family leave in Minnesota but “Republicans are blocking” a Biden-Harris proposal.
Error processing example 14687: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] JD Vance holds “anti-marriage-equality views.”
Error processing example 14688: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo of Kamala Harris’ Aug. 7 rally near Detroit was AI-generated and “there was nobody there.” Attendees pictured “didn’t exist.”
Error processing example 14689: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Dr. Leo Ferreira, a cancer researcher shown in a video clip, was on the plane that crashed in Brazil on Aug. 9.
Error processing example 14690: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Sam Brown publicly supported forcing massive cuts to Social Security and Medicare and was caught on tape saying he admires the plan to phase out Social Security and Medicare in five years.”
Error processing example 14691: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says U.S. Sen. Tammy Baldwin “has done absolutely nothing” about the fentanyl crisis.
Error processing example 14692: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris voted in favor of passing a law in 2022 to tax tips.
Error processing example 14693: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Videos of media talking about Tim Walz as a "happy warrior" are proof of government propaganda legalized by Barack Obama.
Error processing example 14694: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump “cut overtime benefits for millions of workers.”
Error processing example 14695: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “400,000 workers are now in a union that were not in a union when (Biden) became president.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▌ | 1704/2000 [29:28<04:59,  1.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  85%|████████▌ | 1708/2000 [29:28<02:37,  1.85it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1710/2000 [29:34<05:43,  1.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1712/2000 [29:37<06:09,  1.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14696: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nevada Sen. Jacky Rosen said the Biden administration did a "tremendous job" on the U.S. military’s Afghanistan withdrawal.
Error processing example 14697: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California law lets you “rob a store as long as it’s not more than $950” and “not get charged.” Kamala Harris “did that.”
Error processing example 14698: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Elissa Slotkin’s vote for the 2022 Inflation Reduction Act was a vote to “cut Medicare benefits for seniors.”
Error processing example 14699: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Kamala Harris wants to give $25,000 to illegal aliens to buy American homes.”
Error processing example 14700: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “MAGA Republicans like Kari Lake support Project 2025, which would raise the retirement age for 73% of Arizonans.”
Error processing example 14701: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Videos show Kamala Harris intoxicated.
Error processing example 14702: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 329.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Women don't even get their own restroom at the DNC convention.”
Error processing example 14703: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In Montana, we cherish our public lands. But Tim Sheehy called to transfer them off, so the ultra-rich can buy them up.”
Error processing example 14704: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. Sen. Sherrod Brown voted to “give illegals taxpayer funded stimulus checks, health care, even Social Security."
Error processing example 14705: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The “‘Inflation Reduction Act’ increased inflation and drove prices sky high!”
Error processing example 14706: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1714/2000 [29:38<05:02,  1.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1716/2000 [29:38<03:39,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1717/2000 [29:41<05:03,  1.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1718/2000 [29:41<04:11,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1720/2000 [29:42<03:44,  1.25it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1721/2000 [29:42<03:11,  1.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▌ | 1723/2000 [29:43<02:58,  1.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “The Harris-Biden Administration has been caught fraudulently manipulating Job Statistics.”
Error processing example 14707: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump "intends to enact what, in effect, is a national sales tax ... that would raise prices on middle-class families by almost $4,000 a year."
Error processing example 14708: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Prices are up because these corporations are scheming to drive them up.”
Error processing example 14709: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Sen. Jacky Rosen voted to increase taxes on families making less than $75,000 a year.”
Error processing example 14710: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There’s been a 43% increase in violent crimes since I left office.”
Error processing example 14711: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Kamala cast the tiebreaking vote to hire 87,000 new IRS agents to go after your tip income.”
Error processing example 14712: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “JD Vance actually sent a letter last year to the Department of Justice saying, ‘enforce the Comstock Act.’”
Error processing example 14713: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris’ “plan to tax unrecognized Capital Gains mean(s) if your house goes up in value you will have to pay that Tax Even if you don’t sell your House!”
Error processing example 14714: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A zero-tolerance policy during the Trump administration "led to less family separation than under Kamala Harris' border policies.”
Error processing example 14715: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Anything that you lose on the tariff from the perspective of the consumer, you gain in higher wages, so you're ultimately much better off.”
Error processing example 14716: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Bob Casey “voted to sell American oil to China.”
Processing examples:  86%|████████▌ | 1724/2000 [29:43<02:27,  1.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▋ | 1725/2000 [29:45<03:04,  1.49it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▋ | 1726/2000 [29:45<02:26,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▋ | 1727/2000 [29:46<02:55,  1.55it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  86%|████████▋ | 1730/2000 [29:46<01:41,  2.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1732/2000 [29:46<01:12,  3.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1733/2000 [29:48<02:46,  1.60it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14717: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows helicopter “dropping millions” of mosquitoes in Massachusetts towns hit by eastern equine encephalitis virus.
Error processing example 14718: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris “is a communist. … She is really a Marxist.”
Error processing example 14719: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California S.B. 1174 eliminates the requirement for voter ID at polling places.
Error processing example 14720: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Elissa Slotkin “voted recently to let illegals vote in U.S. elections.”
Error processing example 14721: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The order of candidate names on North Carolina’s ballot shows a “clear attempt at voter manipulation.”
Error processing example 14722: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Project 2025 would defund K-12 schools.
Error processing example 14723: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “This month alone, more than 16,000 non-citizens have been removed from the voter rolls in 3 states.”
Error processing example 14724: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The proposal to put golf courses, hotels in Florida state parks “was something that was leaked. … a lot of that stuff was half-baked and was not ready for prime time.”
Error processing example 14725: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Kamala Harris has promised amnesty for the 10 million illegals she allowed in as border czar, making them eligible for Social Security. Studies warn this will lead to cuts in your Social Security benefits."
Error processing example 14726: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Proposed Nevada constitutional amendment to protect abortion rights would put “essentially no limit on access to abortion.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1735/2000 [29:49<02:07,  2.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1736/2000 [29:49<02:10,  2.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1739/2000 [29:49<01:16,  3.43it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1743/2000 [29:53<02:24,  1.78it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1744/2000 [29:53<02:07,  2.00it/s]Error processing example 14727: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Elissa Slotkin “gave your hard-earned tax money to illegal immigrants.”
Error processing example 14728: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden “did a campaign ad at the graves in Arlington in 2020.”
Error processing example 14729: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “BREAKING: Illegal Aliens in San Diego, California, tried to hijack 2 school buses filled with children.”
Error processing example 14730: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Right to Try" experimental drug program saved "thousands and thousands of lives."
Error processing example 14731: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The transgender thing is incredible... your kid goes to school and comes home a few days later with an operation. The school decides what’s going to happen with your child."
Error processing example 14732: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Jon Tester “voted to let men compete against our girls in their sports.”
Error processing example 14733: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Typically you have three to four debates in a U.S. Senate race. (Sen. Tammy Baldwin, D-Wis. has) given me one debate, almost a month after early voting has started."
Error processing example 14734: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Dave McCormick is fully against abortion.”
Error processing example 14735: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. Sen. Sherrod Brown “supported allowing puberty blockers and sex-change surgeries for minor children.”
Error processing example 14736: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] On June 7, 2011, Kamala Harris was involved in a San Francisco hit-and-run crash that injured a woman named Alicia Brown.
Error processing example 14737: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1746/2000 [29:54<02:08,  1.98it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  87%|████████▋ | 1749/2000 [29:54<01:21,  3.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1751/2000 [29:54<01:02,  3.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1753/2000 [30:03<05:37,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Of Bernie Moreno, “Now, he’s arguing for a national abortion ban, no exceptions.”
Error processing example 14738: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Inflation Reduction Act, which Rep. Elissa Slotkin supported, “drove inflation higher.”
Error processing example 14739: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Mark Robinson skips Board of Education vote on teacher bonuses to hold campaign event.”
Error processing example 14740: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Democratic opponent Tammy Baldwin "gave stimulus checks to illegals."
Error processing example 14741: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Tim Sheehy told us his health care plan: ‘We need to return health care to pure privatization.’ That would eliminate Medicare.”
Error processing example 14742: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Haitian immigrants are eating pets and wildlife in Springfield, Ohio.
Error processing example 14743: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris “cast the tiebreaking votes that caused the worst inflation in American history, costing a typical American family $28,000.”
Error processing example 14744: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida’s Amendment 3 is “the monopoly amendment.”
Error processing example 14745: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Tim Sheehy “would let politicians like him ban abortion, with no exceptions for rape or to save a woman’s life, and criminalize women.”
Error processing example 14746: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Laurie Buckhout is “trying to get to Congress to pass a national abortion ban, allowing politicians to ban abortions with no exceptions and prosecute women if they get an abortion, treating women right here like criminals.”
Error processing example 14747: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Less than three months ago, Kamala Harris and her administration blocked 300,000 unvetted Haitian migrants from being deported out of our country."
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1756/2000 [30:04<03:52,  1.05it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1757/2000 [30:04<03:22,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1759/2000 [30:05<03:00,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1762/2000 [30:05<01:54,  2.08it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1763/2000 [30:06<02:12,  1.79it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14748: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Illegal” Haitians in Springfield, Ohio, are “reportedly killing and eating pets.”
Error processing example 14749: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Dallas County Texas (tells) election workers to (illegally) lie on registration & use a church address to register homeless people to vote!”
Error processing example 14750: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Donald Trump said he was going to allow Medicare to negotiate drug prices. He never did. We did.”
Error processing example 14751: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Communicable diseases like HIV and TB (tuberculosis) have skyrocketed” after Haitian migrants came to Springfield, Ohio.
Error processing example 14752: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Unlike other amendments, Amendment 4 has no definitions. … Without definitions these words can mean almost anything.”
Error processing example 14753: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida’s six-week abortion law has “no real exceptions. Not for her health. Not even for rape.”
Error processing example 14754: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “As of today, there is not one member of the United States military who is in active duty in a combat zone, in any war zone around the world, for the first time this century.”
Error processing example 14755: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Jon Tester “supported slashing Social Security benefits”
Error processing example 14756: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "In Springfield (Ohio), they're eating the dogs, the people that came in, they're eating the cats. They're eating, they're eating the pets of the people that live there.”
Error processing example 14757: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mike Rogers supported “laws that could eliminate IVF and birth control.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1766/2000 [30:06<01:23,  2.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1769/2000 [30:12<03:24,  1.13it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  88%|████████▊ | 1770/2000 [30:12<03:07,  1.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▊ | 1771/2000 [30:18<06:35,  1.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▊ | 1772/2000 [30:19<06:06,  1.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14758: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In 12 states, children born alive after a failed abortion have no legal protection, and in three more states children born alive after an abortion had legal rights that governors — like Tim Walz — repealed.”
Error processing example 14759: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Manchester, N.H., gubernatorial candidate Joyce Craig “pushed for a city sales tax. Then, Joyce Craig overrode the tax cap and tried to raise taxes six times.”
Error processing example 14760: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump “started out with $400 million on a silver platter and then filed for bankruptcy six times.”
Error processing example 14761: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Charleroi, Pennsylvania, “has grown by 2,000% due to an influx in migrants.”
Error processing example 14762: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Ruben Gallego’s “first act in Congress was to propose legislation to BAN the use of the term ‘illegal alien.’”
Error processing example 14763: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris is “talking about bringing back the draft.”
Error processing example 14764: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In Project 2025, you’re going to have to register with a new federal agency when you get pregnant.”
Error processing example 14765: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Gubernatorial candidate Kelly Ayotte “has voted for national abortion bans, she has voted to defund Planned Parenthood, and she shepherded Neil Gorsuch through the Supreme Court process, and then celebrated when Roe v. Wade was overturned.”
Error processing example 14766: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Springfield, Ohio, “murders are up by 81%.”
Error processing example 14767: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ryan Routh's Instagram bio says he is a "proud LGBTQ+ member"
Processing examples:  89%|████████▉ | 1775/2000 [30:19<03:20,  1.12it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1776/2000 [30:19<02:47,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1778/2000 [30:22<03:28,  1.07it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1781/2000 [30:23<02:14,  1.63it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1782/2000 [30:25<03:09,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1784/2000 [30:26<02:42,  1.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14768: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump could have destroyed the Affordable Care Act, but “he chose to build upon (it)."
Error processing example 14769: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The apparent assassination attempt against former President Donald Trump on Sept. 15, 2024, was “staged.”
Error processing example 14770: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says opponent Sen. Tammy Baldwin “gave our taxpayer money to a transgender-affirming clinic … that does it without even telling parents."
Error processing example 14771: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “If you make less than $100,000, Trump is about to raise your taxes. Currently, you’re taxed at 10% and then 12%. He would raise that to a flat rate of 15%.”
Error processing example 14772: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The plus in LGBTQ+ includes pedophilia.
Error processing example 14773: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Two years ago, Bob Casey promised us his vote will reduce inflation. ... Casey’s vote for reckless spending” made inflation worse.
Error processing example 14774: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We're bankrupting a lot of hospitals by forcing these hospitals  to provide care for people who don't have the legal right to be in our country.”
Error processing example 14775: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When Haitians go to school in Springfield, Ohio, they “take the place of our children in school” and “each one will have a private interpreter."
Error processing example 14776: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Project 2025 wants to get rid of NOAA” and the National Weather Service.
Error processing example 14777: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris “supports taxpayer-funded sex changes for prisoners and illegal aliens.”
Processing examples:  89%|████████▉ | 1785/2000 [30:32<06:07,  1.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1786/2000 [30:34<06:24,  1.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1787/2000 [30:34<05:22,  1.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1788/2000 [30:34<04:08,  1.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  89%|████████▉ | 1789/2000 [30:35<03:25,  1.03it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|████████▉ | 1791/2000 [30:35<02:21,  1.48it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|████████▉ | 1793/2000 [30:36<01:44,  1.99it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|████████▉ | 1795/2000 [30:36<01:10,  2.91it/s]Error processing example 14778: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 325.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 58.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Wisconsin GOP U.S. Senate candidate Eric Hovde "brags about being in the 1%."
Error processing example 14779: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] First lady Jill Biden led a Cabinet meeting for President Joe Biden and her signature appears on presidential stationery.
Error processing example 14780: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Donald Trump added more to the national debt than any other president, ever.”
Error processing example 14781: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The last union member on a national ticket (was) Ronald Reagan.”
Error processing example 14782: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida "put Tom Walz instead of Tim Walz" on its ballots.
Error processing example 14783: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The Montana Secretary of State left Kamala Harris off the ballot.”
Error processing example 14784: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Project 2025 would “cut” the Federal Emergency Management Agency and “kill federal-backed flood insurance.”
Error processing example 14785: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. Sen. Sherrod Brown voted to “allow transgender biological men to compete in girls’ sports.”
Error processing example 14786: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Missouri registered 78,421 in just one week of February of this year. Out of that number, 23,253 were dead people!"
Error processing example 14787: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There was a bill to basically create a ban to make sure we never become a sanctuary state, that no locality passes those regulations, and (former Manchester Mayor Joyce Craig) opposed that bill.”
Error processing example 14788: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|████████▉ | 1796/2000 [30:40<03:32,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|████████▉ | 1798/2000 [30:40<02:18,  1.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|█████████ | 1801/2000 [30:40<01:22,  2.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|█████████ | 1803/2000 [30:41<01:24,  2.32it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|█████████ | 1805/2000 [30:41<01:02,  3.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “Even before the pandemic, (Donald Trump) lost manufacturing jobs, by most people’s estimates at least 200,000.”
Error processing example 14789: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] North Carolina removed 747,000 voters from the voter rolls to try to “steal the election.”
Error processing example 14790: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump wants to take away the Affordable Care Act.
Error processing example 14791: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] When New Hampshire gubernatorial nominee Kelly Ayotte served on two companies’ boards, one “laid off 18,000 workers” and the other “laid off 1,200 American workers, moving jobs overseas.”
Error processing example 14792: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. Sen. Jon Tester “voted to give taxpayer-funded health care to illegal immigrants."
Error processing example 14793: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Ruben Gallego is “controlled by the cartels. His own father was a Colombian drug trafficker, and so he’s got links to the cartel.”
Error processing example 14794: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows “the bunker-busting bombs that ended the lives of (Hassan) Nasrallah and Hezbollah’s leadership.”
Error processing example 14795: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The ERA could also mandate that schools allow biological males to compete in girls' sports."
Error processing example 14796: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Kamala Harris "let in the 13,099 convicted murderers and opposes all efforts to find them and to remove them."
Error processing example 14797: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris “even wants to legalize fentanyl.”
Error processing example 14798: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. Senate candidate Mike Rogers “believes he should make that decision” about whether to end pregnancies.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|█████████ | 1808/2000 [30:41<00:41,  4.62it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  90%|█████████ | 1810/2000 [30:43<01:00,  3.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1813/2000 [30:44<01:04,  2.89it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1815/2000 [30:44<00:52,  3.52it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14799: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photo shows President Joe Biden lounging on the beach "while towns all over America were being wiped out” by Hurricane Helene.
Error processing example 14800: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Biden Announces: ‘2.4 billion more to Ukraine’ but ‘no more aid for Hurricane Helene.’”
Error processing example 14801: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Georgia Gov. Brian Kemp was calling President Joe Biden after Hurricane Helene but "hasn't been able to get him.”
Error processing example 14802: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says immigrants with temporary protections in Springfield, Ohio, are “illegal immigrants.”
Error processing example 14803: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Federal Emergency Management Agency “is confiscating supplies and donations!”
Error processing example 14804: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] With U.S. Customs and Border Protection’s CBP One app, it “takes less than 5 minutes and zero documentation to get approved as an illegal immigrant and be flown to the United States with air tickets paid for by the American taxpayer.”
Error processing example 14805: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] U.S. Transportation Secretary Pete Buttigieg banned private drones from flying over areas affected by Hurricane Helene.
Error processing example 14806: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The U.S. government is offering Americans loans to pay for flights out of Lebanon while they have been flying Israeli citizens out of Israel on chartered flights for a year “on our dime.”
Error processing example 14807: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "FEMA is out of money and doesn't have money to transfer to those people affected by the hurricane ... they used the money to assist illegal immigrants."
Error processing example 14808: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] FEMA, FAA are “blocking” delivery of Starlink devices and supplies to North Carolina’s hurricane-hit areas.
Processing examples:  91%|█████████ | 1816/2000 [30:47<02:14,  1.37it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1817/2000 [30:49<03:01,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1818/2000 [30:51<03:24,  1.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1819/2000 [30:55<05:14,  1.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1821/2000 [30:55<03:12,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1823/2000 [30:55<02:06,  1.40it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████ | 1824/2000 [30:56<02:22,  1.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████▏| 1825/2000 [30:56<01:56,  1.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████▏| 1826/2000 [31:00<04:04,  1.41s/it]Error processing example 14809: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 53.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Biden administration stole $1 billion “from FEMA to use it for illegal migrants. … And FEMA is now busted. They don’t have any money.”
Error processing example 14810: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The $750 from FEMA that Kamala Harris is offering them is actually a loan, not real relief. And that if they don’t pay it back the feds can seize their property.”
Error processing example 14811: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “With your Democrat governor, you have some of the highest electricity prices and highest energy costs. It’s just about at the top in Wisconsin.”
Error processing example 14812: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 69.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] 2023 FEMA webinar shows the agency is prioritizing its relief efforts around LGBTQ+ people
Error processing example 14813: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] FEMA “blocked a runway” at a South Carolina airport and “halted” hurricane relief flights.
Error processing example 14814: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "We now have historic low unemployment in America among all groups of people."
Error processing example 14815: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says opponent Eric Hovde “just proposed cutting Social Security by 28%.”
Error processing example 14816: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Teachers and nurses and firefighters are paying a higher tax rate than billionaires and the biggest corporations."
Error processing example 14817: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We have cut the flow of fentanyl by half.”
Error processing example 14818: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Hurricane Milton article is evidence they "control and plan these storms."
Error processing example 14819: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  91%|█████████▏| 1827/2000 [31:01<04:04,  1.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1830/2000 [31:04<02:57,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1832/2000 [31:05<02:45,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1835/2000 [31:06<01:39,  1.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1836/2000 [31:08<02:27,  1.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] Of Gaza, “I’ve been there.”
Error processing example 14820: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Patents dating back to the 1800s show that people can control the weather.
Error processing example 14821: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Milwaukee is one of the sex trafficking capitals in our country.”
Error processing example 14822: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Two weeks ago 100 GOP lawmakers voted against additional FEMA funding.”
Error processing example 14823: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Elissa Slotkin “signed a secret agreement with a CCP-linked Chinese battery company.”
Error processing example 14824: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Arizona U.S. Senate candidate Kari Lake “was against” the child tax credit expansion “when the bill was on the floor.”
Error processing example 14825: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris’ “60 Minutes” interview may be “a major Campaign Finance Violation.”
Error processing example 14826: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Arizona was safe” when she moved to the state in 1994, but has become “unsafe.”
Error processing example 14827: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] North Carolina’s new voting rules in response to Hurricane Helene amount to “cheating” or “stealing.”
Error processing example 14828: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] To pay for extending the Trump tax cuts, Dave McCormick has “made clear he’ll slash your Medicare and Social Security, and cut Medicaid for nursing home care.”
Error processing example 14829: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says opponent Eric Hovde “supports a $4 trillion tax plan that would disproportionately advantage the well-off and profitable corporations”
Processing examples:  92%|█████████▏| 1837/2000 [31:08<02:02,  1.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1840/2000 [31:08<01:09,  2.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1841/2000 [31:12<02:52,  1.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1842/2000 [31:15<03:45,  1.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1845/2000 [31:15<02:01,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14830: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republican state Senate candidate Ashlee Adams “campaigned for Mark Robinson and his radical agenda.”
Error processing example 14831: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “It has just come out that Democrats in Washington and the Democrat Governor’s Office of North Carolina (Roy Cooper) were blocking people and money from coming into North Carolina.”
Error processing example 14832: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Scott Lassiter “repeatedly campaigned to promote” North Carolina's “largest-ever” school funding cut, sending public school money to “unaccountable private schools” and ending science education.
Error processing example 14833: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Wages adjusted for inflation were massively up under Donald Trump for Black men. … The (Black-white) wage gap in 2019 was actually shrinking under Donald Trump's administration.”
Error processing example 14834: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 69.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Screenshots of documents show a former student of Democratic vice presidential candidate Tim Walz has lodged sexual abuse allegations against him.
Error processing example 14835: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “About 400,000 manufacturing jobs in the state of Michigan will go away with EV mandates.”
Error processing example 14836: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Sen. Bob Casey "has voted in lockstep with his liberal party to defund the police."
Error processing example 14837: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Defense Department, with the stroke of a pen, has quietly codified its right to deploy lethal force against its own citizens.”
Error processing example 14838: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Detroit’s Oct.15, 2024, absentee ballot return rate proves election fraud.
Error processing example 14839: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris' plan “will raise taxes for the typical American family by an estimated $3,000."
Processing examples:  92%|█████████▏| 1847/2000 [31:15<01:27,  1.76it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  92%|█████████▏| 1849/2000 [31:16<01:20,  1.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1851/2000 [31:19<02:08,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1852/2000 [31:20<01:49,  1.35it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1853/2000 [31:21<02:03,  1.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1854/2000 [31:21<01:39,  1.46it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1855/2000 [31:26<04:19,  1.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1856/2000 [31:28<04:06,  1.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1857/2000 [31:30<04:12,  1.76s/it]Error processing example 14840: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says former President Donald Trump did not lose the 2020 election.
Error processing example 14841: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris “wants struggling seniors to pay more Social Security taxes while she gives Medicare and Social Security to illegals.”
Error processing example 14842: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says opponent Eric Hovde “opposes efforts to negotiate with the big pharmaceutical companies to lower the price of prescription drugs.”
Error processing example 14843: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Eric Hovde “claims anyone who believes in transitioning to a clean energy economy is ‘smoking crack cocaine.’”
Error processing example 14844: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 57.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ballot drop boxes 'were only used for a pandemic.'
Error processing example 14845: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] As part of his “charitable donations,” in 1990 Donald Trump “chartered a private plane to get Nelson Mandela to America when the U.S. government wouldn't help."
Error processing example 14846: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Michigan has “more registered voters than eligible citizens. This creates a huge risk of election fraud.”
Error processing example 14847: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Voting machines in Shelby County, Tennessee, are swapping votes from Kamala Harris to Donald Trump.
Error processing example 14848: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 327.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I’ve not gotten a single call from the White House.”
Error processing example 14849: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Jacky Rosen voted to allow biological men to compete in women’s sports.”
Error processing example 14850: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1859/2000 [31:30<02:26,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1860/2000 [31:30<01:54,  1.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1861/2000 [31:34<03:32,  1.53s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1864/2000 [31:34<01:43,  1.31it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1867/2000 [31:34<01:01,  2.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] A pro-Kamala Harris website shows she supports a mandatory gun buyback program.
Error processing example 14851: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A photo shows The Atlantic published a story headlined “Trump is Literally Hitler.”
Error processing example 14852: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "As of today, we have cut the flow of immigration by over half."
Error processing example 14853: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ruben Gallego wanted to defund the police. He actually co-sponsored legislation to defund the police.”
Error processing example 14854: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vice President Kamala Harris has been “going so far as to call me Adolf Hitler.”
Error processing example 14855: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Michigan U.S. Senate candidate Mike Rogers “wants to cut Medicare and Social Security.”
Error processing example 14856: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Kari Lake is threatening Social Security and Medicare.”
Error processing example 14857: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The United States Postal Service recently changed the delivery method of absentee mail-in ballots” to hide “the evidence of absentee mail-in ballot fraud.”
Error processing example 14858: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “As California attorney general, (Kamala Harris) redefined child sex trafficking, assault with a deadly weapon, and rape of an unconscious person as a totally nonviolent crime.”
Error processing example 14859: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Republican Senate candidate Sam Brown “wants to cut Medicaid and Social Security.”
Error processing example 14860: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows a ‘Democratic operative’ making credible allegations of fraud during the 2020 presidential election in Georgia.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  93%|█████████▎| 1869/2000 [31:36<01:14,  1.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▎| 1871/2000 [31:36<00:56,  2.30it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▎| 1873/2000 [31:37<01:06,  1.91it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1876/2000 [31:42<01:53,  1.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14861: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Donald Trump would “force states to monitor women's pregnancies.”
Error processing example 14862: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] In Lancaster, Pa., “We caught them with 2,600 votes. We caught them cold, 2,600 votes. ... And every vote was written by the same person."
Error processing example 14863: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The lady who leaked passwords for voting systems in Colorado is the same person who tried to remove Trump from the ballot.”
Error processing example 14864: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “It’s now been confirmed by Dominion that their voting machines have a programming issue that will affect vote counts.”
Error processing example 14865: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Data shows some voters cast multiple votes, meaning "at least 164,568 illegal votes have been cast” in Michigan.
Error processing example 14866: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows “ballot mule in Northampton County, Pennsylvania, (dropping) off a large amount of ballots.”
Error processing example 14867: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I never mentioned” wanting to end the Affordable Care Act.
Error processing example 14868: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video shows “voting machines in Kentucky are not allowing voters to select” Donald Trump for president.
Error processing example 14869: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Pentagon reportedly failed to send absentee ballots to active military service members before the election.”
Error processing example 14870: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] If a ballot has a small dot in the box for Vice President Kamala Harris, “any other box filled in, will be void.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1879/2000 [31:42<01:12,  1.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1881/2000 [31:42<00:55,  2.14it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1883/2000 [31:43<00:59,  1.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1884/2000 [31:44<00:53,  2.18it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1885/2000 [31:45<01:05,  1.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  94%|█████████▍| 1886/2000 [31:45<00:54,  2.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14871: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “In Nevada, the GOP is succeeding in throwing out the votes of thousands of young people.”
Error processing example 14872: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Paper ballots and hand counting never breaks down.”
Error processing example 14873: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Vans filled with 50,000 ballots show up in Philadelphia during live MSNBC report.
Error processing example 14874: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “CCTV captures the moment a Trump voter in (Wisconsin) is assaulted by two Kamala Harris thugs at a voting station.”
Error processing example 14875: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Every single county in America, every single county, Kamala Harris did worse than Joe Biden did.”
Error processing example 14876: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Abortion is never medically necessary in the last trimester. Literally never. They just do a C-section.”
Error processing example 14877: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President-elect Donald Trump “has been to Epstein’s island”
Error processing example 14878: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] 20 million Democratic votes “disappeared” in 2024.
Error processing example 14879: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I think they found a way to target Democratic districts and send millions of votes into a black hole. The numbers aren't adding up.”
Error processing example 14880: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 2024 election results show that Joe Biden’s 80 million votes in 2020 were a “lie.”
Error processing example 14881: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Something stinks” that Donald Trump won in several battleground states while Democrats won Senate or governor races in those states.
Processing examples:  94%|█████████▍| 1889/2000 [31:45<00:29,  3.75it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▍| 1891/2000 [31:49<01:34,  1.15it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▍| 1893/2000 [31:51<01:29,  1.19it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▍| 1895/2000 [31:52<01:27,  1.20it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▍| 1897/2000 [31:53<01:14,  1.39it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▍| 1898/2000 [31:57<01:59,  1.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14882: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "BREAKING: Zillow CEO just said on CNBC Trump's deportation plan will help with housing affordability.”
Error processing example 14883: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Fewer people came in under President Biden than came under Donald Trump.”
Error processing example 14884: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Arizona officials caught changing ballots, have been arrested.”
Error processing example 14885: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Inflation-adjusted weekly wages “are lower today than they were 50 years ago.”
Error processing example 14886: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Elon Musk’s Starlink technology manipulated votes in the 2024 election to benefit Donald Trump.
Error processing example 14887: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Every state that does not require voter ID, Kamala won.”
Error processing example 14888: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] A “ballot dump” around 4 a.m. in Milwaukee shows the Wisconsin Senate election was stolen from the Republican candidate.
Error processing example 14889: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Red Cross is set to close shelters across North Carolina on Nov. 15. Families staying in these shelters are being told: ‘If you don’t have a safe place to take your children, you won’t be leaving with them.’”
Error processing example 14890: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says the election results are suspicious because opponent U.S. Sen. Baldwin won “nearly 90%” of absentee ballots in Milwaukee.
Error processing example 14891: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Says Sen. Bob Casey, D-Pa., “is trying to change the outcome of the election by counting NON-CITIZEN votes.”
Processing examples:  95%|█████████▍| 1899/2000 [31:57<01:36,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▌| 1901/2000 [31:57<01:04,  1.54it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▌| 1902/2000 [31:58<01:13,  1.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▌| 1905/2000 [31:59<00:56,  1.69it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  95%|█████████▌| 1907/2000 [32:00<00:39,  2.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14892: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Nissan just announced 9,000 layoffs in Tennessee “to avoid severe losses due to expected tariffs.”
Error processing example 14893: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “There's never been a placebo-controlled study on childhood vaccines.”
Error processing example 14894: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.25 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Ballot dumps" in Pennsylvania, California, North Carolina, Alaska and Wisconsin show election officials are improperly “flipping" down-ballot seats for Democrats.
Error processing example 14895: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] We’ve created 732,000 jobs since I've been governor.
Error processing example 14896: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The U.S Senate accidentally passed a bill banning Trump from becoming President, and Biden is planning to sign it.”
Error processing example 14897: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] FEMA delivered “only four” temporary housing units to Helene victims in North Carolina as of Nov. 20.
Error processing example 14898: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Rachel Maddow MELTS DOWN over Elon Musk posting memes about buying MSNBC.”
Error processing example 14899: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Half of all Ukrainian aid was laundered and sent back to Democrat candidates for re-election."
Error processing example 14900: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Elon Musk said he might run for governor of California.”
Error processing example 14901: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The famed ship at the bottom of the North Atlantic is actually not The Titanic. It is her near-identical sister ship The Olympic."
Error processing example 14902: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1910/2000 [32:00<00:24,  3.66it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1912/2000 [32:02<00:44,  1.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1914/2000 [32:03<00:50,  1.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1915/2000 [32:04<00:49,  1.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1916/2000 [32:09<01:55,  1.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1918/2000 [32:09<01:15,  1.09it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1919/2000 [32:10<01:17,  1.04it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.

[CLAIM] “The Universities of Wisconsin are 43rd out of 50 states in the nation, in terms of public support for our universities. The $855 million (budget request) gets us up to average.”
Error processing example 14903: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Trump won 18-to-24-year-olds in Wisconsin."
Error processing example 14904: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “New $40K AIDS cure is nearly 100% effective and requires two shots yearly.”
Error processing example 14905: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "President Trump has announced a plan to prosecute the Biden administration for the 325,000 kids that went ‘missing’ into human trafficking & sex slavery during their open border policies.”
Error processing example 14906: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] North Carolina Republicans “took money out of western North Carolina” with Hurricane Helene relief bill.
Error processing example 14907: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Mitchell County, North Carolina, sheriff’s deputies “stole” generators intended for Hurricane Helene victims.
Error processing example 14908: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The city of… is being warned to be alert & vigilant as there is a serial killer on the run,”  Robert Thibodeau.
Error processing example 14909: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “After meeting with Elon Musk, Republican leader Sen. John Thune announces plans to cut Social Security.”
Error processing example 14910: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] About 1% of federal employees are “actually working in the office.”
Error processing example 14911: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The 14th Amendment “has never been challenged at SCOTUS.”
Error processing example 14912: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President-elect Donald Trump’s “plan to end birthright citizenship would mean (four) of his children wouldn't be considered US citizens.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1922/2000 [32:10<00:42,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▌| 1924/2000 [32:11<00:40,  1.86it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▋| 1927/2000 [32:11<00:24,  2.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  96%|█████████▋| 1929/2000 [32:11<00:18,  3.74it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14913: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Drones reported in the U.S., including around the U.S. Capitol, have directed-energy weapons that have a “space-based laser generating system that can create pestilence.”
Error processing example 14914: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “A $7.20 an hour McDonalds employee … now gets a cool 1/2 million dollar reward” for helping police find the suspected UnitedHealthcare CEO shooter.
Error processing example 14915: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Dr. Assad will return to ophthalmology after being deposed as president of Syria and plans to set up an eye clinic in Russia.”
Error processing example 14916: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Photos show mysterious drones reported flying over parts of New Jersey in November and December 2024.
Error processing example 14917: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] New Jersey drones are searching for missing radioactive material.
Error processing example 14918: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Government shutdowns in 2013 and 2018 “cost our economy billions of dollars each.”
Error processing example 14919: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Spending plan to prevent a government shutdown includes “a pay increase for members of Congress from $174,000 to $243,000 per year.”
Error processing example 14920: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The subway burning victim's name was Amelia Carter.”
Error processing example 14921: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “After 30 years of Wisconsin’s checking account running a deficit, we've ended every fiscal year I've been governor with a positive balance. Today, we have a $4.5 billion positive balance, and a record-high balance in our state savings account."
Error processing example 14922: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 46.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] H-1B guest workers are “being employed as dog trainers, massage therapists, cooks, and English teachers.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1931/2000 [32:13<00:24,  2.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1932/2000 [32:14<00:30,  2.25it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1934/2000 [32:14<00:24,  2.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1936/2000 [32:15<00:22,  2.87it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1938/2000 [32:15<00:19,  3.21it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1940/2000 [32:17<00:31,  1.93it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14923: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] China has “a mystery disease outbreak” and has declared a “state of emergency.”
Error processing example 14924: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “BREAKING: A second attack in New Orleans has been uncovered, police are searching.”
Error processing example 14925: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Buncombe County “is still demanding property taxes on homes destroyed by Hurricane Helene based on pre-Helene assessments that no longer apply.”
Error processing example 14926: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California wildfires “are a deliberate criminal land grab in preparation for Agenda 2030 and smart cities.”
Error processing example 14927: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “They’re still counting the (2024 election) vote in some areas.”
Error processing example 14928: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin makes it more difficult for its citizens to vote than almost any state in the nation.
Error processing example 14929: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Voter ID "is supported, if you look at any poll, by 70 to 80% of the public."
Error processing example 14930: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “One billionaire couple owns almost all the water in California."
Error processing example 14931: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Blue items that survive” California wildfires are “indicative of DEW's (Directed Energy Weapons).”
Error processing example 14932: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “So (Los Angeles), Hollywood, and California (get) 100% aid for the next 180 days while Western North Carolina got $750.”
Error processing example 14933: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “P Diddy’s California mansion has been completely consumed by fire.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1942/2000 [32:18<00:29,  1.95it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1943/2000 [32:19<00:33,  1.72it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1945/2000 [32:19<00:22,  2.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1946/2000 [32:19<00:19,  2.81it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1947/2000 [32:21<00:41,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  97%|█████████▋| 1949/2000 [32:22<00:26,  1.96it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1950/2000 [32:22<00:21,  2.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14934: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Video claims to show the Los Angeles fires.
Error processing example 14935: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “​​Oregon sent 60 fire trucks to California to help with the fires, but they’re being held in Sacramento for emissions testing.”
Error processing example 14936: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Some of these (California) reservoirs have been dry for 15, 20 years."
Error processing example 14937: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] California wildfires have “nothing to do with climate change.”
Error processing example 14938: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The Equal Rights Amendment has become part of our Constitution.”
Error processing example 14939: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “After the referendum, Milwaukee Public Schools has a larger tax levy than the City of Milwaukee.”
Error processing example 14940: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "Our nation’s tallest mountain … has been called Denali for thousands of years."
Error processing example 14941: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Second lady Usha Vance “will have her citizenship revoked if (President Donald) Trump signs his executive order banning birthright citizenship.”
Error processing example 14942: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United States is the only country with unrestricted birthright citizenship.
Error processing example 14943: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Under Trump's executive order, every single person in America is now legally classified as female.”
Error processing example 14944: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The removal of birthright citizenship only applies to the children of those that are here ILLEGALLY.”
Processing examples:  98%|█████████▊| 1952/2000 [32:26<00:49,  1.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1954/2000 [32:26<00:35,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1955/2000 [32:27<00:33,  1.34it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1956/2000 [32:31<01:05,  1.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1957/2000 [32:31<00:49,  1.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1959/2000 [32:31<00:28,  1.42it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1960/2000 [32:32<00:31,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14945: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Donald Trump signed an executive order to replace the IRS with the External Revenue Service, eliminating tax returns and income tax.
Error processing example 14946: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The U.S. Senate has the votes to remove Trump from office if the House votes to impeach.”
Error processing example 14947: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The U.S. House Select Committee that investigated the Jan. 6, 2021, attack on the U.S. Capitol “deleted and destroyed all of the information that they collected over two years.”
Error processing example 14948: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Donald Trump "just reversed all the cost caps Biden negotiated for anyone on Medicare or Medicaid."
Error processing example 14949: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "The Biden administration kicked 2,000 displaced North Carolinians out of their temporary housing into freezing 20-degree weather," earlier in January.
Error processing example 14950: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] We’re one of the very few nations that allowed birthright citizenship.”
Error processing example 14951: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “As a Californian, we have given more to the recovery of other states than any other state in the union.”
Error processing example 14952: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The words “help” and “trafico” in a Google Maps aerial view of Los Angeles are connected to human trafficking.
Error processing example 14953: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The U.S. has an egg shortage because the “Biden administration and the Department of Agriculture directed the mass killing of more than 100 million chickens.”
Error processing example 14954: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Florida’s in-state college tuition waiver for immigrants in the country illegally costs the state $41 million.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1963/2000 [32:33<00:15,  2.33it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1966/2000 [32:33<00:09,  3.70it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1968/2000 [32:34<00:11,  2.69it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  98%|█████████▊| 1969/2000 [32:34<00:10,  3.06it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▊| 1971/2000 [32:35<00:11,  2.58it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14955: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “DOGE and OMB also found that there was about to be $50 million taxpayer dollars that went out the door to fund condoms in Gaza.”
Error processing example 14956: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Robert F. Kennedy Jr. “made $2.5 million off suing one of the entities that (he would be regulating) and plans to keep getting a take of every lawsuit in the future.”
Error processing example 14957: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Early reports of four survivors show “this DC plane crash story isn’t adding up.”
Error processing example 14958: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The National Human Trafficking Hotline “in the last four years decided that they would no longer report tips to law enforcement, that they would take a more victim-centered approach.”
Error processing example 14959: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I changed the Obama policy (on hiring air traffic controllers) ... And then Biden came in and he changed it.”
Error processing example 14960: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] CNN reported on the Potomac River plane and helicopter crash “hours before it happened.”
Error processing example 14961: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] At Guantánamo Bay, “we'll have the capacity to continue to do there what we've always done. We've always had a presence of illegal immigrants there that have been detained.”
Error processing example 14962: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ring is “alerting people about (Immigration and Customs Enforcement) raids.”
Error processing example 14963: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Federal websites updated with a nine-star flag that represents the Confederacy.
Error processing example 14964: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] From the U.S. Agency for International Development funding, “10 to 30 cents on the dollar is what actually goes to aid.”
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▊| 1973/2000 [32:35<00:08,  3.23it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▊| 1974/2000 [32:39<00:22,  1.17it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1975/2000 [32:39<00:20,  1.24it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1976/2000 [32:40<00:20,  1.16it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1978/2000 [32:43<00:21,  1.01it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1979/2000 [32:43<00:19,  1.06it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1981/2000 [32:44<00:11,  1.65it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14965: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Ukrainian President Volodymyr Zelenskyy “just threw the whole Ukraine money laundering operation under the bus … by stating he’s received less than half of the over $177 billion in aid sent by the United States.”
Error processing example 14966: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] As head football coach at Texas Tech University, “I recruited (Patrick Mahomes) ... I’ve got to be very good friends with him.”
Error processing example 14967: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Elon Musk officially cancelled USAID and fired all the staff. In his words, ‘we just toppled one of the biggest globalist terror organizations in history.’”
Error processing example 14968: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] CBS News and “60 Minutes” “replaced” former Vice President Kamala Harris’ interview answers “with completely different, and far better, answers, taken from another part of the interview.”
Error processing example 14969: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “US Debt Clock” numbers prove “DOGE has saved taxpayers $64 billion dollars in just 17 days.”
Error processing example 14970: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Wisconsin does not “require judges to automatically recuse just because they have done some kind of legal work in the past as a lawyer.”
Error processing example 14971: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 75.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrants… That money is meant for American disaster relief.”
Error processing example 14972: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The United States is “last in education out of 40 states … but we're number one in cost per pupil."
Error processing example 14973: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Rep. Nancy Pelosi has a $223,000 salary and a $202 million net worth. Sen. Mitch McConnell has a $200,000 salary and a $95 million net worth. Sen. Chuck Schumer has a $210,000 salary and a $75 million net worth. Sen. Elizabeth Warren has a $285,000 salary and a $67 million net worth.
Error processing example 14974: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 45.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Under the SAVE Act, “if you are a woman that has changed your name from your birth certificate, let’s say through marriage … you are no longer eligible to vote.”
Processing examples:  99%|█████████▉| 1982/2000 [32:45<00:13,  1.36it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1983/2000 [32:45<00:10,  1.67it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1985/2000 [32:45<00:05,  2.51it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1986/2000 [32:46<00:07,  1.82it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1987/2000 [32:46<00:05,  2.22it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1988/2000 [32:52<00:21,  1.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples:  99%|█████████▉| 1989/2000 [32:57<00:28,  2.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1990/2000 [32:57<00:19,  1.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1991/2000 [32:58<00:15,  1.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1992/2000 [32:58<00:09,  1.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Error processing example 14975: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 77.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The federal workforce grew from 2.1 to 3 million under President Joe Biden, “a 46% increase since Biden took office.”
Error processing example 14976: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Chicago Cubs owner is bankrolling Brad Schimel.
Error processing example 14977: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Former President Joe Biden “fired” 14,000 Keystone XL pipeline workers “​​on day one” of his administration.
Error processing example 14978: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “I am not making this up: Republicans have introduced legislation … to increase your bank fees."
Error processing example 14979: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 79.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 56.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Volodymyr Zelenskyy “started” the war in Ukraine with Russia.
Error processing example 14980: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 85.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.50 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “90% of the (school) districts in Wisconsin already have a policy” banning cellphones during class time.
Error processing example 14981: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 71.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 54.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “Stacey Abrams stole $2 BILLION from taxpayers.”
Error processing example 14982: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 55.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The single biggest driver of our national debt since 2001 has been Republican tax cuts.”
Error processing example 14983: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “You either have XX or XY chromosomes. Those are the options.”
Error processing example 14984: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] President Joe Biden’s immigration policy was “a giant voter importation scam.”
Error processing example 14985: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “71 out of 72 Wisconsin counties filed lawsuits against Purdue Pharma. Brad Schimel refused to join them.”
Processing examples: 100%|█████████▉| 1993/2000 [32:58<00:06,  1.11it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1994/2000 [33:01<00:07,  1.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1996/2000 [33:01<00:03,  1.28it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1997/2000 [33:02<00:02,  1.29it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|█████████▉| 1999/2000 [33:02<00:00,  1.97it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.
Processing examples: 100%|██████████| 2000/2000 [33:05<00:00,  1.17s/it]Processing examples: 100%|██████████| 2000/2000 [33:05<00:00,  1.01it/s]
Error processing example 14986: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Zero votes for 2024 Democratic presidential nominee Kamala Harris in Rockland County, New York, precinct proves Elon Musk hacked 2024 presidential election.
Error processing example 14987: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 57.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “We've never audited government for 100 years.”
Error processing example 14988: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 83.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 53.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] "All armed services are having among the best recruiting results ever” because of Trump policies, and “it was just a few months ago where the results were exactly the opposite.”
Error processing example 14989: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.26 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] Elon Musk’s Department of Government Efficiency is “polling very well. It’s very popular.”
Error processing example 14990: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] The Trump administration identified $1.9 billion in federal dollars going to a “decarbonization of homes” group Stacey Abrams “headed up.”
Error processing example 14991: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 54.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] During President Joe Biden’s four years in office, “31% of U.S. job growth” came from public-sector jobs.
Error processing example 14992: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 81.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.54 GiB memory in use. Of the allocated memory 1.83 GiB is allocated by PyTorch, and 55.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[CLAIM] “The number of full-time jobs (was) dropping almost the entire Biden administration.”
Error processing example 14993: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 73.44 MiB is free. Process 1479422 has 46.96 GiB memory in use. Process 2528377 has 29.51 GiB memory in use. Process 2773541 has 2.55 GiB memory in use. Of the allocated memory 1.84 GiB is allocated by PyTorch, and 56.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Processing completed. Total examples processed: 0
